<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Dendrobiumcgk</title>
        <link>https://Dendrobium123.github.io/</link>
        <description>Recent content on Dendrobiumcgk</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 14 Jun 2025 13:10:42 +0800</lastBuildDate><atom:link href="https://Dendrobium123.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>博客装修日志</title>
        <link>https://Dendrobium123.github.io/p/%E5%8D%9A%E5%AE%A2%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/</link>
        <pubDate>Sat, 14 Jun 2025 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E5%8D%9A%E5%AE%A2%E8%A3%85%E4%BF%AE%E6%97%A5%E5%BF%97/</guid>
        <description>&lt;img src="https://tuchuang.dendrobiumcgk.chat/pic/blog-fitment.jpg" alt="Featured image of post 博客装修日志" /&gt;&lt;h1 id=&#34;缘起&#34;&gt;缘起&lt;/h1&gt;
&lt;p&gt;之前6月10号看了APPLE的发布会，这次新推出的主题叫液态玻璃，虽然推出后马上遭到网友狂喷说特别丑，但说实话还蛮对我胃口的，我其实蛮喜欢这种玻璃风格。一想到我的博客页面还特别简陋，就想试试能不能稍微装修一下。&lt;/p&gt;
&lt;h3 id=&#34;6月14日&#34;&gt;6月14日&lt;/h3&gt;
&lt;h5 id=&#34;sidebar&#34;&gt;sidebar&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 修改了sidebar.scss中的menu模块，改变了sidebar背景透明度，添加了一个三种颜色组合的圆形渐变层加伸缩变化来模仿光晕效果，修改了hover的border效果，使得鼠标移动上去后按钮会做x轴的位移并且border会变色，并且鼠标点击后整个按钮会弹跳一下。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 同理修改了暗色模式的toggle，鼠标放置后会做y轴的轻微位移。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;背景&#34;&gt;背景&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 修改了网页背景颜色为两种混合色&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;avatar&#34;&gt;avatar&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 为头像容器加入玻璃拟态样式，包括 &lt;code&gt;blur()&lt;/code&gt; 和 &lt;code&gt;border&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 添加了头像 hover 动效，放大、光圈泛动，使头像特效类似Instagram的snap限动。&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 修改了文章article的背景颜色，使其看起来是半透明的。同时添加了伸缩的hover，并且修改了原始状态和伸缩后hover的box-shadow使其在光标放上去后效果看着像悬浮起来。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;6月15日&#34;&gt;6月15日&lt;/h3&gt;
&lt;p&gt;重新处理了域名访问过慢的问题，尝试了很多方案，包括使用cloudflare的dns服务，腾讯云的cos+cdn服务等，前者速度比原先还慢，后者因为没备案所以无法执行，本来还启用了腾讯云的cdn加速服务，但因为是回源是托管在vercel上的，所以启用cdn后被微信网页段屏蔽，后来重新解析了dns并且取消了cdn才解决，最后发现ssl证书过期了，重新申请后速度变快不少，但今天6月16日push后访问速度又变龟速了不知道为什么，真的崩溃，想充钱给域名备案了。在中国大陆境内就是无论做什么都要比原先的流程要多一步，真的无语。所以这天没有继续美化网站。&lt;/p&gt;
&lt;h3 id=&#34;6月16日&#34;&gt;6月16日&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 主要修改了底部分页栏pagination，修改了pagination的背景使其变为半透明，添加了hover在page-link上使其在js文件的加持下可以跟随鼠标移动，同时挪开鼠标会自动回弹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 优化了修改过程中内部的page-link滑块被父容器pagination遮挡的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 在pagination上添加了keyframe使得光标放上去后数字会q弹地抖动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 将同样的效果施加在sidebar的menu上，取消了原先的x轴移动。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>对时间感知的思考，观看毕导最新视频后的想法</title>
        <link>https://Dendrobium123.github.io/p/%E5%AF%B9%E6%97%B6%E9%97%B4%E6%84%9F%E7%9F%A5%E7%9A%84%E6%80%9D%E8%80%83%E8%A7%82%E7%9C%8B%E6%AF%95%E5%AF%BC%E6%9C%80%E6%96%B0%E8%A7%86%E9%A2%91%E5%90%8E%E7%9A%84%E6%83%B3%E6%B3%95/</link>
        <pubDate>Sun, 11 May 2025 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E5%AF%B9%E6%97%B6%E9%97%B4%E6%84%9F%E7%9F%A5%E7%9A%84%E6%80%9D%E8%80%83%E8%A7%82%E7%9C%8B%E6%AF%95%E5%AF%BC%E6%9C%80%E6%96%B0%E8%A7%86%E9%A2%91%E5%90%8E%E7%9A%84%E6%83%B3%E6%B3%95/</guid>
        <description>&lt;p&gt;在昨晚看了毕导最新的一期视频，主要内容是从你为什么感觉时间越过越开开始，引出韦伯费希纳定理
$$
\Delta p=k\frac{\Delta S}{S}
$$
&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/15a2efec755f08f95da4e8d1f7f2682861fb59be&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;{\displaystyle \Delta S}&#34;
	
	
&gt;为某量实质变化而 Δp&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/0758c326125ad3d8b96e515c7fd69164ec587b81&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;{\displaystyle \Delta p}&#34;
	
	
&gt; 为某量的主观的感觉变化&lt;/p&gt;
&lt;p&gt;两边积分后会得到一个对数关系的式子
$$
p=k\ln(\frac{S}{S_{0}})
$$
从而由这个对数关系推导出我们所生活的世界是一个对数系统的世界，他举了一个非常细思极恐的例子：&lt;/p&gt;
&lt;p&gt;假设一个人从 &lt;strong&gt;4岁开始有记忆&lt;/strong&gt;，那么到他活到80岁时的主观时间中，他人生时间的中点并不是40岁而是18岁。&lt;/p&gt;
&lt;p&gt;这个推导如下：&lt;/p&gt;
&lt;p&gt;主观时间按对数定义为(系数k取1）：
$$
S(T)=ln⁡(T)	\
$$
S(T)是人主观感受到的时间，T是客观世界的时间&lt;/p&gt;
&lt;p&gt;由于&lt;strong&gt;从4岁开始有记忆&lt;/strong&gt;，所以改成从4岁起算：
$$
S(T)=ln⁡(T)−ln(4)=ln⁡(\frac{T}{4})
$$
所以一个人80岁时，他的主观时间长度为：
$$
S_{total}=ln(\frac{80}{4})=ln(20)
$$
找一个年龄T_m，使得从4岁到T_m的主观时间刚好是全部主观人生时间的一半：
$$
ln(\frac{T_m}{4})=\frac{1}{2}⋅ln(\frac{80}{4})
$$
算出来T_m大概是17.89岁，就差不多是18岁左右。&lt;/p&gt;
&lt;p&gt;当我看到这里的时候突然就楞了一下，之后毕导由此展开，逐渐引出我们生活的这个世界本身就是一个对数系统，通过韦伯费希纳定律所得到的对数定义式解释了为什么化学家用log定义ph浓度，为什么天文学家用log定义恒星视星等，什么地质学家用log定义里氏地震等级，归根结底，是因为这种对数定义符合人的直觉，也就是我们所谓的“多么”。大耳朵图图第二季的主题曲唱到“夜晚到底有多黑？世界到底有多大？“对数系统解释的，就是这个”到底有多么“。&lt;/p&gt;
&lt;p&gt;之后毕导又由此展开讲到了本福特定律，即在自然界的对数系统中，”1“这个数字出现的概率是最大的，趋近于30%。&lt;/p&gt;
&lt;p&gt;”这是为什么呢？难道老天爷更中意1而不是9？“不仅如此，我们发现把那些符合本福特定律的大量数字放在线性的坐标系下他们大量密集地集中在坐标轴的前端，而放在对数坐标系下确变得均匀、有规律。&lt;/p&gt;
&lt;p&gt;随后毕导讲出了这个视频最关键的一句话，”符合本福特定律的数字们，大小都跨越好几个数量级”“在自然界中随机演化来的数字大多是平滑且跨越好几个数量级，就不由自主地具有对数尺度均匀分布的特征”&lt;/p&gt;
&lt;p&gt;“物理世界的数值以指数的形式跨越着数量级，而我们的大脑又用对数运算把他们压缩到可感知的空间中”&lt;/p&gt;
&lt;p&gt;“知觉赋予感觉以意义
因此知觉产生的是对世界的解释
而不是对世界的完美表征。
&amp;ndash; 菲利普·津巴多《普通心理学》”&lt;/p&gt;
&lt;p&gt;最后来谈谈我由此期视频得到的启发，以及我同chatgpt老师从信息论的角度探讨这个问题得到的结论。&lt;/p&gt;
&lt;p&gt;首先的问题是人是如何感知时间的，也许我们可以说人是根据我们的记忆来感知时间，因为往往我们回忆某段时光所采用的媒介是那段时间所经历的事情，那么我们又是如何记录记忆的呢？你可以说我们是通过对身体感官所收集的信息做二次处理来记录的，比如我们记住某一时刻听到的话，看到的东西，闻到的气味，甚至当时脑海里所想的东西，这些复杂的信息通过大脑整合在一起构成了记忆。但既然提到了感觉器官，我就想把这个感受过程类比成某种采样，根据奈奎斯特采样定律，我们采样的频率往往需要真实信号的两倍才能保证无失真地记录，而这个采样频率又取决于我们人体的“时钟”类似电子学中的晶振。&lt;/p&gt;
&lt;p&gt;说到这个人体的“时钟”，gpt老师给我推荐了一篇Michel Planat的论文《On the Cyclotomic Quantum Algebra of Time Perception》其中就提到一个非常有趣的观点，planat认为人脑感知时间的过程可以被建模为一种类似于“锁相”的现象，即大脑内部的神经振荡系统与外部世界的节律性刺激（如钟表节奏、心跳、呼吸等）进行相位同步，但这种同步并不是简单的线性机制，而是受到了1/f 噪声调制，从而导致人类时间感知呈现出非线性、波动性与动态不稳定性。&lt;/p&gt;
&lt;p&gt;撇开这些，我想谈谈我的猜想，我如果按照我们对世界的感知是对数系统这个前提的话，关于时间越过越快这个事从信息论角度我的猜想是我们人脑对外部事物的信息熵也符合韦伯费纳西的对数机制，随着年龄的增长，大脑对信息熵的感知范围也在不断扩展，类似频谱拓宽，从而使得同一件事的信息熵在人生的不同时期被感知的大小完全不同。小时候的带宽小，同一件事的主观信息熵大，长大后的带宽更大，主观信息熵变小，类似于功率谱密度的衰减，而人脑类似于一个选择性注意的滤波器，讲那些信息熵衰减从而趋向于背景噪声的平庸事件进行了筛除，如果说信息熵的定义式是-log（）的话，我们的主观信息熵应该是对此再次log。也就是说随着我们的长大，一方面我们的大脑会压缩我们主观对信息熵的感知，另一方面我们甚至还会主动降噪，将那些被压缩的低熵事件从记忆中清理出去从而保证我们对世界的感知，所以从这两个层面来讲，也许当我们和比我们小十岁二十岁的孩子聊天或者玩耍时，我们的“时间”流逝是真的不一样的。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>魔女的夜宴，绫地宁宁线通关感受</title>
        <link>https://Dendrobium123.github.io/p/%E9%AD%94%E5%A5%B3%E7%9A%84%E5%A4%9C%E5%AE%B4%E7%BB%AB%E5%9C%B0%E5%AE%81%E5%AE%81%E7%BA%BF%E9%80%9A%E5%85%B3%E6%84%9F%E5%8F%97/</link>
        <pubDate>Sat, 04 Jan 2025 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E9%AD%94%E5%A5%B3%E7%9A%84%E5%A4%9C%E5%AE%B4%E7%BB%AB%E5%9C%B0%E5%AE%81%E5%AE%81%E7%BA%BF%E9%80%9A%E5%85%B3%E6%84%9F%E5%8F%97/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/3194a89576eb4049a8b6d9cc912c2b0.png" alt="Featured image of post 魔女的夜宴，绫地宁宁线通关感受" /&gt;&lt;p&gt;坦白说，《魔女的夜宴》是我第一部认真游玩的视觉小说（虽然只推了宁宁一条线((因为我是冲着绫地宁宁的知名度才买的游戏(((甚至一开始还买错了游戏))))))，在这之前，我对玩galgame的人是抱有偏见的。这种二次元游戏似乎只是迎合某些人的幻想，用纯粹的媚男套路和虚幻的情节构建一种逃避现实的舒适圈，并不认为这些作品能传递什么真正有价值的情感，更不可能触及像我这样“坚韧不拔的内心深处”。但当我这两天通关了绫地宁宁线后，也许我需要重新审视自己的看法。&lt;/p&gt;
&lt;h3 id=&#34;下面大概介绍一下本作宁宁线的剧情内容和构成故事高潮的终极矛盾&#34;&gt;下面大概介绍一下本作宁宁线的剧情内容和构成故事高潮的终极矛盾&lt;/h3&gt;
&lt;h4 id=&#34;主要背景&#34;&gt;主要背景：&lt;/h4&gt;
&lt;p&gt;大概是男主拥有读取他人心情的超能力，（比如对面的人难过，男主会闻到苦味，生气会闻到一股酸味这样的。。导致性格阴暗低沉）女主绫地宁宁有一段不怎么愉快的童年经历（原生家庭矛盾导致父母离婚以至于影响到了她本人的性格和处事方式：变得圆滑，冷漠，没有朋友之类的）于是为了实现父母和好并拥有愉快童年的愿望和一只有魔力的猫签订契约变成魔女（代价是。。不大方便透露，总之非常戏剧化）&lt;/p&gt;
&lt;h4 id=&#34;主线剧情&#34;&gt;主线剧情：&lt;/h4&gt;
&lt;p&gt;大概是宁宁为了积攒实现愿望的能量创办社团帮助同学，在同学感到满足时收集溢出的精神能量，然后男主在一次与宁宁的戏剧性会面中不小心吸收了这些能量导致宁宁努力白费，背景设定男主由于内心的空洞才导致能量被吸收，于是男主为了弥补过错也是为了改变自己阴暗的性格加入了社团一起帮助同学来和宁宁收集能量，在这个过程中两人长期相处共事，在经历了一系列挑战和事件后共同成长，也发展出了恋爱的感情。&lt;/p&gt;
&lt;h4 id=&#34;终极的矛盾&#34;&gt;终极的矛盾：&lt;/h4&gt;
&lt;p&gt;男主在与宁宁的相处中内心的空洞逐渐被幸福快乐填满，导致原先被吸收的能量一点点回到宁宁原先积攒的容器中，加速了宁宁实现愿望的过程，照理来说这是再好不过的事了，原先男主也是这么认为的。直到剧情揭示宁宁许下的愿望是回到父母离婚前获得一个愉快童年，这意味着宁宁将回到过去，这与当前和男主幸福生活的现实产生矛盾，&lt;strong&gt;换句话说男主和宁宁生活越幸福，他们所相处的时间就越少，未来就越绝望和痛苦&lt;/strong&gt;。故事进入最激烈的情感冲突阶段，宁宁的命运和男主的努力似乎达到了无法调和的矛盾点。超现实设定在此处成为打破二人关系的决定性力量。&lt;/p&gt;
&lt;h4 id=&#34;主线结局&#34;&gt;主线结局&lt;/h4&gt;
&lt;p&gt;再三思考下，二人决定珍惜最后时光，来了一场尽兴的约会。最后在夕阳下的学校活动室内，两人相拥-分别，在分别前男主拜托宁宁将自己的记忆以能量方式一起带走（剧情设定男主用别的方式劝说宁宁导致宁宁并不知道这是男主的记忆），为了在宁宁人生重启后的某一天能重新和她坠入爱河。最后，宁宁发动能力并化作光芒消失了，男主以牺牲自我和情感为代价，让宁宁“自由”或幸福。宁宁消失后，男主和整个世界的记忆也开始发生修正，他不再记得宁宁，好像这个人从未存在过，尽管最后男主偶遇了世界修正后的宁宁，但也只是眼神相视一瞥，然后作品进入开放式结局。&lt;/p&gt;
&lt;h4 id=&#34;restart线剧情&#34;&gt;&lt;strong&gt;restart线剧情&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;这部分故事发生在主线结束，你重新开始游戏时，宁宁回到了过去，并拥有原先的所有记忆，而原先男主的记忆则被压缩为一块晶体一起回到了过去，此时父母虽然不再吵架，但也早无任何感情，而宁宁为了能重新遇见男主，将未来的变数控制在最小，即将原先愿望作废，让父母离婚一切同重来前一样，她考上了原先的学校，租了原先的房子，在忍受着原先记忆所带来的巨大的孤独和绝望中重复之前的生活，但当她再一次遇见男主时，这个世界的男主早却已没有原先的记忆，宁宁在崩溃中却误打误撞让男主接触到了原先的记忆晶体，至此男主记忆恢复，两人终于实现了真正意义上的重逢，迎接他们的是不需要重来的幸福时光。&lt;/p&gt;
&lt;p&gt;好吧，也许你看完以后会觉得又是这种老套剧情，我承认，在2025年的今天，这样的故事对人们来说不再有吸引力了，但值得注意的一点是这个故事流程完美符合一个好故事该有的“八步叙事法”，剧情跌宕起伏，最重要的是，我花了10个小时左右的时间在一天当中充分体验了整个故事，有时一部两小时的电影就足够扣动心弦让人代入其中，何况是整整10个小时的互动作品，想不被感动也很困难吧喂！(っ °Д °;)っ&lt;/p&gt;
&lt;p&gt;好了，接下来我想谈谈我对这个作品的一些思考，毕竟我过完宁宁线的剧情后，用小学写作文的一句话讲“我的心情久久不能平静”，&lt;del&gt;这是真切的感受，当故事结束响起音乐re:start 君とまた出逢えて（与你再次重逢）和滚动cast列表的时候，我真的不争气地流下了眼泪ε(┬┬﹏┬┬)3&lt;/del&gt;，引发我思考的，&lt;/p&gt;
&lt;p&gt;一是本作为什么会给我巨大的震撼和足量的后劲（我现在有时候还是满脑子的宁宁的故事，被疯狂硬控），&lt;/p&gt;
&lt;p&gt;二是从男主心中的“空洞”出发，我想借助最近了解的精神分析中的“对象a”和“欲望”等概念谈谈宁宁和“空洞”的隐喻。&lt;/p&gt;
&lt;p&gt;&lt;del&gt;ps：当然我本人并不是哲学和心理学的科班出身，本文所涉及到的“理论”纯粹为个人兴趣平时瞎看的理解，希望专业人士能指出问题所在以及不要嘲笑本老鼠。/(ㄒoㄒ)/&lt;/del&gt;~~&lt;/p&gt;
&lt;h4 id=&#34;heading&#34;&gt;&lt;/h4&gt;
&lt;h3 id=&#34;玩家的感动与震撼&#34;&gt;&lt;strong&gt;玩家的感动与震撼&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;人的感动可以来自任何地方&#34;&gt;人的感动可以来自任何地方。&lt;/h4&gt;
&lt;p&gt;我过去常常觉得只有那些超现实的宏大情节才能创造夸张和不合常理的戏剧冲突。在这个故事的产出与消耗循环越来越快的时代，作品的冲突只有足够激烈，足够不可思议，似乎才能勉强在现代人见怪不怪，波澜不惊的心中荡起一丝涟漪。但这次长达10个小时的游玩让我也意识到了，不是故事不够离奇，冲突不够强烈，而是我们失去了长时间专注，沉浸在同一个故事中的机会。&lt;/p&gt;
&lt;p&gt;你也许会说，这个galgame又是魔法又是穿越又是契约又是超能力的背景设定难道还不够夸张不够不可思议吗？我不否认这些宏大情节在剧情推动上的作用，但他们更像是锦上添花。对我来说，游戏里第一次遇见宁宁讲话，第一次因为契机加入超自研社团，到后来第一次为了解决川上同学的约会模板问题而和宁宁排练模拟约会，第一次参加宁宁的学习辅导小组，第一次一起举办万圣节party，第一次在宁宁生病的时候看望她，等等等无数个第一次（是的，我全都历历在目，哪怕这只是游戏设定的桥段）&lt;/p&gt;
&lt;p&gt;正是这些日常的不能再日常的桥段以及在这个过程中男女主互相态度的微妙变化乃至表情和行为的微妙变化构成了感动的源泉。是的，这就好像是你在度过另一种人生一样，巨大的文本量和角色语音堆砌起来的不仅是游戏的比特容量，更是接近现实生活点点滴滴的生活的体验，也许是我也曾经有那么一瞬间有那样的怦然心动，有那样的尴尬和无地自容，才能让我在推进故事时有同样发自内心的喜悦和难过。&lt;del&gt;说出来你们可能不信，我是全程在皱眉、痛苦面具和姨母笑的loop中过完了游戏。&lt;/del&gt;&lt;/p&gt;
&lt;h4 id=&#34;情感投射的消失和剥离感满满的后劲&#34;&gt;情感投射的消失和”剥离感“——满满的“后劲”&lt;/h4&gt;
&lt;p&gt;让我们用稍微装b一些的词来进行以下部分的分析：你是否也有过看完一本书或者一部影片后怅然若失，甚至陷入虚无的感受呢？&lt;/p&gt;
&lt;p&gt;虚无，也许是我在游玩后最强烈的情感。当看到宁宁手捧鲜花在万圣节的party上发言的那一刻，我忐忑地敲击鼠标，下一幕，屏幕变得雪白，片尾曲随之响起，cast也慢慢从底部滚动上来。就像电影院里电影结束的一瞬间，周围的灯亮起，观众们拎着不剩几个爆米花的纸筒转身伸起懒腰，保洁阿姨拖着大大的垃圾桶站在出口前一样。“结束了。”10个小时的沉浸已经让我产生了情感投资，就像宁宁在剧情中的消失一样，对我来说，故事的结束意味着我在这段故事中作为旁观者也要消失了，也许他们会继续过着幸福快乐的生活。但这一切与这个屏幕前的人无关。是的，我，与这个世界无关。佛洛依德有提过一个词叫“客体丧失”，指的是个体在情感或心理上失去了某个重要的“客体”——通常指的是亲密关系中的人、事物或信念。也许在潜意识中我认为“宁宁”已经从我的生活中消失了。因为我们在游戏世界中的某种“关系”结束了。尽管我们并不是真的失去了宁宁，只是失去了与她互动和陪伴的过程，失去了情感投射的媒介和对象。&lt;/p&gt;
&lt;p&gt;对于虚无，有人说“人对精神世界的过度探索会让人滑向虚无主义”，在故事结束后对故事过度的挖掘何不算一种过度探索呢，我们希望我们和角色的情感连接是“永恒的”，但游戏和影片作为一个有终点的叙事媒介，注定无法满足这种需求。这种“永恒感“随着游戏结束而破灭，我们无法陪伴他们，甚至无法看到他们幸福的样子，这时意识到我们从头到尾都是个“局外人”。也难怪空虚和怅然若失了。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20250107134847086.png&#34; alt=&#34;image-20250107134847086&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;男主心中的空洞实在界的中心与对象a&#34;&gt;男主心中的“空洞”：实在界的中心与对象a&lt;/h3&gt;
&lt;p&gt;如果你阅读了最前面关于游戏剧情的部分（没读过也不要紧），你就会知道，整个故事的背景中，男主保科（也就是玩家扮演的角色）心中先天性的拥有一个巨大的“空洞”，正是因为这个空洞的存在导致了男主拥有读取他人心情的能力，也导致了他吸收了宁宁收集的能量从而使得主线剧情得以展开。&lt;/p&gt;
&lt;p&gt;现在，让我们将目光汇聚在这个“空洞”上，我们留意到正是这个空洞的存在支撑了男主所有的行为逻辑“空洞”既是他内心深处的不安源泉，又是驱动男主存在的核心动力。如果将“空洞”视为一种隐喻，则可以将其与拉康精神分析中的一些理论联系起来。&lt;/p&gt;
&lt;p&gt;在这之前，有必要简要地介绍和归纳一下拉康在精神分析中的一些基本概念，这里只列举符号界和实在界。&lt;/p&gt;
&lt;h4 id=&#34;首先是符号界&#34;&gt;首先是符号界&lt;/h4&gt;
&lt;p&gt;有些地方也译为象征界，简单的说就是可以被我们用人话描述的东西，即用语言符号编织的世界。当然，由可以被描述的东西就有无法被描述的东西，这种未经过符号处理的原生世界就是实在界。&lt;/p&gt;
&lt;h4 id=&#34;实在界&#34;&gt;实在界&lt;/h4&gt;
&lt;p&gt;实在界喻指语言符号所覆盖不到的地方，它是一种原初状态。在这里，我们稍微举一个让人细思极恐的小栗子：问问你自己，当我们在形容心情悲伤的时候，除了使用”难过“、”悲伤“等名词以及描述哭泣，忧郁等行为的词之外，你还能想到什么？或者说回到一个你还无法说话的时刻，有一种东西涌了上来，它充满了你的身体。它是湿的，是热的，是不受控制的。你想哭泣，但你甚至不知道哭是什么，只是试图发出声音，本能地尖叫或抽泣。你感觉胸口快要裂开了，但你不知道它叫什么，也不知道它来自哪里。当你回望那里，是不是感到了一阵空白和不知所措？你感到焦虑和恐慌，这就是混沌的实在界。&lt;/p&gt;
&lt;h4 id=&#34;对象aobject-a&#34;&gt;对象a(object a)&lt;/h4&gt;
&lt;p&gt;拉康认为在实在界的中心存在着一个永远抗拒符号化的坚硬内核。这个内核是一种空隙和短缺，当我们靠近这个空隙想将其语言化时，总有一堵高墙将我们挡在外面，高墙内部的东西被称为”对象a(object a)“或者原质(the thing)~~（尽管后来齐泽克极力将这两者做了区别，但在这里我们先简要将他们归为一类，因为好像真的差不多。。）~~有时对象a也被解释为“欲望”的对象，此欲望不是指具体的欲望，而是使你产生欲望的念头，比如当你实现一个愿望后，很快又会出现新的欲望，对象a驱使你生成欲望，本质是一种永远无法被真正获得的东西。拉康还认为，每个主体在进入符号界时，由于对象a这堵高墙的影响，都会经历一种“被阉割”的状态。主体不再是完整的自我，而是在语言和他者的框架中构建自身，使得主题的中心形成了一个永远无法填补的“空洞”。注意这个“空洞”，这就是我想表达的”隐喻“。&lt;/p&gt;
&lt;p&gt;”&lt;em&gt;&lt;strong&gt;欲望围绕着它编织自身，而欲望的中心是短缺，是空隙，这一空隙是实在界的剩余。&lt;/strong&gt;&lt;/em&gt;“——《齐泽克精神分析学文论》-赵淳&lt;/p&gt;
&lt;h4 id=&#34;柊史男主的空洞与欲望&#34;&gt;柊史（男主）的“空洞”与欲望&lt;/h4&gt;
&lt;p&gt;在游戏背景中，男主的”空洞“是一种天然的“阉割”，压抑着他作为主体去表达自己情感的能力。空洞最初是不可见的，对保科本人而言更是毫无觉察。他只知道自己拥有某种能力，可以具体感知他人的情感，这让他与周围的世界保持一种微妙的距离感。他既能感知他人，却无法真正与他人连接，这种能力表面上让他“更多地”理解了别人，但实际上也让他离别人“更远”。在第一章中男主的空洞吸收了宁宁收集的能量，随后又被告知只要“空洞”被填满，能量就能返还。换句话说，原先的能量并未消除空洞，只是占据了这个空间。而之后男主和宁宁所做的一切看似是修补这个洞，实际只是将这个空洞进行转移，置换原先的能量。就像欲望永远无法被满足，它永远是匮乏的。&lt;strong&gt;齐泽克曾说“当我们面对欲望的对象时，更多的满足是通过围绕着它(指对象 a)来跳舞而得到的，却不是径直地走向它”&lt;/strong&gt;，或者说我们不是不想走向它，而是根本无法走向它。当保科询问相马（与宁宁签订契约的黑猫，同时也是告知男主空洞存在的人）如何修补空洞时，相马说她也不知道。尽管随着剧情推进，当男主与宁宁相爱，两人互相理解成长，一起迈向幸福时，“空洞”看似得到了修复。但我们都知道，主线的剧情设定有着【“空洞”被修复=幸福生活的崩溃】这一结论。&lt;/p&gt;
&lt;p&gt;在主线剧情的最后，相马给男主的解决方案是，将他的记忆做成能量碎片移交给将要回到过去的宁宁，但这种方法无疑是在男主“即将被修复的空洞”位置再次捅出一个窟窿。可以看到，自始至终，“空洞”从未被修补，所有的幸福和欲望的满足带来的是另一个更大的“空洞”。&lt;/p&gt;
&lt;h3 id=&#34;绫地宁宁代表着什么&#34;&gt;绫地宁宁代表着什么&lt;/h3&gt;
&lt;p&gt;其实这里才是我最想说的地方，也是之前多次提及的矛盾所在：宁宁在这个过程中扮演的角色，一种同时包含吸引与排斥的混合物。这一点不仅体现在她自己身上，还体现在她与男主的人物关系发展上。&lt;/p&gt;
&lt;p&gt;从宁宁的视角来看，作为魔女，维持其魔力和自身存在的根基是她多年前许下的那个幼稚的愿望，而这个愿望与她在当下的幸福是排斥的，即宁宁获得幸福的代价正是她自己。设定上的宁宁拥有着原生家庭的悲剧：年幼时父母感情破裂，离异后，自己跟随父亲生活，在父亲构建再婚家庭时，始终无法接纳为人亲善的继母。“和我缔结契约的时候，宁宁是孤身一人。父母已经离婚了好几年，但她却无法融入新的家庭，仿佛整个世界都没有她的容身之处一样”游戏中的相马做出了这样的补充。与之形成对比的是第六章后和男主顺利成为恋人的宁宁，这时的她不再是孤身一人，既和男主拥有着甜蜜的恋爱关系，又和其乐融融的社团成员组成了知己知彼的小圈子。宁宁也许从没想到后面这一切的发生，过去的她以一种看得见摸得着的方式否定了自己的未来。&lt;/p&gt;
&lt;p&gt;从男主保科的视角来看，宁宁在某种程度上是他的”对象a“的能指。她是保科在游戏后期欲望的驱动力。前面说过，对象a是欲望的原因，而非欲望本身。它是一种永远无法被真正捕获的对象，主体的欲望围绕对象a的缺失而展开。在游戏中，说不上宁宁是保科的“空洞填补者”，但她的存在推动了保科对自我的反思和改变，甚至让他开始主动追求幸福的可能性。然而，随着两人关系的深入，宁宁的不可得性逐渐显现，正如游戏的一条成就讲的那样：绝望的尽头是深爱。爱发生在绝望之前，但在也出现在绝望的背后。绝望的来源是永远的缺失。美好的幻象驱使我们不断接近它，但在一个具体的欲望得到满足后转而又陷入空虚，当我们走向欲望时才发现，它的背后其实什么也没有，真正想要追求的那个东西其实留在了我们的身后，而我们却无法回头。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20250107134746976.png&#34; alt=&#34;image-20250107134746976&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;最后的事情&#34;&gt;最后的事情&lt;/h3&gt;
&lt;p&gt;虽然我前面一直在强调和分析的是关于”越靠近，越远离”这个情节，但本作的另一个核心其实是关于陪伴和成长的。绫地宁宁和保科柊史毫无疑问是有着先天或后天的某种心理缺陷的，他们心中的空洞在互相陪伴的过程中虽说无法被填满，但最后都选择了与这种缺失达成和解。宁宁在restart故事中回到过去，但她对原生家庭破裂的事实却仍表现为不作为，她的空洞并未被填补，她做的只是与自己的和解，包括尝试接受继母的存在、缓和彼此的关系。而保科在restart线中接受了宁宁一并带回的记忆时也对宁宁说“现在的我和曾经你认识的我并不完全是同一个人”。在触碰到记忆的那刻，对于幻象的憧憬使得他做出了正确的选择——阴沉的自己选择了阳光的自己，而阳光的自己选择了与心中的空洞和平共处。两人的幸福表面上是对互相的接纳与支持，实际离不开与自己内心深处的和解。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20250107134815121.png&#34; alt=&#34;image-20250107134815121&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;幸福的结局让人感到失落，除了之前所说的关于情感投射消失和剥离感的原因之外，最根本的原因便是：幸福的结局本就是一个幻象，它有意无意地提醒着我们：有这样一个空洞在我们的心中，重要的不是去填补它，消除它，而是意识到它，感受到它，尝试接受它。真正的关键并不在于幻象背后那个不可触碰的绫地宁宁，而在于我们能否像保科一样，探知自己心中的“空洞”，能否意识到幸福是我们在这个空洞周围的舞蹈，是一种在“未得”中持续追逐的状态。尽管幸福或许不会以我们期待的方式到来，但它在追逐的过程中，始终以一种微光的形式存在。这微光，正是我们生命中最真实、最温暖的部分，它虽不起眼，却悄悄地滋养着那些未曾被满足的愿望和未能被抚平的伤痕。&lt;/p&gt;
&lt;p&gt;ps：我也好想在手指疼的时候被宁宁这么安抚&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20250107134658884.png&#34; alt=&#34;image-20250107134658884&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;再次ps：&lt;/p&gt;
&lt;p&gt;这个场景原型居然是京都站，没想到我也之前也意外的圣地巡礼了，这里用的钱王的照片&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20250107135452345.png&#34; alt=&#34;image-20250107135452345&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20250107135552294.png&#34; alt=&#34;image-20250107135552294&#34; style=&#34;zoom:50%;&#34; /&gt;
</description>
        </item>
        <item>
        <title>控制论Norbert Wiener读书笔记</title>
        <link>https://Dendrobium123.github.io/p/%E6%8E%A7%E5%88%B6%E8%AE%BAnorbert-wiener%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Wed, 01 Jan 2025 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E6%8E%A7%E5%88%B6%E8%AE%BAnorbert-wiener%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;p&gt;这本书属于通识类，没什么数学公式，读起来比其他专业书容易许多&lt;/p&gt;
&lt;h3 id=&#34;序章&#34;&gt;序章&lt;/h3&gt;
&lt;h5 id=&#34;时不变系统&#34;&gt;时不变系统&lt;/h5&gt;
&lt;p&gt;相对于原点变化具有时间上的不变性，比如一个电路实验你无论是在什么时候做，它的现象和持续时长都是恒定的不会今天一个样明天另一个样，这就是时不变性。&lt;/p&gt;
&lt;p&gt;而三角函数sinnt与cosnt就是对同一平移群下某种重要不变量，而一般函数$$e^{iw(t+τ)} =e^{iwt}e^{iwτ}$$由欧拉公式可知其本质与三角函数形式相同，对其时间上的平移只是改变其前面的系数不改变形式。&lt;/p&gt;
&lt;h5 id=&#34;分形结构&#34;&gt;分形结构&lt;/h5&gt;
&lt;p&gt;“在生物学上模拟某些反映生命核心现象的内容。遗传之所以成为可能，细胞之所以能繁殖，必须靠细胞中携带的遗传信息的成分能够构建出与其自身类似的携带遗传信息的结构。“在工程领域可以使用机器的功能元件为模板制作另一个类似元件类似遗传学中的基因，作者的想法是通过频率来实现，比如分子光谱的频率来作为携带生物特性的模板，基因的自组织性可能是频率自组织性的一种表现。&lt;/p&gt;
&lt;p&gt;作者在1961年就提出了学习机（机器学习）的实现可能&lt;/p&gt;
&lt;h3 id=&#34;导言&#34;&gt;导言&lt;/h3&gt;
&lt;p&gt;科学在各个领域都呈现出日渐狭窄的趋势即再也不会有全能的科学家可以不受限制地称自己既是数学家、物理学家或生物学家了。尽管如此，但在这些科学家遇到与自己领域相邻的领域时只能以自己的见解命名自己的发现，全然不顾本身从属于这一领域的人的意见，这就导致了在某些学科出现了命名和立法纠缠不清的状态，同一件事物在数学，统计学，电气工程领域有着不同的叫法。因此，现代科学对空白领域的探索需要的是一个全能的科学家团队。&lt;/p&gt;
&lt;h5 id=&#34;微分器&#34;&gt;微分器&lt;/h5&gt;
&lt;p&gt;求解偏微分方程的计算机，通过引入电视的扫描过程解决了原先复杂的多变量函数表示问题，而扫描过程会带来大量的数据量，作者和布什博士通过以下方法解决这一问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用数字的加法和乘法装置，放弃布什微分器的以度量为基础。&lt;/li&gt;
&lt;li&gt;通过电子管而不是机械驱动来使运算更快速&lt;/li&gt;
&lt;li&gt;根据贝尔实验室的经验，在仪器上采用二进制而不是十进制，更为经济&lt;/li&gt;
&lt;li&gt;整个操作顺序由机器本身执行&lt;/li&gt;
&lt;li&gt;内置存储装置确保快速记录可靠数据和擦除&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;内置在飞机和飞弹的控制装置预测物体的运动状态&#34;&gt;内置在飞机和飞弹的控制装置预测物体的运动状态&lt;/h5&gt;
&lt;p&gt;飞机和导弹的运动速度之快无法做到时时刻刻精确地计算出当前状态。”要对曲线的未来进行预测，就要对其过去进行某种运算“&lt;/p&gt;
&lt;h5 id=&#34;控制工程的反馈&#34;&gt;控制工程的反馈&lt;/h5&gt;
&lt;p&gt;”如果我们希望一个运动遵循给定模式，应将此模式与实际运动间的差值当作新输入值去调节该运动，使受控部件以更接近给定模式的路径“举例：船舶上的转向舵&lt;/p&gt;
&lt;h5 id=&#34;反馈&#34;&gt;反馈&lt;/h5&gt;
&lt;p&gt;作者举例：人要拿起一只笔，无需单独命令每块参与活动的肌肉和骨骼进行收缩，而是让整个身体和神经系统作为一个整体进行。&lt;/p&gt;
&lt;p&gt;“整个过程简单地说就是，尚未捡起的铅笔的体积量会在各个阶段逐步减少，且这一部分动作并非完全有意识的”&lt;/p&gt;
&lt;h5 id=&#34;本体感受&#34;&gt;本体感受&lt;/h5&gt;
&lt;p&gt;即通过视觉、触觉等人体传感器反馈回神经系统的信号量，没有这些我们无法完成捡起笔这一动作，且人会处于“共济失调”（一种行为笨拙，缺乏规律的神经疾病），作者指出这种神经疾病的缘由是因为脊髓神经传递的运动感觉被破坏。&lt;/p&gt;
&lt;h5 id=&#34;过度反馈与癫痫&#34;&gt;过度反馈与癫痫&lt;/h5&gt;
&lt;p&gt;作者提出过度反馈可能与不完全反馈一样对机体产生严重阻碍。提出一个假设“是否存在一种病理状态：患者在尝试某些自发行为时，超出标记范围，进入一种无法控制的振荡”&lt;/p&gt;
&lt;p&gt;作者提到他们的观点远超当时神经生理学家流行的观点&amp;mdash;&lt;strong&gt;中枢神经系统不再是接受来自感官的信息输入并将其释放到肌肉的独立器官，相反，它的一些最典型的活动只能解释为循环过程：从神经系统进入肌肉，再通过感觉器官重新进入神经系统。&lt;/strong&gt;&lt;/p&gt;
&lt;h5 id=&#34;建立工程设计科学&#34;&gt;建立工程设计科学&lt;/h5&gt;
&lt;p&gt;对最优预测问题原本难以解决的问题通过引入对预测的均方误差的显式表达式从而转化为最小化问题，进而转变为数学问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;滤波器设计中的最小化思想体现：滤波器设计问题就是通过被背景噪声干扰损坏的信息的算符来恢复原始信息。其最优设计取决于信息和噪声各自以结合在一块后的统计性质。由此将通信工程设计变成了一门统计科学，变为统计力学的分支。&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;基于信息量的统计科学通信工程&#34;&gt;基于信息量的统计科学通信工程&lt;/h5&gt;
&lt;p&gt;作者与统计学家费雪，贝尔实验室的香农博士同时产生了这个想法（真的牛b），三人产生这个想法的缘由各不相同&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;费雪的动机来源于经典统计理论&lt;/li&gt;
&lt;li&gt;香农的动机来源于信息编码的相关观点&lt;/li&gt;
&lt;li&gt;作者维纳的动机来源于电子滤波器中的噪声与信息的观点&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;信息熵&#34;&gt;信息熵&lt;/h5&gt;
&lt;p&gt;信息量是对系统有序程度的度量，系统的熵则是对系统无序程度的度量，两者互为对方的负数。&lt;/p&gt;
&lt;p&gt;此类问题引导作者等人向热力学第二定律（：孤立系统自发向热力学平衡方向演进）进行思考，此类问题在生命科学领域也有研究即生命的第三种基本现象：应激性（属于传播理论的范畴）&lt;/p&gt;
&lt;h5 id=&#34;控制论学科的诞生&#34;&gt;控制论学科的诞生&lt;/h5&gt;
&lt;p&gt;作者和罗森布鲁斯博士的科学家小组在多年前就意识到以通信工程、控制论和统计力学为中心的一系列问题在本质上是统一的，无论是在机械还是在生物组织中。于是为了填补这块领域的空白，作者等人将这个既是机器又是生物控制和通信理论的整个领域成为&lt;strong&gt;控制论&lt;/strong&gt;，整个名称来源于希腊语”舵手“，选择这个名称是为了纪念关于反馈机制的论文（由麦克斯韦于1868年发表，是一篇关于调速器的文章），作者还希望提及这样一个事实，船舶的舵机是最早出现且发展最完善的一种反馈机制。&lt;/p&gt;
&lt;h5 id=&#34;作者心中的守护神&#34;&gt;作者心中的守护神&lt;/h5&gt;
&lt;p&gt;“如果让我从科学史中选择一位控制论的守护神，我会选择莱布尼茨。他的哲学围绕两个密切相关的概念&amp;mdash;-普遍符号论和推理演算概念展开。当今的数学符号和符号逻辑从中衍生而来。”&lt;/p&gt;
&lt;h5 id=&#34;控制论创建初期&#34;&gt;控制论创建初期&lt;/h5&gt;
&lt;p&gt;大师云集，有图灵，香农，作者，沃尔特皮茨等。皮茨很早就开始研究突触将神经纤维连接到具有给定综合性能的系统的问题，增加了香农早期工作中不突出的因素。图灵的思想（用时间作为参数，考虑含有循环的网络以及突触和其他延迟）启发了他们。&lt;/p&gt;
&lt;h5 id=&#34;真空管和电子管&#34;&gt;真空管和电子管&lt;/h5&gt;
&lt;p&gt;现代真空管是实现神经电路和系统等效的理想方法。神经元放电的全有或全无特性恰好类似在二进制标度上确定一个数字时所做的单一选择。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>突然打开blog发现图片居然全都崩坏？！</title>
        <link>https://Dendrobium123.github.io/p/%E7%AA%81%E7%84%B6%E6%89%93%E5%BC%80blog%E5%8F%91%E7%8E%B0%E5%9B%BE%E7%89%87%E5%B1%85%E7%84%B6%E5%85%A8%E9%83%BD%E5%B4%A9%E5%9D%8F/</link>
        <pubDate>Fri, 26 Jul 2024 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E7%AA%81%E7%84%B6%E6%89%93%E5%BC%80blog%E5%8F%91%E7%8E%B0%E5%9B%BE%E7%89%87%E5%B1%85%E7%84%B6%E5%85%A8%E9%83%BD%E5%B4%A9%E5%9D%8F/</guid>
        <description>&lt;p&gt;​		本来想传几张照片到博客的，结果从日本回来发现博客的照片全都裂了，一打开七牛云才发现原来是ssl证书过期了，没办法只能重新申请一个了。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240728201600345.png&#34; alt=&#34;image-20240728201600345&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;点开发现证书已过期。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240728212705397.png&#34; alt=&#34;image-20240728212705397&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;还好，七牛云上可以免费申请TrustAsia的免费域名证书。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240728213319088.png&#34; alt=&#34;image-20240728213319088&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;填完信息以后，只要在自己申请域名的云那边解析一下就可以用了。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240728213447056.png&#34; alt=&#34;image-20240728213447056&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;虽然不知道为什么，我一开始复制记录值里的3b5835&amp;hellip;这行数字的时候，粘贴出来总是不对，感觉像是加密了一样，最后还是手打出来的，有没有懂哥能告诉我为什么。&lt;/p&gt;
&lt;p&gt;最后吐槽一下，之前七牛云的机器人系统帮我申请的那个ssl证书，信息填的也太随便了。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240728213713661.png&#34; alt=&#34;image-20240728213713661&#34; style=&#34;zoom:50%;&#34; /&gt;
</description>
        </item>
        <item>
        <title>突然的想法</title>
        <link>https://Dendrobium123.github.io/p/%E7%AA%81%E7%84%B6%E7%9A%84%E6%83%B3%E6%B3%95/</link>
        <pubDate>Sat, 01 Jun 2024 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E7%AA%81%E7%84%B6%E7%9A%84%E6%83%B3%E6%B3%95/</guid>
        <description>&lt;p&gt;突然翻到了以前高三整理的文档，那个时候痴迷于西西弗斯和存在主义，也许有时候我的想法已经在不知不觉间被这种哲学思想所影响。&lt;/p&gt;
&lt;p&gt;我还记得去年23年的6月几号来着（我给忘了，我受力王邀请去莲花路吃回转寿司，那天吃完饭后我们几个一边散步一边聊天，似乎两三个钟头把小半个莘庄都给逛完了，力王和段带我灵活地穿梭于那几条对他们来说无比熟悉对我却略显陌生的小道。段离开后，我又和力王骑着共享单车四处溜达，从过往回忆到如今生活经历，从哪个女孩好看到哲学意识形态。我问力王“地铁一号线什么时候关闭？”力王回答我说在双休日可以运营至23点，我没做思考便记下了。&lt;/p&gt;
&lt;p&gt;而当我来到莲花路地铁站时却被通知最后一班富锦路方向的列车已经离开，站点马上要停运了。&lt;/p&gt;
&lt;p&gt;我突然意识到，坏了，这里距离学校有接近25公里路程呢！嘿，天呐，我该怎么办才好？&lt;/p&gt;
&lt;p&gt;&lt;em&gt;忘了说了，当时的第二天我需要在早晨6点30在学校的某个门口准时登上发车前往闵行航天研究院实习的校巴，我当时还想着晚上早点入睡呢！&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;怎么办？打车？我一看打车的报价，接近100多人民币，不不不，我本来就是因为力王请客吃饭才赶来的，如果这会打车岂不是要我多出一份莫名其妙的饭钱吗？不，不行。&lt;/p&gt;
&lt;p&gt;那，骑共享单车？也许这是个好主意？我一看地图软件的预估时间，接近2个小时，我想：两个小时，现在是10点出头（也许是这个时间），这么看来回去也不过12点多而已，可能是个比较好的选择相比于打车？&lt;/p&gt;
&lt;p&gt;我扫了车，沿着地铁路线开始骑行，我当时的心态其实并没有那么多的怨气，我是这样想的，这不是一次夜骑上海的机会吗，你见过夜晚的上海吗？我不是指那种充满打了鸡血的年轻人蹦迪的夜生活上海，而是属于普通人的，一个平常上下班时可能有很多汽车拥堵而错过沿途风景的上海，我相信不会有人无聊到在各种高架桥上度过上海的夜生活。&lt;/p&gt;
&lt;p&gt;我骑啊骑。心情从原来的焦虑逐渐变得平缓，先是沿着地铁路线骑，然后跟随导航进入大马路，路过各种小区和门口的商铺，骑的口渴了就下来找个全家买瓶矿泉水，10:40左右的时候各种小区商品房的灯还熙熙攘攘地亮着，随后逐渐变的黑暗。我路过1号线的地铁口，远远看到地铁站的灯光本来还有一丝侥幸想坐个地铁，骑近一看卷帘门都拉上了，我看我还是慢慢骑吧。晚上的上海公路其实还蛮热闹的，有类似测绘员一样的工作人员会拿着经纬仪一样的仪器不知道在做什么的，也有给路面磨淡的标识重新上漆修复的，还有开着柏油车给破损的路面重新上沥青的，这些事件在平时根本见不到吧，我当时越来越觉得，也许今天这个拍脑袋的决定搞不好是个很有意思的选择。我路过长宁区古北的某条渡过苏州河的高架桥时，我一开始没有走桥，结果被苏州河挡住了去路，当我回头原路返回大桥入口时，我看到旁边的几家酒吧门口居然有几个老头盘腿坐在路边，我还以为是和尚在打坐。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/58b611b5eecf72996bc634293ef7dfc.jpg&#34; alt=&#34;58b611b5eecf72996bc634293ef7dfc&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/535c95fc04e5cc99340cd81c4a8d9f8.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;535c95fc04e5cc99340cd81c4a8d9f8&#34;
	
	
&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;后面的事我有点记不大清了，我只记得大渡河路好长好长好长，明明坐地铁的时候一下就坐过去了，但骑行的时候却超级折磨，导航不停地说还有5km，还有2km&amp;hellip;在普陀区和宝山区的交界时候还给我导航到一个建筑工地里了，后面又给我导航到一个地下隧道，那个时候已经接近1点了，我也处于一种迷迷糊糊的状态，我记得我还在宝山区一块没怎么开发的地上见到了类似大桥的桥墩一样的建筑，好吧，超级奇妙的感觉。&lt;/p&gt;
&lt;p&gt;直到最后回到学校，已然到了凌晨2点，结果发现我的室友们居然没一个在睡觉，很强。&lt;/p&gt;
&lt;p&gt;之所以相隔接近一年才记录这件事情，&lt;del&gt;纯粹是因为我懒&lt;/del&gt;，是因为突发奇想想翻以前高中用的讯飞语记，结果翻到了之前记录的各种句子，看到那句&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“&lt;em&gt;穷尽现在不欲其所无，穷尽其所有重要的不是生活的最好，而是生活得最多。&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;没有任何一种命运是对人的惩罚，只要竭尽全力去穷尽它就应该是幸福的&lt;/em&gt; 。”&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;heading&#34;&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/6b445ef4dce62aa8f44ed5400c0c072.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;6b445ef4dce62aa8f44ed5400c0c072&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>基于PReNet渐近递归网络与gam注意力机制的图像去雨</title>
        <link>https://Dendrobium123.github.io/p/%E5%9F%BA%E4%BA%8Eprenet%E6%B8%90%E8%BF%91%E9%80%92%E5%BD%92%E7%BD%91%E7%BB%9C%E4%B8%8Egam%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8/</link>
        <pubDate>Fri, 24 May 2024 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E5%9F%BA%E4%BA%8Eprenet%E6%B8%90%E8%BF%91%E9%80%92%E5%BD%92%E7%BD%91%E7%BB%9C%E4%B8%8Egam%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%9B%BE%E5%83%8F%E5%8E%BB%E9%9B%A8/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/image-20240524215731380.png" alt="Featured image of post 基于PReNet渐近递归网络与gam注意力机制的图像去雨" /&gt;&lt;h3 id=&#34;背景摘要&#34;&gt;背景+摘要&lt;/h3&gt;
&lt;p&gt;图像作为视觉信息的主要传播媒介，在拍摄过程中常常会受到如雨、雾、雪等 外界因素的干扰，这些因素会显著降低图像的质量或掩盖关键信息，进而影响图像 的后续处理和应用。图像去雨便成为了亟需解决的问题，其主要挑战包括准确识别 各种大小和形状的雨滴，并在保持图像细节的同时去除这些干扰元素。因为除雨对于确保图像信息的准确传达至关重要，尤其是在交通监控和自动驾驶系统等对视觉精度要求较高的应用中。在雨天拍摄的未经处理的图像可能会导致视觉系统错误识别或遗漏关键信息，从而影响决策和操作的安全性和效率。&lt;/p&gt;
&lt;p&gt;在图像去雨的研究中，传统方法和基于深度学习的策略是两种主流技术。&lt;/p&gt;
&lt;p&gt;而随着深度学习今年 来在计算视觉领域的逐步应用，基于深度学习的去雨技术已成为近年来的研究热点。 这种方法利用大规模数据集对深度网络模型进行训练，然后将训练好的模型应用于 待处理的图像，从而有效地去除图像中的雨滴。&lt;/p&gt;
&lt;p&gt;本论文围绕对单画幅图像去雨展开研究，采用深度学习的方法，在PReNet算法 基础上提出了一种通过集成GAM注意力机制来增强去雨性能的新方法。PReNet是 一个高效的递归网络，专注于去除图像中的雨滴，但在处理复杂场景时仍存在一些 局限性。为了解决这一问题，本文在每个残差模块后引入了GAM注意力机制，通 过加强网络对有雨区域的感知能力，从而更精准地去除雨迹。本文在Rain100L和 Rain1400 等主流去雨数据集上进行了广泛的实验，实验结果显示，与原始PReNet模 型相比，集成了GAM注意力机制的新模型在峰值信噪比（PSNR）和结构相似性指 数（SSIM）上都有显著提高。结果证明了GAM注意力机制在去雨任务中的有效性 和潜在的应用价值。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240525000452595.png&#34; alt=&#34;image-20240525000452595&#34; style=&#34;zoom:80%;&#34; /&gt;&lt;/center&gt;
&lt;h3 id=&#34;prenet是什么&#34;&gt;&lt;strong&gt;PReNet是什么&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;以下为PReNet论文原文：&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ccc; padding: 10px; border-radius: 5px;&#34;&gt;
  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1901.09221&#34;&gt;[1901.09221] Progressive Image Deraining Networks: A Better and Simpler Baseline (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;PReNet是一种渐进递归网络，通过在基于ResNet的网络中引入递归层，以及将阶段性结果和原始多雨图像作为每个ResNet的输入，来提高去雨的性能。该网络能够有效地去除雨滴，对于图像去雨领域的研究具有重要意义，并可作为未来图像去雨研究的合适基础。&lt;/p&gt;
&lt;p&gt;PReNet，全称为Progressive Recurrent Network，是一个专门为单图像去雨任务设计的深度学习架构。这种网络采用了递归神经网络的设计思想，通过逐步精细化的方式去除图像中的雨滴，逐渐恢复干净的背景图像。本文选取PReNet模型作为图像去雨算法的基本框架，接下来对PReNet网络结构进行介绍。&lt;/p&gt;
&lt;h3 id=&#34;prenet结构&#34;&gt;PReNet结构&lt;/h3&gt;
&lt;h4 id=&#34;1-输入层&#34;&gt;(1) 输入层&lt;/h4&gt;
&lt;p&gt;输入卷积层由一个核大小为3*3、填充大小为1的标准卷积层构成，其后跟随有ReLU非线性激活函数,该层输入为6通道,输出为32通道。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;2-循环层&#34;&gt;(2) 循环层&lt;/h4&gt;
&lt;p&gt;循环层使用的是LSTM(Long short-term memory),LSTM结构有两个传输状态$c^t$和$h^t$。通过前一个阶段传递下来的状态 $h^{t-1}$以及当前的输入状态 $x^t$拼接训练得到四个状态。LSTM的内部结构如下图所示,图中第一个公式得到的$c^t$是传给下一个阶段的状态，$h^t$表示该阶段的隐藏状态，$y^t$表示输出。
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524221149862.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;⊙表示表示矩阵中对应元素相乘的操作,而相乘的两个矩阵为同型矩阵,⊕表示矩阵加法操作。LSTM结构内部有忘记、选择记忆和输出这三个阶段。&lt;/p&gt;
&lt;p&gt;LSTM的输入为64通道，输出为32通道,卷积核大小为3*3,填充大小为1。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv_g&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tanh&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv_o&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;3-resblocks&#34;&gt;(3) ResBlocks&lt;/h4&gt;
&lt;p&gt;ResBlocks是通过递归展开一个残差块5次来实现的。每个这样的残差块包括两个普通的卷积层，每层后接一个ReLU激活函数,ResBlocks中的每个卷积层输入输出通道数均为32,核大小为3*3,填充大小为1。整个ResBlocks的结构如图3.2所示,图中的5个ResBlock具有相同结构,且参数共享。而单个残差块以残差块1为例，结构如下方代码所示。由于整个模型网络参数主要来自ResBlocks,因此使得模型整体大小由于重复计算而显著减小。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;res_conv1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524221312386.png&#34; style=&#34;zoom:67%;&#34; /&gt;&lt;/center&gt;
&lt;h4 id=&#34;4-输出层&#34;&gt;(4) 输出层&lt;/h4&gt;
&lt;p&gt;输出层是由一个核大小为3*3的标准卷积层构成,填充大小为1,输入为32通道,输出为3通道。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stride&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;5-损失函数&#34;&gt;(5) 损失函数&lt;/h4&gt;
&lt;p&gt;在网络训练方面,PReNet使用了负结构相似性(-SSIM)损失函数。&lt;/p&gt;
&lt;h4 id=&#34;6-总体结构&#34;&gt;(6) 总体结构&lt;/h4&gt;
&lt;p&gt;PReNet模型的6个阶段共享相同的网络参数,将一个ResBlock在一个阶段内反复展开5次,以显著减少网络的参数。PReNet模型包含四个部分:输入卷积层、循环层、ResBlocks和输出卷积层,其整体结构如图3.3所示,左边的图为单个阶段的具体结构,右边图为PReNet模型的整体结构。PReNet模型阶段t的结构的推导公式如下:
$$
x^{t-0.5} =f_{\text{in}}(x^{t-1}, y), \\
s^t = f_{\text{recurrent}}(s^{t-1}, x^{t-0.5}), \\
x^t = f_{\text{out}}(f_{\text{res}}(s_t)).
$$
其中, $f_{\text{in}}$$x^{t-1} $和待去雨图像 y 的拼接结果,第一阶段的输入是两个待去雨图像的拼接, $x^{t-0.5}$ 即为输入层的输出, $s^{t-1}$为上一阶段循环层$LSTM$ 传递过来的信息,$f_{\text{recurrent}} $ 表示循环层的作用,$s^t$表示当前阶段循环层的输出,$f_{\text{res}}$表示$Resblocks$, $f_{\text{out}} $表示输出层, $x^t$表示当前阶段的去雨结果。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524221729723.png&#34; alt=&#34;image-20240524221729723&#34; style=&#34;zoom:80%;&#34; /&gt;&lt;/center&gt;
&lt;h3 id=&#34;gam模块&#34;&gt;GAM模块&lt;/h3&gt;
&lt;p&gt;论文《Global Attention Mechanism: Retain Information to Enhance Channel-Spatial Interactions》&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ccc; padding: 10px; border-radius: 5px;&#34;&gt;
  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.05561&#34;&gt;[2112.05561] Global Attention Mechanism: Retain Information to Enhance Channel-Spatial Interactions(arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id=&#34;1作用&#34;&gt;1、作用&lt;/h4&gt;
&lt;p&gt;这篇论文提出了全局注意力机制（Global Attention Mechanism, GAM），旨在通过保留通道和空间方面的信息来增强跨维度交互，从而提升深度神经网络的性能。GAM通过引入3D排列与多层感知器（MLP）用于通道注意力，并辅以卷积空间注意力子模块，提高了图像分类任务的表现。该方法在CIFAR-100和ImageNet-1K数据集上的图像分类任务中均稳定地超越了几种最新的注意力机制，包括在ResNet和轻量级MobileNet模型上的应用。&lt;/p&gt;
&lt;h4 id=&#34;2机制&#34;&gt;2、机制&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524222051713.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20240524222051713&#34;
	
	
&gt;
$$
F_2 = M_c(F_1) \otimes F_1
\\
F_3 = M_s(F_2) \otimes F_2
$$
1、&lt;strong&gt;通道注意力子模块&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;利用3D排列保留跨三个维度的信息，并通过两层MLP放大跨维度的通道-空间依赖性。这个子模块通过编码器-解码器结构，以一个缩减比例r（与BAM相同）来实现。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524222841628.png&#34; alt=&#34;image-20240524222841628&#34; style=&#34;zoom:80%;&#34; /&gt;&lt;/center&gt;
&lt;p&gt;2、&lt;strong&gt;空间注意力子模块&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;为了聚焦空间信息，使用了两个卷积层进行空间信息的融合。同时，为了进一步保留特征图，移除了池化操作。此外，为了避免参数数量显著增加，当应用于ResNet50时，采用了分组卷积与通道混洗。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524222939131.png&#34; alt=&#34;image-20240524222939131&#34; style=&#34;zoom:80%;&#34; /&gt;&lt;/center&gt;
&lt;h4 id=&#34;3独特优势&#34;&gt;3、独特优势&lt;/h4&gt;
&lt;p&gt;1、&lt;strong&gt;效率与灵活性&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;GAM展示了与现有的高效SR方法相比，如IMDN，其模型大小小了3倍，同时实现了可比的性能，展现了在内存使用上的高效性。&lt;/p&gt;
&lt;p&gt;2、&lt;strong&gt;动态空间调制&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;通过利用独立学习的多尺度特征表示并动态地进行空间调制，GAM能够高效地聚合特征，提升重建性能，同时保持低计算和存储成本。&lt;/p&gt;
&lt;p&gt;3、&lt;strong&gt;有效整合局部和非局部特征&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;GAM通过其层和CCM的结合，有效地整合了局部和非局部特征信息，实现了更精确的图像超分辨率重建。&lt;/p&gt;
&lt;h4 id=&#34;4代码&#34;&gt;4、代码&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;46
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;47
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;48
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;49
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;50
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;51
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;52
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.nn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;GAM_Attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GAM_Attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 通道注意力子模块&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;channel_attention&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 降维，减少参数数量和计算复杂度&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 非线性激活&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 升维，恢复到原始通道数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 空间注意力子模块&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spatial_attention&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 使用7x7卷积核进行空间特征的降维处理&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 批归一化，加速收敛，提升稳定性&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 非线性激活&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 使用7x7卷积核进行空间特征的升维处理&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 批归一化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 输入张量的维度信息&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 调整张量形状以适配通道注意力处理&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x_permute&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;permute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 应用通道注意力，并恢复原始张量形状&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x_att_permute&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;channel_attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_permute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 生成通道注意力图&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x_channel_att&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_att_permute&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;permute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 应用通道注意力图进行特征加权&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_channel_att&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 生成空间注意力图并应用进行特征加权&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x_spatial_att&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spatial_attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;out&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_spatial_att&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;out&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 示例代码：使用GAM_Attention对一个随机初始化的张量进行处理&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 随机生成输入张量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取输入张量的维度信息&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;net&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GAM_Attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 实例化GAM_Attention模块&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;net&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 通过GAM_Attention模块处理输入张量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 打印输出张量的维度信息&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;se-net模块&#34;&gt;SE Net模块&lt;/h3&gt;
&lt;p&gt;论文《Squeeze-and-Excitation Networks》&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ccc; padding: 10px; border-radius: 5px;&#34;&gt;
  &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1709.01507&#34;&gt;[1709.01507] Squeeze-and-Excitation Networks(arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id=&#34;1作用-1&#34;&gt;1、作用&lt;/h4&gt;
&lt;p&gt;SE 模块通过引入一个新的结构单元——“Squeeze-and-Excitation”（SE）块——来增强卷积神经网络的代表能力。是提高卷积神经网络（CNN）的表征能力，通过显式地建模卷积特征通道之间的依赖关系，从而在几乎不增加计算成本的情况下显著提升网络性能。SE模块由两个主要操作组成：压缩（Squeeze）和激励（Excitation）&lt;/p&gt;
&lt;h3 id=&#34;2机制-1&#34;&gt;2、&lt;strong&gt;机制&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1、压缩操作：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SE模块首先通过全局平均池化操作对输入特征图的空间维度（高度H和宽度W）进行聚合，为每个通道生成一个通道描述符。这一步有效地将全局空间信息压缩成一个通道向量，捕获了通道特征响应的全局分布。这一全局信息对于接下来的重新校准过程至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2、激励操作：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在压缩步骤之后，应用一个激励机制，该机制本质上是由两个全连接（FC）层和一个非线性激活函数（通常是sigmoid）组成的自门控机制。第一个FC层降低了通道描述符的维度，应用ReLU非线性激活，随后第二个FC层将其投影回原始通道维度。这个过程建模了通道间的非线性交互，并产生了一组通道权重。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3、特征重新校准：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;激励操作的输出用于重新校准原始输入特征图。输入特征图的每个通道都由激励输出中对应的标量进行缩放。这一步骤有选择地强调信息丰富的特征，同时抑制不太有用的特征，使模型能够专注于任务中最相关的特征。&lt;/p&gt;
 &lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524230759056.png&#34; alt=&#34;image-20240524230759056&#34; style=&#34;zoom:80%;&#34; /&gt;&lt;/center&gt;
&lt;h3 id=&#34;3独特优势-1&#34;&gt;3、&lt;strong&gt;独特优势&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;1、&lt;strong&gt;通道间依赖的显式建模&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;SE Net的核心贡献是通过SE块显式建模通道间的依赖关系，有效地提升了网络对不同通道特征重要性的适应性和敏感性。这种方法允许网络学会动态地调整各个通道的特征响应，以增强有用的特征并抑制不那么重要的特征。&lt;/p&gt;
&lt;p&gt;2、&lt;strong&gt;轻量级且高效&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;尽管SE块为网络引入了额外的计算，但其设计非常高效，额外的参数量和计算量相对较小。这意味着SENet可以在几乎不影响模型大小和推理速度的情况下，显著提升模型性能。&lt;/p&gt;
&lt;p&gt;3、&lt;strong&gt;模块化和灵活性&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;SE块可以视为一个模块，轻松插入到现有CNN架构中的任何位置，包括ResNet、Inception和VGG等流行模型。这种模块化设计提供了极大的灵活性，使得SENet可以广泛应用于各种架构和任务中，无需对原始网络架构进行大幅度修改。&lt;/p&gt;
&lt;p&gt;4、&lt;strong&gt;跨任务和跨数据集的泛化能力&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;SENet在多个基准数据集上展现出了优异的性能，包括图像分类、目标检测和语义分割等多个视觉任务。这表明SE块不仅能提升特定任务的性能，还具有良好的泛化能力，能够跨任务和跨数据集提升模型的效果。&lt;/p&gt;
&lt;p&gt;5、&lt;strong&gt;增强的特征表征能力&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;通过调整通道特征的重要性，SENet能够更有效地利用模型的特征表征能力。这种增强的表征能力使得模型能够在更细粒度上理解图像内容，从而提高决策的准确性和鲁棒性。&lt;/p&gt;
&lt;h3 id=&#34;4代码-1&#34;&gt;4、&lt;strong&gt;代码：&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.nn&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;SEAttention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 初始化SE模块，channel为通道数，reduction为降维比率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;channel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reduction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;avg_pool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;AdaptiveAvgPool2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 自适应平均池化层，将特征图的空间维度压缩为1x1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sequential&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 定义两个全连接层作为激励操作，通过降维和升维调整通道重要性&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;channel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;channel&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reduction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 降维，减少参数数量和计算量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReLU&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# ReLU激活函数，引入非线性&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;channel&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reduction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;channel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 升维，恢复到原始通道数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Sigmoid激活函数，输出每个通道的重要性系数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 权重初始化方法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;init_weights&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;modules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 遍历模块中的所有子模块&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;isinstance&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 对于卷积层&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kaiming_normal_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mode&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;fan_out&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 使用Kaiming初始化方法初始化权重&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 如果有偏置项，则初始化为0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;isinstance&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 对于批归一化层&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 权重初始化为1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 偏置初始化为0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;isinstance&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 对于全连接层&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.001&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 权重使用正态分布初始化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;constant_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 偏置初始化为0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 前向传播方法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取输入x的批量大小b和通道数c&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;avg_pool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 通过自适应平均池化层后，调整形状以匹配全连接层的输入&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 通过全连接层计算通道重要性，调整形状以匹配原始特征图的形状&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand_as&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 将通道重要性系数应用到原始特征图上，进行特征重新校准&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 示例使用&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 随机生成一个输入特征图&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;se&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SEAttention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;channel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reduction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 实例化SE模块，设置降维比率为8&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;se&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 将输入特征图通过SE模块进行处理&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 打印处理后的特征图形状，验证SE模块的作用&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;消融实验ablation&#34;&gt;消融实验ablation&lt;/h3&gt;
&lt;h4 id=&#34;实验环境&#34;&gt;实验环境&lt;/h4&gt;
&lt;p&gt;本文的实验平台为64位windows11系统，GPU为NVIDIA GeForce RTX 4070(12GB/华硕)，搭配的处理器为intel i5-13490F。Anaconda版本为23.7.4。cuda版本为12.1。Pytorch版本2.1.2。Python版本为3.9.18。tensorboard版本为2.16.2。&lt;/p&gt;
&lt;p&gt;本实验中所有的网络都使用相同的训练设置,填充大小是100*100,batchsize大小是18,本文使用优化器为ADAM来训练模型,对Rain100L数据集的训练周期为100个epoch(对于其他大数据集如Rain1400等由于时间问题，并没有达到这个迭代周期),初始学习率设置为0.001,当epoch分别为30、50、80时,学习率均乘以0.2。&lt;/p&gt;
&lt;h4 id=&#34;评估指标&#34;&gt;评估指标&lt;/h4&gt;
&lt;p&gt;本文使用图像去雨领域常用的图像质量评价指标PSNR、SSIM来对算法进行有效评估与比较。&lt;/p&gt;
&lt;h5 id=&#34;峰值信噪比psnr&#34;&gt;峰值信噪比PSNR&lt;/h5&gt;
&lt;p&gt;PSNR 的计算基于均方误差（MSE）。首先计算 MSE，它是原始图像和失真图像对应像素差的平方值的平均值。PSNR的值越高表示处理后的图像与原图越接近。PSNR通过下面的公式计算：&lt;/p&gt;
&lt;p&gt;​ &lt;br&gt;
$$
PSNR = 10\cdot\log_{10}\left(\frac{{MAX}^2}{{MSE}}\right)
$$
其中，$\text{MAX}$是可能的最大像素值，通常对于8位图像是255，$\text{MSE}$（Mean Squared Error）表示均方误差。而MSE的计算方式如下：&lt;/p&gt;
&lt;p&gt;$$
MSE = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n(I(i, j) - K(i, j))^2
$$
其中，$m$ 和 $n$ 分别是图像的行数和列数，$I(i, j)$ 是原始图像在位置 $(i, j)$ 的像素值，$K(i, j)$ 是失真或重建图像在位置 $(i, j)$ 的像素值。&lt;/p&gt;
&lt;h5 id=&#34;结构相似性ssim&#34;&gt;结构相似性SSIM&lt;/h5&gt;
&lt;p&gt;SSIM基于三个比较维度：亮度（luminance）、对比度（contrast）和结构（structure）。SSIM指数通过比较两幅图像的亮度、对比度和结构的相似性来计算。SSIM的值范围从-1到1，其中1表示两图像完全相同。其计算方式如下：&lt;/p&gt;
&lt;p&gt;$$
\text{SSIM}(x, y) = \left( \frac{2 \mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1} \right) \times \left( \frac{2 \sigma_{xy} + C_2}{\sigma_x^2 + \sigma_y^2 + C_2} \right) \times \left( \frac{\sigma_{xy} + C_3}{\sigma_x \sigma_y + C_3} \right)
$$&lt;/p&gt;
&lt;p&gt;其中
$\mu_x$ 和 $\mu_y$ 分别是图像 $x$ 和 $y$ 的平均亮度。
$\sigma_x^2$ 和 $\sigma_y^2$ 分别是图像 $x$ 和 $y$ 的方差。
$\sigma_{xy}$ 是图像 $x$ 和 $y$ 的协方差。
$C_1, C_2, C_3$ 是小常数，用以维持稳定性，通常 $C_3 = \frac{C_2}{2}$。&lt;/p&gt;
&lt;h4 id=&#34;实验数据集&#34;&gt;实验数据集&lt;/h4&gt;
&lt;p&gt;本文的所有的消融实验都是在合成的小雨数据集Rain100L上进行的，Rain100L数据集里面包含有雨图片和对应的无雨图片,雨纹的密度较小,其训练集有200张有雨图片,测试集有100张有雨图片。对比实验在Rain100L，Rain1400以及Rain12上进行。&lt;/p&gt;
&lt;h4 id=&#34;损失函数&#34;&gt;损失函数&lt;/h4&gt;
&lt;p&gt;本文的loss function选择与原PReNet论文一直，采用negative SSIM作为loss，通过取负，我们可以将最大化SSIM的问题转化为最小化负SSIM的问题从而符合求最小损失这个前提。且ssim的评估方式更符合人眼直觉。本loss function表达式为：
$$
L_s=-SSIM
$$&lt;/p&gt;
&lt;p&gt;我在第一层卷积层后缝合了gam模块&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;PReNet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;recurrent_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;use_GPU&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PReNet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iteration&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;recurrent_iter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use_GPU&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;use_GPU&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gam&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GAM_Attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#GAM&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;col&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iteration&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gam&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_list&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;实验设计&#34;&gt;实验设计&lt;/h4&gt;
&lt;p&gt;本部分通过引入SE通道注意力机制模块以及GAM全局注意力机制模块，分别安插在PReNet网络的不同位置进行性能比对，从而筛选出更优质的改进方案。嵌入位置如下图：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524231523089.png&#34; alt=&#34;image-20240524231448362&#34; style=&#34;zoom:80%;&#34; /&gt;&lt;/center&gt;
&lt;p&gt;对于不同注意力模块以及其在网络中嵌入的不同位置，后以不同名字指代如表1.1所示。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;注意力模块的相对位置&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;后续的网络命名&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;GAM模块在残差块内&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_GAM_R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;SE 模块在残差块内&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_SE_R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;GAM模块在输入层后&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_GAM_afterConv0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;SE 模块在输入层后&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_SE_afterConv0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;GAM模块在输入层后，SE模块在残差块内&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_GAM_SE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;SE 模块在输入层后，GAM模块在残差块内&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_SE_GAM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;GAM模块同时在输入层后以及残差块内&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_GAM_GAM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;在rain100l上的消融实验结果&#34;&gt;在Rain100L上的消融实验结果&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;模型名称&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PSNR&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;SSIM&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.48&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.979&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_GAM_R&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;font color=&#34;#dd0000&#34;&gt;37.97&lt;/font&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;font color=&#34;#006600&#34;&gt;0.980&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_SE_R&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.69&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.979&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_GAM_afterConv0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.49&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.979&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_SE_afterConv0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.49&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.979&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_GAM_SE&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.45&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.979&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_SE_GAM&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;font color=&#34;#006600&#34;&gt;37.92&lt;/font&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;font color=&#34;#dd0000&#34;&gt;0.981&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PReNet_GAM_GAM&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.91&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0.980&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;针对Rain100L数据集的实验结果表明，当GAM模 块单独集成在每个残差块内时，网络性能提升最为明显。这一发现表明GAM模块在 处理图像中的全局信息方面具有显著优势，能够有效地提升去雨效果。&lt;/p&gt;
&lt;h3 id=&#34;对比实验&#34;&gt;对比实验&lt;/h3&gt;
&lt;p&gt;将上文所提的PReNet_GAM_R方案在Rain1400、Rain100L和Rain12这 三个合成数据集上与3种经典的基于深度学习的图像去雨算法JORDER、RESCAN、 PReNet 算法以及本文提的PReNet_GAM_R方案进行定性定量评估比较。&lt;/p&gt;
&lt;p&gt;并在真实雨 图上进行主观感觉上的比较,来验证本方案的有效性。&lt;/p&gt;
&lt;h4 id=&#34;1-合成数据集&#34;&gt;(1) 合成数据集&lt;/h4&gt;
&lt;p&gt;本文方案与上述4种去雨算法的定量比较如表4.6所示,红色标注即为最高值。 从表中可以看出,PReNet_GAM_R方案在三个合成数据集上均能取得较高的PSNR和 SSIM 指标值,且其在三个合成数据集上的指标值基本高于其他四个去雨算法。 需要说明的是本对比实验中，PReNet_GAM_R 针对 Rain1400 数据集的训练 epoch 仅为 8 个 epochs，主要原因为 Rain1400 数据集庞大从而导致训练时间长的 问题，在RTX4070显卡夜以继日地连续72小时的工作情况下也仅仅训练到了第8 个epoch，因此此处的对比数据比原先PReNet性能略低一筹。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;Measure&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;JORDER&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;RESCAN&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;PReNet&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;strong&gt;PReNet_GAM_R&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Rain100L&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PSNR/SSIM&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.61/0.974&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;29.80/0.881&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;37.48/0.979&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;font color=&#34;#dd0000&#34;&gt;37.97/0.980&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Rain12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PSNR/SSIM&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;33.92/0.953&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;mdash;&amp;ndash;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;36.66/0.961&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;font color=&#34;#dd0000&#34;&gt; 37.04/0.962&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Rain1400&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;PSNR/SSIM&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;mdash;&amp;ndash;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&amp;mdash;&amp;ndash;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;font color=&#34;#dd0000&#34;&gt;32.60/0.946&lt;/font&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;32.41/0.945&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;2-真实雨图&#34;&gt;(2) 真实雨图&lt;/h4&gt;
&lt;p&gt;接下来将改进的PReNet_GAM_R与原先的PReNet在真实雨图上的去雨效果进 行比对，如下方图所示，从左到右依次为原始图像，PReNet图像，PReNet_GAM_R 图像，从上到下一共展示三组真实雨图，此处所用于测试的模型为在Rain1400数据 集上训练8epochs的PReNet_GAM_R网络 。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524234636220.png&#34; alt=&#34;image-20240524234528791&#34;  /&gt;&lt;/center&gt;
&lt;p&gt;通过观察可以发现，PReNet_GAM_R改进后的去雨图像在清晰度上比PReNet的 去雨图像更为清晰，在细节部分没有出现丢失的情况，整幅图片看起来更为透彻，在 图像的清晰度提高和细节保留更完整，从而提供了更加令人满意的视觉效果。此外， 从感官体验的角度来看，图像的细节，如建筑线条、环境纹理和边缘清晰度等，在去 除雨水后都被更好地保留了下来，几乎与无雨的原始场景无异。&lt;/p&gt;
&lt;h3 id=&#34;最后的网络结构&#34;&gt;最后的网络结构&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240524215731380.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;总结与展望&#34;&gt;总结与展望&lt;/h3&gt;
&lt;h4 id=&#34;总结&#34;&gt;总结&lt;/h4&gt;
&lt;p&gt;本文介绍了在PReNet网络中集成不同的注意力机制模块的融合实验。通过详细的实验设计，分析了SE和GAM模块单独插入原网络输入层后和残差块内的效果，还考察了这两种模块同时使用时的性能变化。实验结果通过PSNR和 SSIM 指标进行量化，以评估不同配置对去雨效果的影响。针对Rain100L数据集的 实验结果表明，当GAM模块单独集成在每个残差块内时，网络性能提升最为明显。 本文的研究表明，注意力机制，尤其是全局注意力机制在提高深度网络处理复 杂去雨任务中的有效性。GAM模块通过其对全局信息的高效整合，使得网络能够更 准确地识别和去除图像中的雨滴，同时保留更多的背景细节。这一点在图像去雨的应用中尤为重要，因为背景信息的保留直接关系到去雨图像的视觉质量和实用价值。&lt;/p&gt;
&lt;h4 id=&#34;展望&#34;&gt;展望&lt;/h4&gt;
&lt;p&gt;本文改进的GAM融合PReNet去雨算法在进行单幅图像去雨工作时有一定的 提升效果,但与最新发布的去雨研究相比，仍存在一定的差距。此外，受限于硬件 资源和时间成本，本研究未能对多个大型数据集进行深入分析，甚至在唯一使用的 Rain1400 数据集上的训练周期也仅限于8个epochs。这些限制使得研究结果可能未 能完全体现潜在的优化效果。因此，未来研究的方向和展望可以从以下几个方面进 行扩展和深化：&lt;/p&gt;
&lt;p&gt;(1) 本文已经尝试了集成不同的注意力机制以优化去雨性能。未来工作可以继续 在这一方向上进行创新，例如探索新的神经网络架构或改进现有的深度学习模块，以适应去雨处理的特定需求。此外，调整和优化模型参数也是实现更有效去雨处理的 关键途径。&lt;/p&gt;
&lt;p&gt;(2) 尽管通过合成的虚拟雨图可以以更多样本地进行全监督学习，但这些数据往 往无法完全复现真实雨景的复杂性和多样性。因此，开发和使用更接近真实世界条 件的雨图数据集将是未来研究的重要方向。这将帮助模型更好地理解和处理实际场 景中的雨效应，提高去雨技术的实际应用价值。&lt;/p&gt;
&lt;p&gt;(3) 目前常用的图像质量评估指标如PSNR和SSIM虽然提供了量化的性能评估， 但并不能完全代表人类的视觉感知。应该设计一种更符合人类视觉感知的评价方法， 能更准确地反映去雨效果的优劣，这对于推动去雨技术的发展具有重要意义。&lt;/p&gt;
&lt;p&gt;(4) 考虑到降雨的复杂性，将模型驱动方法与数据驱动方法结合起来，可能是解 决去雨问题的一种有效策略。这种融合方法可以充分利用两种学习方式的优势，提高去雨算法的准确性和鲁棒性。 希望未来通过在上述几方面的努力，可以使得去雨研究有望实现更大的突破，为相关领域带来更深远的影响。&lt;/p&gt;
&lt;h2 id=&#34;写在最后的ps&#34;&gt;写在最后的ps&lt;/h2&gt;
&lt;p&gt;所谓的毕设就这么莫名其妙的搞完了，其实工作量很小，多半的工作量是前期阅读论文以及复现各种其他的去雨算法来选择一个比较合适的改进平台，后期所有的实验基本上只花了几天的功夫，反正现在答辩也弄完了，后面空闲的时间可能会做几个开源项目，然后再次投入那场“永恒轮回”的地狱中。&lt;/p&gt;
&lt;p&gt;复旦get out，中科院 come instead！&lt;/p&gt;
</description>
        </item>
        <item>
        <title>时间的轮廓</title>
        <link>https://Dendrobium123.github.io/p/%E6%97%B6%E9%97%B4%E7%9A%84%E8%BD%AE%E5%BB%93/</link>
        <pubDate>Fri, 17 May 2024 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E6%97%B6%E9%97%B4%E7%9A%84%E8%BD%AE%E5%BB%93/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/image-20240517233703783.png" alt="Featured image of post 时间的轮廓" /&gt;&lt;h3 id=&#34;火焰战士&#34;&gt;火焰战士&lt;/h3&gt;
&lt;p&gt;不知从何时起我开始觉得天空好美，&lt;/p&gt;
&lt;p&gt;可能是因为那个少女消失在那里的缘故吧，&lt;/p&gt;
&lt;p&gt;我有些羡慕她，&lt;/p&gt;
&lt;p&gt;没有翅膀的我，&lt;/p&gt;
&lt;p&gt;只有将希望和憧憬寄托在她的身上，&lt;/p&gt;
&lt;p&gt;在天空中飞翔的她，能够感受到我的思念吗？&lt;/p&gt;
&lt;p&gt;天空不再是遥不可及，&lt;/p&gt;
&lt;p&gt;她一定把我的梦想带上了天际，&lt;/p&gt;
&lt;p&gt;我坚信。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20240517233703783.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20240517233703783&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;不知从何时起，&lt;/p&gt;
&lt;p&gt;我一直在做一个梦，&lt;/p&gt;
&lt;p&gt;一遍又一遍，&lt;/p&gt;
&lt;p&gt;没有开始也没有终结，&lt;/p&gt;
&lt;p&gt;我暗自祈祷这不是梦，&lt;/p&gt;
&lt;p&gt;因为梦总会结束，&lt;/p&gt;
&lt;p&gt;于是我开始等待，&lt;/p&gt;
&lt;p&gt;等待着某人将我唤醒，&lt;/p&gt;
&lt;p&gt;我仿佛坐在空无一人的山顶上，&lt;/p&gt;
&lt;p&gt;听着一个脚步声由远而近，&lt;/p&gt;
&lt;p&gt;也许那之后才是梦的开始，&lt;/p&gt;
&lt;p&gt;仿佛时间已经停止，&lt;/p&gt;
&lt;p&gt;我一直在等待着，&lt;/p&gt;
&lt;p&gt;直到我已经忘记了，&lt;/p&gt;
&lt;p&gt;为何要等待，&lt;/p&gt;
&lt;p&gt;但是正如黑夜之后一定是黎明，&lt;/p&gt;
&lt;p&gt;奇迹一定会到来，&lt;/p&gt;
&lt;p&gt;我坚信&lt;/p&gt;
</description>
        </item>
        <item>
        <title>PyTorch基础笔记_01</title>
        <link>https://Dendrobium123.github.io/p/pytorch%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0_01/</link>
        <pubDate>Thu, 18 Apr 2024 21:02:02 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/pytorch%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0_01/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/pytorch_0.jpg" alt="Featured image of post PyTorch基础笔记_01" /&gt;&lt;h1 id=&#34;pytorch基础笔记_01&#34;&gt;PyTorch基础笔记_01&lt;/h1&gt;
 &lt;center&gt;&lt;/center&gt;&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=3914386&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;&lt;center&gt;&lt;/center&gt;
&lt;p&gt;常用库：torch，numpy，pandas，matplotlib&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.nn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;nn&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;optim&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#优化器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.nn.functional&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#用到的函数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;基础使用&#34;&gt;基础使用&lt;/h3&gt;
&lt;p&gt;假设一个最简单的神经网络，没有隐藏层，只有输入层和输出层&lt;/p&gt;
&lt;p&gt;eg：5个输入神经元，7个输出神经元，全连接
$$
y=w\cdot x+b
$$
x为输入层，形状为（1,5）；
$$
\begin{bmatrix}x_1 &amp;amp; x_2&amp;amp;x_3&amp;amp;x_4&amp;amp;x_5\end{bmatrix}
$$
Y为输出层，形状为（1,7）#同上
$$
\begin{bmatrix}y_1 &amp;amp; y_2&amp;amp;y_3&amp;amp;y_4&amp;amp;y_5&amp;amp;y_6&amp;amp;y_7\end{bmatrix}
$$
w为权重，形状为（5,7) #全连接，
$$
\begin{bmatrix}w_{11}&amp;amp;w_{12}&amp;amp;w_{13}&amp;amp;w_{14}&amp;amp;w_{15}&amp;amp;w_{16}&amp;amp;w_{17}
\\w_{21} &amp;amp; w_{22}&amp;amp;w_{23}&amp;amp;w_{24}&amp;amp;w_{25}&amp;amp;w_{26}&amp;amp;w_{27}
\\w_{31} &amp;amp; w_{32}&amp;amp;w_{33}&amp;amp;w_{34}&amp;amp;w_{35}&amp;amp;w_{36}&amp;amp;w_{37}
\\w_{41} &amp;amp; w_{42}&amp;amp;w_{43}&amp;amp;w_{44}&amp;amp;w_{45}&amp;amp;w_{46}&amp;amp;w_{47}
\\w_{51} &amp;amp; w_{52}&amp;amp;w_{53}&amp;amp;w_{54}&amp;amp;w_{55}&amp;amp;w_{56}&amp;amp;w_{57}
\end{bmatrix}
$$
b为bias偏置，每个输出神经元输出前都会经过bias调整，所以形状为7&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#定义各个参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;requires_grad&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#生成5*7的随机数矩阵来填充w,同时设定w是需要后续进行梯度下降更新的属性，后半部分缺失会导致程序报错&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;requires_grad&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.001&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#设置学习率&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#x与w为矩阵，使用矩阵乘法@,同时调用事前导入为F的激活函数，这里选择relu函数（1）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#开始求loss，假定本案例为一个分类任务，选择交叉熵损失函数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross_entropy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#开始求参数梯度，进行梯度下降算法更新参数，pytorch提供了一个函数方法backward()，可以直接帮助我们找到所有参数梯度，无需自己算&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#这步只求了梯度，顺利运行的前提是需要提前设定参数是需要求梯度的属性&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;grad&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#查看w经过梯度下降所得的参数值，！还没有进行参数的更新！见（2）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;grad&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#这步才是更新了参数（3）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;两种损失函数&#34;&gt;两种损失函数&lt;/h4&gt;
&lt;p&gt;1.分类任务一般使用交叉熵损失函数：&lt;/p&gt;
&lt;p&gt;$$
\text{Cross-Entropy}= -\sum_{i=1}^n [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]
\\y_i：\text{ 类别的真实标签（通常为0或1）}
\\\hat{y_i}: \text{对应类别的预测概率}
$$&lt;/p&gt;
&lt;p&gt;2.回归任务一般使用均方误差损失函数&lt;/p&gt;
&lt;p&gt;$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
\\y_i：\text{真实值}
\\\hat{y_i}:预测值
\\n:样本数量
$$
损失函数使用时需注意该函数的传参&lt;/p&gt;
&lt;p&gt;（1）结果：&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/y.png&#34; alt=&#34;y&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;（2）结果：&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/w_grad.png&#34; alt=&#34;w_grad&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;（3）结果：&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/w=w-lrw.png&#34; alt=&#34;image-20240418172233464&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;搭建模型&#34;&gt;搭建模型&lt;/h3&gt;
&lt;p&gt;搭建网络需要定义一个类，eg此处输入图像为rgb彩色图像，像素为48*48&lt;/p&gt;
&lt;p&gt;ps: jupyter lab查看函数参数快捷键shift+tab&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;M_CNN&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#定义网络，父类为之前导入的nn类&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#初始化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;super&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#父类的函数方法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#设计的卷积层，调用nn父类中的Conv2d卷积函数，参数见（4）eg：输入为彩色图像（输入为3），输出16个通道，卷积核大小为3*3；padding=1为填充1像素保证输出的长宽不变&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conv2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;padding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#规范化，通道数为16#第一层卷积的输出通道数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MaxPool&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kernel_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#池化层&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#网络全连接层，将处理完的输入图像所有像素拼接为一维，加入中间有1000个神经元的隐藏层&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#1000个隐藏层1神经元，最送到100个隐藏层2神经元，由于12*12*32的输入太大，故需要不同神经元数目的隐藏层过渡&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#最后输出为7个神经元&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#输入先过卷积，后过规范器，最后通过激活函数relu，更新原有输入&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MaxPool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#经过池化后图像宽高对半减&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bn2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conv2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#经过第二个卷积层&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MaxPool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#进入全连接之前，需要讲3维张量撑成1维的向量，-1代表可能用小批量进行训练，不确定数目，代表自适应，后面的数值代表进入的张量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#进入全连接&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;全连接层参数计算&#34;&gt;全连接层参数计算：&lt;/h4&gt;
&lt;p&gt;$$
一个(3,48\times48)图像通过一个(3,16,kernel=3\times3,步长为1)的卷积层，\\输出为(16,48\times48)图像，\\通过第一个池化层，窗口为2\times2,步长为2，\\输出为(16，24\times24),\\经过第二个卷积层(16，32，kernel=3\times3,步长为1)\\输出为(32,24\times24)，\\通过第二个池化层，变为(32，12\times12)
$$&lt;/p&gt;
&lt;p&gt;（4）nn.Conv2d函数的参数：&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/nn.Conv2d.png&#34; alt=&#34;image-20240418174521676&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h3 id=&#34;数据处理&#34;&gt;数据处理&lt;/h3&gt;
&lt;p&gt;读取数据，转化为dataset，再由dataset转化为dataloader&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DataLoader&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torchvision&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datasets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tranforms&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torchvision.datasets&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ImageFolder&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transforms&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Compose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;transforms&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ToTensor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#读取图像后将数据转换为tensor类型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;transforms&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,))&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#去均值，除标准差的归一化操作&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ImageFolder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;file_path&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tansform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#通过imagefolder来为读取的数据进行标注，后半部分使用tranform进行格式转化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dataloader&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataLoader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#shuffle意义为每个batch开始时顺序是否要被打乱&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;跑模型&#34;&gt;跑模型&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#检测GPU使用情况&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda:0&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;is_available&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cpu&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Using device&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#创建模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CNN_M&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#定义损失函数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_fun&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross_entropy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#定义优化器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;opt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Adam&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#parameters为模型的所有参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;epochs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#以batchsize为32的步长，每个批次通过一遍网络，直至所有样本被扫完作为一个epoch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;epochs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataloader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#每次的小batch做的事&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#input为作为输入的数据，labels为获取标签，to(device)为把数据往显卡里送&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#进入模型的forward第一层到最后一层，结果给y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cross_entropy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#通过计算得的y与labels标签正确答案进行损失计算&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#计算梯度值&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;opt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#优化器进行梯度下降，更新参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;opt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zero_grad&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#参数的梯度清零，不然进入下一轮梯度计算时会累积上一次的结果&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;ps最后来一张之前tutu带我看的不认识的vtuber演唱会&#34;&gt;ps:最后来一张之前tutu带我看的不认识的Vtuber演唱会&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/pytorch01ps.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>深度学习_00</title>
        <link>https://Dendrobium123.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_00/</link>
        <pubDate>Sun, 14 Apr 2024 12:20:50 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_00/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/深度学习00cover.jpg" alt="Featured image of post 深度学习_00" /&gt;&lt;center&gt;&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=565970070&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;h2 id=&#34;深度学习基础知识&#34;&gt;深度学习基础知识&lt;/h2&gt;
&lt;p&gt;几种“学习”间的关系&lt;/p&gt;
&lt;h4 id=&#34;机器学习---最大的概念&#34;&gt;机器学习&amp;mdash;最大的概念&lt;/h4&gt;
&lt;p&gt;&amp;lt;让机器通过学习的方式得到一个可以解决问题的模型&amp;gt;&lt;/p&gt;
&lt;p&gt;学习方法：KNN（K近邻），k means（k均值解决聚类问题），SVM，深度学习&lt;/p&gt;
&lt;p&gt;机器学习，隐式学习&lt;/p&gt;
&lt;p&gt;神经网络：输入层，隐藏层，输出层&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/神经网络.jpg&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;center&gt;
&lt;p&gt;神经元&lt;/p&gt;
&lt;p&gt;不改变网络层和算法的情况下，影响输出结果的是各神经元连接线路上的数值权重&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/神经元.jpg&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;center&gt;
&lt;p&gt;除了一系列加减乘除的线性变换外，还引入了激活函数&lt;/p&gt;
&lt;h3 id=&#34;激活函数阶跃函数不用&#34;&gt;激活函数：阶跃函数（不用&lt;/h3&gt;
&lt;p&gt;希望通过梯度下降的方式求得参数更新的过程，阶跃函数无法正常求导，需要引入δ函数，因此使用别的函数作为激活函数Sigmoid，以此解决阶跃函数不可导的问题&lt;/p&gt;
&lt;h4 id=&#34;sigmoid&#34;&gt;Sigmoid：&lt;/h4&gt;
&lt;p&gt;$$
S(x)=\dfrac{1}{1+e^{-x}}
$$&lt;/p&gt;
&lt;p&gt;sigmoid导数：
$$
S&amp;rsquo;(x)=\dfrac{e^{-x}}{(1+e^{-x})^2}=S(x)(1-S(x))
$$
Sigmoid函数及其导数的图像：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/sigmoid.jpg&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;center&gt;
&lt;center&gt;注：取值范围在0—1间的sigmoid函数叫logistic函数&lt;center&gt;
&lt;h4 id=&#34;tanh&#34;&gt;tanh：&lt;/h4&gt;
&lt;p&gt;$$
tanh=\dfrac{e^x-e^{-x}}{e^x+e^{-x}}
$$&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/tanh.jpg&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;范围在（-1，1）间的激活函数&lt;/p&gt;
&lt;h4 id=&#34;relu函数&#34;&gt;Relu函数&lt;/h4&gt;
&lt;p&gt;$$
f(x)=\begin{cases}x &amp;amp; x\geq0 \\0&amp;amp;x&amp;lt; 0\end{cases}
$$&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/relu.jpg&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;p&gt;每个神经元所做的事：
$$
g_{output}=g(w_1\times a+w_2\times b+w_3\times c+w_4\times d)—&amp;gt;relu
$$
注：a,b,c,d为权重值，神经元输出结果为各参数加权后通过一个relu函数所得的值&lt;/p&gt;
&lt;p&gt;机器学习的目的是在给定前提情况下，寻找能得到最好输出的w参数们&lt;/p&gt;
&lt;h4 id=&#34;梯度下降&#34;&gt;梯度下降&lt;/h4&gt;
&lt;p&gt;如何寻找需要的W&lt;/p&gt;
&lt;p&gt;通过当前所计算得出的结果与已知的正确结果做差，考虑到所得结果的正负号问题，采用对式子求平方的方式（平方好求导，绝对值不好求导）
$$
L=(f(x)-y)^2
$$&lt;/p&gt;
&lt;p&gt;L越小，模型性能越好，f(x)与参数w有关，因此L也是个关于w的函数。&lt;/p&gt;
&lt;p&gt;可以通过调整w来使L的取值变小&lt;/p&gt;
&lt;p&gt;动态更新W，eg：初始值w0，第一刻w1&amp;hellip;..
$$
W_1=W_0-lr\cdot \frac{\partial L}{\partial W_0}
\\W_2=W_1-lr\cdot \frac{\partial L}{\partial W_1}
\\&amp;hellip;
$$&lt;/p&gt;
&lt;h4 id=&#34;局部最优全局最优&#34;&gt;局部最优/全局最优&lt;/h4&gt;
&lt;p&gt;类似高等数学函数章节中的极值和最值问题，局部导数为0的极值点不代表此处是整个函数的极值&lt;/p&gt;
&lt;center&gt;~~乐经良：说明它是一个地头蛇~~&lt;center&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;ps:顺带吐槽一句，这个hugo对LateX数学公式的键入好像不是很友好，比如公式间的换行用要用\\，但是他识别代码的时候只识别一条杠\，这就导致像&lt;/em&gt;
$$
f(x)=\begin{cases}x &amp;amp; x\geq0 \\0&amp;amp;x&amp;lt; 0\end{cases}
$$
&lt;em&gt;这种分段的函数会显示成这样&lt;/em&gt;
$$
f(x)=\begin{cases}x &amp;amp; x\geq0 \0&amp;amp;x&amp;lt; 0\end{cases}
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;byd后来我发现你只需要打3个\就能解决问题了。&lt;/em&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>中国后现代主义与网络亚文化的发展以“抽象文化”为例</title>
        <link>https://Dendrobium123.github.io/p/%E4%B8%AD%E5%9B%BD%E5%90%8E%E7%8E%B0%E4%BB%A3%E4%B8%BB%E4%B9%89%E4%B8%8E%E7%BD%91%E7%BB%9C%E4%BA%9A%E6%96%87%E5%8C%96%E7%9A%84%E5%8F%91%E5%B1%95%E4%BB%A5%E6%8A%BD%E8%B1%A1%E6%96%87%E5%8C%96%E4%B8%BA%E4%BE%8B/</link>
        <pubDate>Sun, 03 Mar 2024 13:10:42 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E4%B8%AD%E5%9B%BD%E5%90%8E%E7%8E%B0%E4%BB%A3%E4%B8%BB%E4%B9%89%E4%B8%8E%E7%BD%91%E7%BB%9C%E4%BA%9A%E6%96%87%E5%8C%96%E7%9A%84%E5%8F%91%E5%B1%95%E4%BB%A5%E6%8A%BD%E8%B1%A1%E6%96%87%E5%8C%96%E4%B8%BA%E4%BE%8B/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/db7641dcd583699a45fbb9a138a5679.jpg" alt="Featured image of post 中国后现代主义与网络亚文化的发展以“抽象文化”为例" /&gt;&lt;p&gt;这篇文章是我之前一门通识课所布置的作业，今天闲的没事就想水一篇文章。&lt;/p&gt;
&lt;p&gt;小石我纵横现代简中互联网多年，对如今的网络文化也有一定的思考，借着通识课作业的契机一并谈谈我对当今网络抽象话的了解。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;中国后现代主义与网络亚文化的发展&#34;&gt;中国后现代主义与网络亚文化的发展&lt;/h1&gt;
&lt;h1 id=&#34;以抽象文化为例&#34;&gt;以“抽象文化”为例&lt;/h1&gt;
&lt;p&gt;近年来，随着网络社交平台和短视频行业的发展，我们越来越不难注意到，有这样一类越来越频繁浮现的群体。他们往往说着荒诞又难以理解的话语，洒脱地嘲讽或攻击一些公众形象，同时又无时不刻进行着自我矮化。这类群体在后疫情时期越发壮大逐渐从网络世界的边缘浮现至仅次于主流文化的亚文化地位。&lt;strong&gt;在理解了后现代主义和解构主义后，我们可以说这类“抽象文化”的诞生和扩张正是后现代主义思潮在当今中文网络世界的具体表现之一。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;后现代主义&lt;/strong&gt;一词在法国哲学家利奥塔出版了《后现代状态》一书后正式出道，它是一种基于对西方现代社会进行反思和批判而形成的文化思潮。而其核心理论之一便是宏大叙事的消亡。宏大叙事被认为是现代性的一种体现，是一种使特定意识形态世界观乃至价值观正当化的趋势，也是一种对事物运行规律和人类历史进程的宏观解释。但后现代主义主张对本质的否定和对非理性的推崇，强调多元性、差异性，反对同一性，并具有不确定性、分裂性、解构性及戏谑性等特点。而解构主义则是由法国哲学家德里达提出的，其原本的主张是对中心主义的一种批判，是破坏有中心的二元对立状态，而在今天，解构一词作为一种行为可以用于任何对象和场合，成为了后现代主义中的一环 。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240303133013.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;center&gt;
&lt;p&gt;&lt;em&gt;&lt;center&gt;Jean-Francois Lyotard&lt;/center&gt;&lt;/em&gt;&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240303132930.png&#34; alt=&#34;Jacques Derrida&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;center&gt;
&lt;center&gt;Jacques Derrida&lt;/center&gt;
&lt;h3 id=&#34;center为什么说目前网络抽象文化的扩张是中国后现代主义的具体表现呢center&#34;&gt;&lt;center&gt;&lt;strong&gt;为什么说目前网络抽象文化的扩张是中国后现代主义的具体表现呢？&lt;/strong&gt;&lt;center&gt;&lt;/h3&gt;
&lt;p&gt;首先抽象文化本身仅仅只是一个指代，其本身并没有具体的定义，因为其本身就是网络去中心化的产物。这批亚文化的创作者们在17,18年左右伴随着李赣和孙笑川之流的直播间而诞生，但值得注意的是【孙笑川】本身并不是该亚文化的定义，它只是抽象文化的一个符号。而抽象文化的符号是无穷无尽的，因为其本身就是一种会无序扩张的社会模因。我在这里举一些最近抽象文化的新网络符号，比如说“yy丁真”系列；抽象小卖部系列；asoul女团；你说的对，但原神是一款XXX系列；东北往事；柯洁直播间等等，与之类似的还有油管的xqc，Ishowspeed，veibae等主播的直播间。抽象文化的群体很喜欢对一些互联网上的土味视频进行解构和二次创作，比如上述的抽象小卖部和东北往事等视频，其原本只是类似于通过卖丑，猎奇等行为换取播放量的视频，但在抽象文化的二次塑形中爆发了前所未有的生命力，原本的猎奇被视为荒诞的艺术，各类卖丑整活的行为也被视为一种对社会规训下的现实的反抗。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/640.gif#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240303133718.png#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240303133844.png#pic_center&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;center安迪沃霍尔曾说在未来每个人都能出名15分钟center&#34;&gt;&lt;center&gt;&lt;strong&gt;安迪沃霍尔曾说，在未来，每个人都能出名15分钟。&lt;/strong&gt;&lt;center&gt;&lt;/h3&gt;
&lt;p&gt;因此，每个人也都可能被吸纳进抽象文化的符号中，因为当你进入了这个飞速迭代的网络世界后，发掘和被消费是在一个很短的周期内被完成的，一个梗，一个meme的生命力是很短暂的，尽管如此它们却可以在如此短的时间中扩张至互联网的每一个角落。但人们迫切地需要新的故事和梗来娱乐和消费，因此这种符号的产生就和人工智能训练集和产出内容一样，无时不刻不在自我迭代和吸收新的叙事。&lt;/p&gt;
&lt;h3 id=&#34;center后现代主义在这其中中起到了推波助澜的作用center&#34;&gt;&lt;center&gt;&lt;strong&gt;后现代主义在这其中中起到了推波助澜的作用。&lt;/strong&gt;&lt;center&gt;&lt;/h3&gt;
&lt;p&gt;一方面，后现代主义中的解构化、碎片化和去中心化等因素助推了抽象文化滋生。同时因为当今包含宏大叙事的现代主义在中国仍占据主导地位，对事物的评判仍保有一个中心化的标准，这正是抽象文化理所应当存在的土壤。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;与早年qq空间的杀马特之流不同，抽象文化本身的特点就是对传统鄙视链的彻底摧毁。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;曾经的杀马特文化有各式各类的家族，不同家族之间等级严明，在家族中的地位甚至与你加入的时间有关。**其本质与其说是对资本社会的反抗不如说是通过建立另一个帝国以此逃避原有的社会秩序。**但抽象文化圈却是其完全相反的另一面，比如在百度贴吧中一个人资历可以通过铭牌显示出来，如果入吧的时间早，发言帖子多会增加自己的经验从而提高在本吧的等级。而孙笑川吧中总会出现“黄牌赶紧给绿牌磕一个”这样的话术（黄牌等级高，绿牌等级低）。通过这样完全颠倒的权力体系说明了抽象文化的对鄙视链结构的否定。与之类似的，还有丁真的爆火。在21年初官方为了宣传少数民族和扶贫工作仅仅因为丁真的纯真笑容便将这位学历连小学都没有的文盲聘请至国企工作同时大肆宣传将其捧成了一个网红。大部分网民对此是愤慨且否认的，但批评并不能阻止这样的事发生，因此在后期他们选择将丁真彻底解构，如同第二个孙笑川一样，丁真也成为了抽象文化的符号，网民们肆无忌惮地发表丑化丁真的表情包，将一切他们认为由于官方宣传而火爆但德不配位的人称之为XX丁真，如赛博丁真，滑雪丁真等等。这体现了抽象文化的反权威性和多元性。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240303133939.png&#34; style=&#34;zoom:50%;#pic_center&#34; /&gt;
&lt;h3 id=&#34;center抽象文化也有着和所有后现代主义思潮一样的弊病center&#34;&gt;&lt;center&gt;&lt;strong&gt;抽象文化也有着和所有后现代主义思潮一样的弊病，&lt;/strong&gt;&lt;center&gt;&lt;/h3&gt;
&lt;h3 id=&#34;center鲍德里亚说过每个共同体的建构是不同的但解构都是大同小异的center&#34;&gt;&lt;center&gt;鲍德里亚说过：每个共同体的建构是不同的，但解构都是大同小异的&lt;center&gt;&lt;/h3&gt;
&lt;p&gt;抽象文化下的网民们乐忠于这样的一种解构，乃至于任何事物在他们面前都只是一堆可供娱乐和消费的符号。一个最直接的例子“地狱笑话”这个词原意是笑了就要下地狱，因为其所取笑的对象本身是由于各种意外而发生悲剧的个体，比如侏儒症患者，残疾人，父母双亡等等。但地狱笑话吧却会以此为依据对其取笑，这样的一种解构是超脱道德的，他们会用草莓来嘲笑因生前想吃草莓，最后却因糖尿病而死的“墨茶”，用直升机戏谑科比等等。为所欲为的解构和消解严肃性成了他们唯一的目的，这本身就是荒诞的，同时也让他们失去了严肃讨论的土壤。&lt;/p&gt;
&lt;h3 id=&#34;center抽象文化在后现代皮囊下的现代性center&#34;&gt;&lt;center&gt;&lt;strong&gt;“抽象文化”在[后现代]皮囊下的[现代性]&lt;/strong&gt;&lt;center&gt;&lt;/h3&gt;
&lt;p&gt;在这种后现代思潮的混乱扩张中，许多刚接触抽象文化的新网民会由于其现代化的思想，以遵从威权的思考角度盲目跟风，此时复制相关的话术成为了他们彰显自己抽象属性的方式，用这样一种不加以思考的刻奇的方式加入到后现代思潮的狂欢当中。就像维特根斯坦说的“人不是思考了再说话，而只是说话”。在这时抽象文化的后现代属性消失，它成为了现代化的一种延伸。&lt;/p&gt;
&lt;p&gt;中文互联网经历过各种非主流时期，从早年的杀马特时期，火星文时期，到如今的饭圈文化，抽象文化等。尽管他们迟早会被时间吞噬，但抽象文化作为后现代主义思潮在中国的一种具体体现，其本身将永久地改变整个中文互联网以至于改变新一代网民的思考方式。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240303134040.png&#34; style=&#34;zoom:50%;#pic_center&#34; /&gt;</description>
        </item>
        <item>
        <title>模拟电子技术笔记</title>
        <link>https://Dendrobium123.github.io/p/%E6%A8%A1%E6%8B%9F%E7%94%B5%E5%AD%90%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Fri, 01 Mar 2024 21:56:02 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E6%A8%A1%E6%8B%9F%E7%94%B5%E5%AD%90%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/微信图片_20240303141619.jpg" alt="Featured image of post 模拟电子技术笔记" /&gt;&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=330 height=86 src=&#34;//music.163.com/outchain/player?type=2&amp;id=3947467&amp;auto=1&amp;height=66&#34;&gt;&lt;/iframe&gt;
&lt;h1 id=&#34;模拟电路&#34;&gt;模拟电路&lt;/h1&gt;
&lt;h3 id=&#34;模拟电路是指用来对模拟信号进行传输变换处理放大测量和显示等工作的电路它主要包括放大电路信号运算和处理电路振荡电路调制和解调电路及电源&#34;&gt;模拟电路是指用来对模拟信号进行传输、变换、处理、放大、测量和显示等工作的电路。它主要包括放大电路、信号运算和处理电路、振荡电路、调制和解调电路及电源。&lt;/h3&gt;
&lt;h3 id=&#34;何为模拟&#34;&gt;何为模拟？&lt;/h3&gt;
&lt;p&gt;在模拟电路中，电压高低（或电流大小）是模拟了所代表的物理量的变化，例如声音信号，声音被话筒转换成电压时，电压的高低直接反映了音量大小，声音的频率（音调）直接就是电信号的频率，此谓模拟的概念。&lt;/p&gt;
&lt;h3 id=&#34;半导体基础&#34;&gt;半导体基础&lt;/h3&gt;
&lt;h4 id=&#34;1本征半导体无杂质的稳定结构其本身导电能力差一般为4价的硅晶体&#34;&gt;1.本征半导体：无杂质的稳定结构，其本身导电能力差（一般为+4价的硅晶体&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;其中不同原子的电子会形成共价键&lt;/li&gt;
&lt;li&gt;载流子浓度低&lt;/li&gt;
&lt;li&gt;易受温度影响&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2何为载流子&#34;&gt;2.何为载流子？&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;脱离共价键的电子称为自由电子（negative&lt;/li&gt;
&lt;li&gt;原先的空缺位称为空穴（positive&lt;/li&gt;
&lt;li&gt;方便数学处理，将空穴视为一种粒子，因此可与自由电子称为载流子&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3杂质半导体在本征半导体中掺杂5价元素和3价元素&#34;&gt;3.杂质半导体（在本征半导体中掺杂+5价元素和+3价元素&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;N型半导体：多数载流子为自由电子（掺杂5价元素磷，施主原子—&amp;gt;PN结构中为➕&lt;/li&gt;
&lt;li&gt;P型半导体：多数载流子为空穴（掺杂3价元素硼，受主原子—&amp;gt;PN结构中为➖&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;4pn结单向导电性的原因&#34;&gt;4.PN结（单向导电性的原因&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;本质原因（2 stages
&lt;ul&gt;
&lt;li&gt;扩散运动（多子参与）：参杂的交接面的剧烈浓度差导致，作用后，P区与N区交界面饱和，载流子不再扩散，中间部分称为耗尽层。
&lt;ul&gt;
&lt;li&gt;高掺杂—&amp;gt;耗尽层宽度变窄&lt;/li&gt;
&lt;li&gt;低掺杂—&amp;gt;耗尽层交宽&lt;/li&gt;
&lt;li&gt;多子意义为大部分载流子&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;漂移运动（少子参与）：扩散运动后，耗尽层不是一个电中和的状态，因此会有內电场，从而在內电场的作用下，少子受电场力作用产生的漂移运动
&lt;ul&gt;
&lt;li&gt;P区自由电子飘向N区，N区空穴飘向P区，最终参与扩散运动的多子数等于参与漂移的少子数，达到动态平衡，PN结由此产生。&lt;/li&gt;
&lt;li&gt;少子意义为少部分载流子&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;5单向导电性半导体存在意义&#34;&gt;5.单向导电性（半导体存在意义&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;P+正电，N为负电，电场被削弱，PN结导通
&lt;ul&gt;
&lt;li&gt;正向导通时，因接触电场的存在，将会在结上形成一固定降压，硅PN结降压一般为0.6V左右，为了好算直接按0.7V计算（0.7必定够用~）&lt;/li&gt;
&lt;li&gt;正向导通时—&amp;gt;耗尽层变窄—&amp;gt;扩散运动加剧，多子运动浓度高—&amp;gt;扩散电流上升，从而导通&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;N加负，P加正（反向电压—&amp;gt;截止
&lt;ul&gt;
&lt;li&gt;耗尽层变大—&amp;gt;内建电场增强—&amp;gt;利于漂移运动—&amp;gt;扩散现象减弱—&amp;gt;漂移电流减少，从而截止&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;6pn结的反向击穿&#34;&gt;6.PN结的反向击穿&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;高浓度掺杂—&amp;gt;齐纳击穿
&lt;ul&gt;
&lt;li&gt;耗尽层窄，从而很小的反向电压就能击穿&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;低浓度掺杂—&amp;gt;雪崩击穿
&lt;ul&gt;
&lt;li&gt;耗尽层宽，需要较大电流才能击穿（由于载流子是以类似链式反应的速率1撞10，10撞100的效率反向运动，类似雪崩，因此被称为雪崩击穿&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;两种击穿都会产生大量的热，从而损坏元器件&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;二极管&#34;&gt;二极管&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;硅管，导通电压 0.6～0.8v&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;锗管，导通电压  0.1～0.3v&lt;/p&gt;
&lt;p&gt;​																			&lt;em&gt;伏安特性曲线&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/fb68bf930f1d7a747b71a4a038ecc02.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;整流作用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;整流二极管的原理是利用PN结的非线性特性，在正向偏置下允许电流通过，在反向偏置下阻止电流通过。这使得整流二极管可以用于电路中的整流（将交流信号转换为直流信号）和电流保护等应用&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240303122125.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;二极管等效电路&#34;&gt;二极管等效电路&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240303122328.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;二极管微变等效电路&#34;&gt;二极管微变等效电路&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;静态工作点：二极管添加直流偏置，此时在伏安特性曲线上所对应的点为静态工作点，此时可将二极管等效为一个动态电阻 公式&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;稳压二极管&#34;&gt;稳压二极管&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;硅管反向击穿时，在一定反向电流范围内，可表现出稳压特性&lt;/li&gt;
&lt;li&gt;要保证在一定功率下工作，超出该功率就会爆炸&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>我做了一个异世界的梦</title>
        <link>https://Dendrobium123.github.io/p/%E6%88%91%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BC%82%E4%B8%96%E7%95%8C%E7%9A%84%E6%A2%A6/</link>
        <pubDate>Sat, 24 Feb 2024 20:46:50 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E6%88%91%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BC%82%E4%B8%96%E7%95%8C%E7%9A%84%E6%A2%A6/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/cover terraria.jpg" alt="Featured image of post 我做了一个异世界的梦" /&gt;&lt;h1 id=&#34;纪念一下terraria大师模式的通关&#34;&gt;纪念一下terraria大师模式的通关&lt;/h1&gt;
&lt;p&gt;​	虽然之前就知道terraria有四种职业玩法，但从来没试过只用一种职业搭配通关游戏，因为这个游戏的自由度太高了，在没有了解功略的情况下很难自发有意识地控制自己的套装搭配，我想大部分人玩这个游戏都是所谓的散人套装吧。不得不说，分工明确的开荒确实能提高不少效率，在一昧的战斗之外，我也是第一次发现了建筑的乐趣，来看看我造的小家吧！&lt;/p&gt;
&lt;h2 id=&#34;我造的小家&#34;&gt;&lt;em&gt;我造的小家&lt;/em&gt;&lt;/h2&gt;
&lt;h4 id=&#34;一些内饰&#34;&gt;&lt;em&gt;&lt;strong&gt;一些内饰&lt;/strong&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221161415_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221161346_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221161256_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221160402_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221160534_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221160844_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221161523_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240219203923_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221161454_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;center&gt;这次玩的时候不管是跟npc买还是从各种地牢和洞穴拆，总之弄了超多的画~~&lt;center&gt;
&lt;h4 id=&#34;一些外景&#34;&gt;&lt;em&gt;一些外景&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;&lt;del&gt;这次跟视频学了一下造鸟居和小桥，我只能说雀食好看&lt;/del&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221161204_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221161037_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/20240221161023_1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;最后还有我们四人的大合影&#34;&gt;最后还有我们四人的大合影！&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/1ca2fa7afb21fc1fd3e3349f81635d7.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/81139fc77f34c40ddb71ae9ff7b9fc8.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/d6e7d98f14f5afb91828e7268589d7b.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;“我见证了一位星子的诞生，是的，一位星子”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;“星子的诞生使这个世界开始动摇”&lt;/p&gt;
&lt;p&gt;“星子杀掉了那只夜晚中的可怖眼球”&lt;/p&gt;
&lt;p&gt;“星子斩断了腐化之地的那条巨大的虫子”&lt;/p&gt;
&lt;p&gt;“星子绞碎了那猩红之地中大脑的幻想”&lt;/p&gt;
&lt;p&gt;“星子碾碎了那暴君之骨，让地牢重见天日”&lt;/p&gt;
&lt;p&gt;“我指引星子来到地狱，让他献祭出世界之墙”&lt;/p&gt;
&lt;p&gt;“古老的光明与黑暗之魂已被释放”&lt;/p&gt;
&lt;p&gt;“星子敲碎了那危险是祭坛，挖掘那些不曾拥有过的矿物”&lt;/p&gt;
&lt;p&gt;“这将是一个恐怖的夜晚……你感到来自于大地的震动……你感觉周围的空气越来越冷……”&lt;/p&gt;
&lt;p&gt;“地面那巨大的机械蠕虫以一种奇怪的姿势停下，天空中的那对双眼一直注视着什么，寒冷的机械骨骼包围住了一个人”&lt;/p&gt;
&lt;p&gt;“星子来到丛林，来寻找那粉色的花苞”&lt;/p&gt;
&lt;p&gt;“我听到了一声刺耳的尖叫，是那个古老的花朵的声音”
“星子向我告别，或许他很难再回来”&lt;/p&gt;
&lt;p&gt;“我听到了石头坠落的声音，神庙中的守护者轰然倒塌”&lt;/p&gt;
&lt;p&gt;“我注视着地牢，从白昼看到黑夜，地牢上方闪耀出奇怪的光芒”
“天界生物已入侵”&lt;/p&gt;
&lt;p&gt;“星子斩断着那四根天界之柱，拿到天界碎片，合成出那来自天界一般的武器”&lt;/p&gt;
&lt;p&gt;“我感到视线越来越暗，来自那位不可一世的神明，而那不可一世的神明即将降世，我提醒着星子，星子却说他早有准备”&lt;/p&gt;
&lt;p&gt;“天空化为黑暗，四周变得昏沉，在一个耀眼的光芒中，那位神明，降临于泰拉大陆上”&lt;/p&gt;
&lt;p&gt;“我注视着他们的战斗，很快，我瞪大双眼，看到了我这辈子都不曾想象过的景象”&lt;/p&gt;
&lt;p&gt;“神明歪着头说到：&amp;lsquo;你并不是神，但你有着神一般的光芒，你杀死了我，而你的征途却不止于此。&amp;rsquo;”&lt;/p&gt;
&lt;p&gt;“神明死亡，而星子，化为了真正的星。”&lt;/p&gt;
&lt;p&gt;&lt;del&gt;末尾的文字来源b站up主雁老师&lt;/del&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>第一篇博客</title>
        <link>https://Dendrobium123.github.io/p/hello-world/</link>
        <pubDate>Fri, 23 Feb 2024 15:46:50 +0800</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/hello-world/</guid>
        <description>&lt;img src="http://tuchuang.dendrobiumcgk.chat/pic/cover2.jpg" alt="Featured image of post 第一篇博客" /&gt;&lt;h2 id=&#34;我只想说做blog好难尤其是对我这种毫无经验的若至来说&#34;&gt;&lt;em&gt;我只想说，做blog好难，尤其是对我这种毫无经验的若至来说&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;markdown语法现在也没研究明白，不知道要怎么弄&lt;/p&gt;
&lt;p&gt;第一次发博客就随便放两张我们胡桃的照片好了，啊~真可爱啊。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/VRChat_2024-01-22_00-01-45.608_2560x1440.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/VRChat_2024-01-21_23-46-55.640_2560x1440.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>文档(っ °Д °;)っ</title>
        <link>https://Dendrobium123.github.io/archives/</link>
        <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
        
        <guid>https://Dendrobium123.github.io/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>《加缪笔记》读书笔记</title>
        <link>https://Dendrobium123.github.io/p/%E5%8A%A0%E7%BC%AA%E7%AC%94%E8%AE%B0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E5%8A%A0%E7%BC%AA%E7%AC%94%E8%AE%B0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h3 id=&#34;加缪笔记1935-1959&#34;&gt;加缪笔记：1935-1959&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;美是难以容忍的，它使我们绝望，我们希望一瞬即为永恒，拉长全部的时间。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将我们从最大的痛苦中拯救出来的，是那种被抛弃的、孤立无援的感觉，但还是孤立不到以下程度，以至于”另外的人“在我们的不幸中”观察“我们。在这个意义上，我们片刻的幸福时光有时是，被抛弃的感觉充实了我们，引起了我们无边的忧愁。也是在这种意义上，幸福只不过是对我们不幸的一种怜悯感情，在穷人中令人印象深刻的——上帝将得意置于绝望旁边正如将解药置于病痛旁边。&lt;/p&gt;
&lt;p&gt;（幸福是通过对比得出的）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果一种焦虑压迫我，那是因为感觉到一种不可触及的瞬间在指间流过，像水银珠一样&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>读书笔记</title>
        <link>https://Dendrobium123.github.io/p/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</guid>
        <description>&lt;h3 id=&#34;heading&#34;&gt;&lt;/h3&gt;
&lt;h2 id=&#34;电子学paul-horowitz&#34;&gt;《电子学》Paul Horowitz&lt;/h2&gt;
&lt;h3 id=&#34;第一章-电子学基础&#34;&gt;第一章-电子学基础&lt;/h3&gt;
&lt;h4 id=&#34;电阻&#34;&gt;电阻&lt;/h4&gt;
&lt;p&gt;在放大器中，电阻常被用作有源器件负载，偏置电路或反馈原件，与电容结合可形成时间常数以作滤波器使用。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在电源电路中损耗功率，以减小相应电压&lt;/li&gt;
&lt;li&gt;在逻辑电路中作为总线和线路终端以及“上拉”和“下拉”电阻&lt;/li&gt;
&lt;li&gt;在高压电路中用于测量电压与均衡串接中的二极管or电容的泄露电流&lt;/li&gt;
&lt;li&gt;在射频电路中，甚至可以作为线圈以取代电感&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最常用电阻为1/4w和1/2w的碳合成电阻&lt;/p&gt;
&lt;h5 id=&#34;串联并联&#34;&gt;串联并联&lt;/h5&gt;
&lt;p&gt;最佳电路设计的标准是，已完成的电路对元件的精度值变化不敏感。&lt;/p&gt;
&lt;h5 id=&#34;输入输出&#34;&gt;输入输出&lt;/h5&gt;
&lt;p&gt;通常用传递函数H研究这个问题，H传递函数是已测量到的输出与所加输入之比&lt;/p&gt;
&lt;h5 id=&#34;戴维南等效电路&#34;&gt;戴维南等效电路&lt;/h5&gt;
&lt;p&gt;任何一个有电阻与电压源连接的二端口网络可以等效为一个电阻R与一个电压源V串联的电路&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将电源挨个接地用叠加定理计算端口的等效电压&lt;/li&gt;
&lt;li&gt;用开路电压与短路电流求出等效电阻&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;等效源电阻与电路负载效应&#34;&gt;等效源电阻与电路负载效应&lt;/h5&gt;
&lt;p&gt;在无源分压电路的输出端接负载电阻会使分压器的输出电压下降（由源电阻引起），因此需要一个“坚挺”电压源（即输出电压不随负载加入而变化），通常在分压电路中采用小电阻或使用晶体管或运放等有源器件。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;由于负载而引起这种不希望的开路电压下降的现象被称为“电路加载效应”，应使$R_{load}&amp;raquo;R_{internal}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;功率传输&#34;&gt;功率传输&lt;/h5&gt;
&lt;p&gt;在$R_{load}=R_{source}$时信号源到负载的功率传输可取最大值&lt;/p&gt;
&lt;h5 id=&#34;小信号电阻&#34;&gt;小信号电阻&lt;/h5&gt;
&lt;p&gt;对流经电流I不与两端V成比例的非线性元器件，可以通过对其V-I曲线上施加电压微小变化以微积分中的微分思想来在一个小邻域内进行线性近似，从而获得$$ΔV/ΔI$$即dV/dI，这个被称为小信号电阻，增量电阻或动态电阻。&lt;/p&gt;
&lt;h4 id=&#34;信号&#34;&gt;信号&lt;/h4&gt;
&lt;h5 id=&#34;正弦信号&#34;&gt;正弦信号&lt;/h5&gt;
&lt;p&gt;最大优点是，它恰好是描述自然界中许多现象以及线性电路特性的微分方程的解&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;线性电路特性：两个输入信号之和激励的输出相应等于单个输入分别激励的输出相应之和即线性特性满足于：$$O(A+B)=O(A)+O(B)$$&lt;/li&gt;
&lt;li&gt;频率响应：电路是输出正弦波的幅度特性随输入正弦波频率的函数关系而改变&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;信号幅度与分贝&#34;&gt;信号幅度与分贝&lt;/h5&gt;
&lt;p&gt;描述信号幅度的量&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;峰峰值（pp）：2倍幅度值&lt;/li&gt;
&lt;li&gt;均方根值（rms）：$$V_{rms}=\frac{1}{\sqrt{2}}A=0.707A$$&lt;/li&gt;
&lt;li&gt;分贝：比较两个信号相应幅度的大小
&lt;ol&gt;
&lt;li&gt;分贝为1贝的1/10，“贝”现在不常用了&lt;/li&gt;
&lt;li&gt;$$dB=20lg\frac{A_2}{A_1}$$,其中A1与A2分别表示两个信号的幅度，由于lg2=0.3010，则对应的比值分贝数为6dB&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;其他信号&#34;&gt;其他信号&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;斜坡信号，三角波信号&lt;/li&gt;
&lt;li&gt;噪声：源于热随机噪声，其电压特性可以用其功率谱密度或其振幅分布来描述，常见噪声有&lt;strong&gt;带限高斯白噪声&lt;/strong&gt;，其功率谱密度在某一段频率内是相等的；对噪声电压的振幅进行大量即使测量时，其振幅满足高斯分布，由电阻产生（johnson噪声）&lt;/li&gt;
&lt;li&gt;方波，其边缘不是方的，有一个几纳秒到几毫秒间的上升斜坡&lt;/li&gt;
&lt;li&gt;脉冲：由脉冲振幅与脉冲宽度定义；“占空比”（脉冲宽度与周期之比）&lt;/li&gt;
&lt;li&gt;阶跃与尖脉冲，阶跃函数u(t)，尖脉冲是一个持续时间很短的跳跃信号&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;逻辑电平&#34;&gt;逻辑电平&lt;/h5&gt;
&lt;p&gt;即高电平和低电平，分别对应布尔逻辑状态中的0与1，精确的电压在数字电路中是不必要的，只需区分两种状态即可&lt;/p&gt;
&lt;h5 id=&#34;信号源&#34;&gt;信号源&lt;/h5&gt;
&lt;p&gt;包括信号发生器，脉冲发生器与函数发生器&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;信号发生器：正弦振荡器，给出一个宽频域范围内的信号并有精确的幅度控制装置（通常为一个成为衰减器的电阻分压器）&lt;/li&gt;
&lt;li&gt;脉冲发生器：只产生脉冲信号，且信号的宽度、频率、振幅、极性和上升时间均可调&lt;/li&gt;
&lt;li&gt;函数发生器：最灵活的一种信号源，可以得到正弦波，三角波与方波函数&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;电容与交流电路&#34;&gt;电容与交流电路&lt;/h4&gt;
&lt;p&gt;一旦进入信号的世界，就会用到两种在直流电路中用的很少但在交流电路中却非常有用的元件：电容与电感；电容差不多是每种电路应用的基本元件，可用于波形的产生，滤波，阻塞与旁路也可用于积分器与微分器。在与电感结合应用时，可构成一种特性尖锐的滤波器以从背景噪声中滤出所需信号。&lt;/p&gt;
&lt;h5 id=&#34;电容&#34;&gt;电容&lt;/h5&gt;
&lt;p&gt;性质  $$Q=CV$$:在一个具有C法拉的电容两端加V伏电压时，该电容的一个极板上就有Q库伦的电荷存储，另一块极板为-Q库伦的电荷。&lt;/p&gt;
&lt;p&gt;对此式两端对时间t求导可以得到： $$I=C\frac{dV}{dt}$$&lt;/p&gt;
&lt;p&gt;电容的基本结构就是两块导体相互靠近（但不接触），对较大的电容需要让两块导体的面积更大，靠得更近。通常将导体放在一块薄的绝缘材料上&lt;/p&gt;
&lt;h5 id=&#34;电容的串并联&#34;&gt;电容的串并联&lt;/h5&gt;
&lt;p&gt;电容并联为电容值之和：$$C_{total}V=Q_{total}=C_1V+C_2V+C_3V+&amp;hellip;$$&lt;/p&gt;
&lt;p&gt;即$$C_{total}=C_1+C_2+C_3+&amp;hellip;$$&lt;/p&gt;
&lt;p&gt;电容串联关系式为电阻并联关系式&lt;/p&gt;
&lt;p&gt;$$C_{total}=\frac{1}{\frac{1}{C_1}+\frac{1}{C_2}+\frac{1}{C_3}+\frac{1}{C_4}+&amp;hellip;}$$&lt;/p&gt;
&lt;h5 id=&#34;rc电路随时间变化的v与i&#34;&gt;RC电路：随时间变化的V与I&lt;/h5&gt;
&lt;p&gt;对微分方程$$C\frac{dV}{dt}=I=-\frac{V_i-V}{R}$$有通解：$$V=V_i+Ae^{-\frac{t}{RC}}$$&lt;/p&gt;
&lt;p&gt;RC被成为时间常数，原电路为在t=0时接上电源。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;衰减至平衡状态：当t&amp;raquo;5RC时，V升至Vi，即在5倍的时间常数内，一个电容充电或放电到最终值的1%范围内&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;微分器&#34;&gt;微分器&lt;/h5&gt;
&lt;p&gt;$$V(t)=RC\frac{d}{dt}V_{in}(t)$$,电容和负载串联，测负载上的电压变化为输入电压的导数&lt;/p&gt;
&lt;h5 id=&#34;积分器&#34;&gt;积分器&lt;/h5&gt;
&lt;p&gt;R与C串联，输出接在C的两端&lt;/p&gt;
&lt;p&gt;$$V(t)=\frac{1}{RC}\int_{0}^{t} V_{in}(t)dt+常数$$&lt;/p&gt;
&lt;h4 id=&#34;电感与变压器&#34;&gt;电感与变压器&lt;/h4&gt;
&lt;h5 id=&#34;电感&#34;&gt;电感&lt;/h5&gt;
&lt;p&gt;电感中的电流变化率取决于它两端所加电压
定义式：$$V=L\frac{dI}{dt}$$,单位为亨利H，在电感两端加电压会引起电流以斜坡函数形式上升，电感与电容类似，其相关功率没有转化为热量而是以能量的形式存储在电感的磁场中。
电感在射频电路中用途最多。作为射频扼流圈成为调谐电路的一部分，一对紧密耦合的电感可以构成变压器。&lt;/p&gt;
&lt;h5 id=&#34;变压器&#34;&gt;变压器&lt;/h5&gt;
&lt;p&gt;两个紧密耦合线圈，分别称为初级与次级。在初级加交流电压会引起次级电压出现，次级电压以变压器匝数比的倍数增加，电流则与匝数比成反比，因为总功率不变。&lt;/p&gt;
&lt;p&gt;变压器的功率传输效率高。
电源变压器能输出大量不同类型的次级电压与电流。&lt;/p&gt;
&lt;h5 id=&#34;阻抗与电抗&#34;&gt;阻抗与电抗&lt;/h5&gt;
&lt;p&gt;一个包含电容与电感的分压器会有一个依赖于频率的分压比，会使输入波形变形
但电容与电感本身上线性元件，其线性表示为：由某个频率f的正弦波激励的线性电路的输出本身也是同频率的正弦波（至多改变幅度和相位）
阻抗是推广的电阻，电感与电容有电抗，电阻器有电阻。
1.阻抗=电阻+电抗
阻抗已经包含了一切&lt;/p&gt;
&lt;h4 id=&#34;电抗电路的频率分析&#34;&gt;电抗电路的频率分析&lt;/h4&gt;
&lt;h5 id=&#34;电压与电流的复数表示&#34;&gt;电压与电流的复数表示&lt;/h5&gt;
&lt;p&gt;为了同时表示关于幅值与相移，引入&lt;strong&gt;复数&lt;/strong&gt;，通过对复数表达式进行加减就无需再对三角函数进行运算，电子学中虚数符号通常用j而不是i，为了避免和电流i搞混&lt;/p&gt;
&lt;p&gt;$$V(t)=Re(Ve^{jωt})=Re(V)cosωt-Im(v)sinωt$$
$$I(t)=Re(Ie^{jωt})=Re(I)cosωt-Im(I)sinωt$$
在原先的代数式两边乘$$e^{jωt}$$&lt;/p&gt;
&lt;h5 id=&#34;电容与电感的电抗&#34;&gt;电容与电感的电抗&lt;/h5&gt;
&lt;p&gt;由$$V(t)=Re(V_oe^{jωt})$$
而$$I=C\frac{dV}{dt}$$
克制$$I(t)=-V_0Cωsinωt=Re(\frac{V_0e^{jωt}}{-\frac{j}{ωC}})=Re(\frac{V_0e^{jωt}}{X_c})$$
由此推出一个电容的电抗为$$-\frac{j}{ωC}$$
类似的，电感的电抗为$$jωL$$
在只含有电容与电感的电路中总含有一个纯的虚数，这说明电压与电流间总有90度的相位差&lt;/p&gt;
</description>
        </item>
        <item>
        <title>回忆最初的感动</title>
        <link>https://Dendrobium123.github.io/p/%E5%9B%9E%E5%BF%86%E6%9C%80%E5%88%9D%E7%9A%84%E6%84%9F%E5%8A%A8/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://Dendrobium123.github.io/p/%E5%9B%9E%E5%BF%86%E6%9C%80%E5%88%9D%E7%9A%84%E6%84%9F%E5%8A%A8/</guid>
        <description>&lt;h5 id=&#34;缘起&#34;&gt;缘起&lt;/h5&gt;
&lt;p&gt;​		其实我很久前就想做和这次类似的事情了，b站有个up主叫@The梁某人，他的视频定位是童年回忆区，顾名思义就是和大家一块回忆小时候看过的动画片，从以前比较火爆的《猪猪侠》，《果宝特攻》到成龙历险记，高米迪，ben10等稍微冷门的动画片，他都有所涉及，并且我是每次更新都不会拉下，看得津津有味。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/image-20241230203454766.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20241230203454766&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;​		要说为什么，可能因为小时候和同学回忆动画片主题曲的时候就数我一个记的最全吧，毕竟对我来说就没几首歌会唱。每次去ktv，周围的小伙伴要么唱各种流行音乐像周杰伦林俊杰，要么就是唱一些外语歌，而我一张口就是各种动画片主题曲，小时候甚至还会在早上出门前专门用mp3单曲循环几遍动画片的歌曲从而能在课间的动画片回忆上能遥遥领先。&lt;/p&gt;
&lt;p&gt;​	&lt;strong&gt;今天想说的便是这些稍微一遐想似乎就能让人回到过去的动画片，或者说，一种能让人穿越时空的，怀旧的介质。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当然，我不是说[过去的动画片=怀旧]，它只是一把能开启过去时空的钥匙，而这把钥匙对每个人都是不同的，它可以是一件物品，一段文字，甚至是一股气味。对我来说最好的钥匙，就是动画片的主题曲和那模糊的画面。&lt;/p&gt;
&lt;img src=&#34;http://tuchuang.dendrobiumcgk.chat/pic/fdede6dcd588cd17e3f41a1748da070.jpg&#34; alt=&#34;fdede6dcd588cd17e3f41a1748da070&#34; style=&#34;zoom:50%;&#34; /&gt;
&lt;h5 id=&#34;先说说几部让我印象深刻的动画片吧&#34;&gt;先说说几部让我印象深刻的动画片吧&lt;/h5&gt;
</description>
        </item>
        <item>
        <title>检索(＠_＠;)</title>
        <link>https://Dendrobium123.github.io/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://Dendrobium123.github.io/search/</guid>
        <description></description>
        </item>
        <item>
        <title>友链( •̀ ω •́ )</title>
        <link>https://Dendrobium123.github.io/%E5%8F%8B%E9%93%BE-%CC%80-%CF%89-%CC%81/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://Dendrobium123.github.io/%E5%8F%8B%E9%93%BE-%CC%80-%CF%89-%CC%81/</guid>
        <description>&lt;h4 id=&#34;heading&#34;&gt;&lt;/h4&gt;
&lt;h4 id=&#34;姑且先放vrc和同学还有网友们的链接吧&#34;&gt;姑且先放&lt;del&gt;vrc和&lt;/del&gt;同学还有网友们的链接吧（&lt;/h4&gt;
&lt;h4 id=&#34;交换友链可以给我发邮件哦&#34;&gt;交换友链可以给我发邮件哦！&lt;/h4&gt;
</description>
        </item>
        
    </channel>
</rss>
