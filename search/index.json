[{"content":"PyTorch基础笔记_01 常用库：torch，numpy，pandas，matplotlib\n1 2 3 4 5 import torch import numpy as np import torch.nn as nn from torch import optim as optim#优化器 import torch.nn.functional as F#用到的函数 基础使用 假设一个最简单的神经网络，没有隐藏层，只有输入层和输出层\neg：5个输入神经元，7个输出神经元，全连接 $$ y=w\\cdot x+b $$ x为输入层，形状为（1,5）； $$ \\begin{bmatrix}x_1 \u0026amp; x_2\u0026amp;x_3\u0026amp;x_4\u0026amp;x_5\\end{bmatrix} $$ Y为输出层，形状为（1,7）#同上 $$ \\begin{bmatrix}y_1 \u0026amp; y_2\u0026amp;y_3\u0026amp;y_4\u0026amp;y_5\u0026amp;y_6\u0026amp;y_7\\end{bmatrix} $$ w为权重，形状为（5,7) #全连接， $$ \\begin{bmatrix}w_{11}\u0026amp;w_{12}\u0026amp;w_{13}\u0026amp;w_{14}\u0026amp;w_{15}\u0026amp;w_{16}\u0026amp;w_{17} \\\\w_{21} \u0026amp; w_{22}\u0026amp;w_{23}\u0026amp;w_{24}\u0026amp;w_{25}\u0026amp;w_{26}\u0026amp;w_{27} \\\\w_{31} \u0026amp; w_{32}\u0026amp;w_{33}\u0026amp;w_{34}\u0026amp;w_{35}\u0026amp;w_{36}\u0026amp;w_{37} \\\\w_{41} \u0026amp; w_{42}\u0026amp;w_{43}\u0026amp;w_{44}\u0026amp;w_{45}\u0026amp;w_{46}\u0026amp;w_{47} \\\\w_{51} \u0026amp; w_{52}\u0026amp;w_{53}\u0026amp;w_{54}\u0026amp;w_{55}\u0026amp;w_{56}\u0026amp;w_{57} \\end{bmatrix} $$ b为bias偏置，每个输出神经元输出前都会经过bias调整，所以形状为7\n1 2 3 4 5 6 7 8 9 10 11 12 13 #定义各个参数 w=torch.randn(5,7,requires_grad=True)#生成5*7的随机数矩阵来填充w,同时设定w是需要后续进行梯度下降更新的属性，后半部分缺失会导致程序报错 b=torch.randn(7,requires_grad=True) x=torch.randn(1,5) Y=torch.randn(1,7) lr=0.001#设置学习率 y=F.relu(x @ w + b) #x与w为矩阵，使用矩阵乘法@,同时调用事前导入为F的激活函数，这里选择relu函数（1） #开始求loss，假定本案例为一个分类任务，选择交叉熵损失函数 loss=F.cross_entropy(y,Y) #开始求参数梯度，进行梯度下降算法更新参数，pytorch提供了一个函数方法backward()，可以直接帮助我们找到所有参数梯度，无需自己算 loss.backward()#这步只求了梯度，顺利运行的前提是需要提前设定参数是需要求梯度的属性 w.grad#查看w经过梯度下降所得的参数值，！还没有进行参数的更新！见（2） w=w -lr * w.grad#这步才是更新了参数（3） 两种损失函数 1.分类任务一般使用交叉熵损失函数：\n$$ \\text{Cross-Entropy}= -\\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)] \\\\y_i：\\text{ 类别的真实标签（通常为0或1）} \\\\\\hat{y_i}: \\text{对应类别的预测概率} $$\n2.回归任务一般使用均方误差损失函数\n$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\y_i：\\text{真实值} \\\\\\hat{y_i}:预测值 \\\\n:样本数量 $$ 损失函数使用时需注意该函数的传参\n（1）结果：\n（2）结果：\n（3）结果：\n搭建模型 搭建网络需要定义一个类，eg此处输入图像为rgb彩色图像，像素为48*48\nps: jupyter lab查看函数参数快捷键shift+tab\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class M_CNN(nn.Module):#定义网络，父类为之前导入的nn类 def __init__(self):#初始化 super().__init__()#父类的函数方法 self.conv1 =nn.Conv2d(3,16,kernel_size=3,padding=1)#设计的卷积层，调用nn父类中的Conv2d卷积函数，参数见（4）eg：输入为彩色图像（输入为3），输出16个通道，卷积核大小为3*3；padding=1为填充1像素保证输出的长宽不变 self.conv2 =nn.Conv2d(16,32,kernel_size=3,padding=1) self.bn1=nn.BatchNorm2d(16)#规范化，通道数为16#第一层卷积的输出通道数 self.bn2=nn.BatchNorm2d(32) self.MaxPool=nn.MaxPool2d(kernel_size=2)#池化层 self.fc1=nn.Linear(12*12*32,1000)#网络全连接层，将处理完的输入图像所有像素拼接为一维，加入中间有1000个神经元的隐藏层 self.fc2=nn.Linear(1000,100)#1000个隐藏层1神经元，最送到100个隐藏层2神经元，由于12*12*32的输入太大，故需要不同神经元数目的隐藏层过渡 self.fc3=nn.Linear(100,7)#最后输出为7个神经元 def forward(self,x): x=F.relu(self.bn1(self.conv1(x)))#输入先过卷积，后过规范器，最后通过激活函数relu，更新原有输入 x=self.MaxPool(x)#经过池化后图像宽高对半减 x=F.relu(self.bn2(self.conv2(x)))#经过第二个卷积层 x=self.MaxPool(x) x=x.view(-1,12*12*32)#进入全连接之前，需要讲3维张量撑成1维的向量，-1代表可能用小批量进行训练，不确定数目，代表自适应，后面的数值代表进入的张量 x=F.relu(self.fc1(x))#进入全连接 x=F.relu(self.fc2(x)) x=F.relu(self.fc3(x)) return x 全连接层参数计算： $$ 一个(3,48\\times48)图像通过一个(3,16,kernel=3\\times3,步长为1)的卷积层，\\\\输出为(16,48\\times48)图像，\\\\通过第一个池化层，窗口为2\\times2,步长为2，\\\\输出为(16，24\\times24),\\\\经过第二个卷积层(16，32，kernel=3\\times3,步长为1)\\\\输出为(32,24\\times24)，\\\\通过第二个池化层，变为(32，12\\times12) $$\n（4）nn.Conv2d函数的参数：\n数据处理 读取数据，转化为dataset，再由dataset转化为dataloader\n1 2 3 4 5 6 7 8 9 from torch.utils.data import DataLoader from torchvision import datasets,tranforms from torchvision.datasets import ImageFolder transform=transforms.Compose([ transforms.ToTensor()#读取图像后将数据转换为tensor类型 transforms.Normalize(mean=(0.5,),std=(0.5,))#去均值，除标准差的归一化操作 ]) dataset=ImageFolder(\u0026#39;file_path\u0026#39;,tansform=transform)#通过imagefolder来为读取的数据进行标注，后半部分使用tranform进行格式转化 dataloader=DataLoader(dataset,batch_size=32,shuffle=True)#shuffle意义为每个batch开始时顺序是否要被打乱 跑模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #检测GPU使用情况 device=torch.device(\u0026#39;cuda:0\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) print(\u0026#39;Using device\u0026#39;,device) #创建模型 model=CNN_M().to(device) #定义损失函数 loss_fun=F.cross_entropy #定义优化器 opt=optim.Adam(model.parameters(),lr=0.01)#parameters为模型的所有参数 epochs=8#以batchsize为32的步长，每个批次通过一遍网络，直至所有样本被扫完作为一个epoch for epoch in range(epochs): for i,data in enumerate(dataloader,0):#每次的小batch做的事 inputs,labels=data[0].to(device),data[1].to(device)#input为作为输入的数据，labels为获取标签，to(device)为把数据往显卡里送 y=model(inputs)#进入模型的forward第一层到最后一层，结果给y loss=F.cross_entropy(y,labels)#通过计算得的y与labels标签正确答案进行损失计算 loss.backward()#计算梯度值 opt.step()#优化器进行梯度下降，更新参数 opt.zero_grad()#参数的梯度清零，不然进入下一轮梯度计算时会累积上一次的结果 print(i,loss) ps:最后来一张之前tutu带我看的不认识的Vtuber演唱会 ","date":"2024-04-18T21:02:02+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/pytorch_0.jpg","permalink":"https://Dendrobium123.github.io/p/pytorch%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0_01/","title":"PyTorch基础笔记_01"},{"content":"\r深度学习基础知识 几种“学习”间的关系\n机器学习\u0026mdash;最大的概念 \u0026lt;让机器通过学习的方式得到一个可以解决问题的模型\u0026gt;\n学习方法：KNN（K近邻），k means（k均值解决聚类问题），SVM，深度学习\n机器学习，隐式学习\n神经网络：输入层，隐藏层，输出层\n神经元\n不改变网络层和算法的情况下，影响输出结果的是各神经元连接线路上的数值权重\n除了一系列加减乘除的线性变换外，还引入了激活函数\n激活函数：阶跃函数（不用 希望通过梯度下降的方式求得参数更新的过程，阶跃函数无法正常求导，需要引入δ函数，因此使用别的函数作为激活函数Sigmoid，以此解决阶跃函数不可导的问题\nSigmoid： $$ S(x)=\\dfrac{1}{1+e^{-x}} $$\nsigmoid导数： $$ S\u0026rsquo;(x)=\\dfrac{e^{-x}}{(1+e^{-x})^2}=S(x)(1-S(x)) $$ Sigmoid函数及其导数的图像：\n注：取值范围在0—1间的sigmoid函数叫logistic函数\rtanh： $$ tanh=\\dfrac{e^x-e^{-x}}{e^x+e^{-x}} $$\n范围在（-1，1）间的激活函数\nRelu函数 $$ f(x)=\\begin{cases}x \u0026amp; x\\geq0 \\\\0\u0026amp;x\u0026lt; 0\\end{cases} $$\n每个神经元所做的事： $$ g_{output}=g(w_1\\times a+w_2\\times b+w_3\\times c+w_4\\times d)—\u0026gt;relu $$ 注：a,b,c,d为权重值，神经元输出结果为各参数加权后通过一个relu函数所得的值\n机器学习的目的是在给定前提情况下，寻找能得到最好输出的w参数们\n梯度下降 如何寻找需要的W\n通过当前所计算得出的结果与已知的正确结果做差，考虑到所得结果的正负号问题，采用对式子求平方的方式（平方好求导，绝对值不好求导） $$ L=(f(x)-y)^2 $$\nL越小，模型性能越好，f(x)与参数w有关，因此L也是个关于w的函数。\n可以通过调整w来使L的取值变小\n动态更新W，eg：初始值w0，第一刻w1\u0026hellip;.. $$ W_1=W_0-lr\\cdot \\frac{\\partial L}{\\partial W_0} \\\\W_2=W_1-lr\\cdot \\frac{\\partial L}{\\partial W_1} \\\\\u0026hellip; $$\n局部最优/全局最优 类似高等数学函数章节中的极值和最值问题，局部导数为0的极值点不代表此处是整个函数的极值\n~~乐经良：说明它是一个地头蛇~~\rps:顺带吐槽一句，这个hugo对LateX数学公式的键入好像不是很友好，比如公式间的换行用要用\\\\，但是他识别代码的时候只识别一条杠\\，这就导致像 $$ f(x)=\\begin{cases}x \u0026amp; x\\geq0 \\\\0\u0026amp;x\u0026lt; 0\\end{cases} $$ 这种分段的函数会显示成这样 $$ f(x)=\\begin{cases}x \u0026amp; x\\geq0 \\0\u0026amp;x\u0026lt; 0\\end{cases} $$\nbyd后来我发现你只需要打3个\\就能解决问题了。\n","date":"2024-04-14T12:20:50+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/深度学习00cover.jpg","permalink":"https://Dendrobium123.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_00/","title":"深度学习_00"},{"content":"这篇文章是我之前一门通识课所布置的作业，今天闲的没事就想水一篇文章。\n小石我纵横现代简中互联网多年，对如今的网络文化也有一定的思考，借着通识课作业的契机一并谈谈我对当今网络抽象话的了解。\n中国后现代主义与网络亚文化的发展 以“抽象文化”为例 近年来，随着网络社交平台和短视频行业的发展，我们越来越不难注意到，有这样一类越来越频繁浮现的群体。他们往往说着荒诞又难以理解的话语，洒脱地嘲讽或攻击一些公众形象，同时又无时不刻进行着自我矮化。这类群体在后疫情时期越发壮大逐渐从网络世界的边缘浮现至仅次于主流文化的亚文化地位。在理解了后现代主义和解构主义后，我们可以说这类“抽象文化”的诞生和扩张正是后现代主义思潮在当今中文网络世界的具体表现之一。\n后现代主义一词在法国哲学家利奥塔出版了《后现代状态》一书后正式出道，它是一种基于对西方现代社会进行反思和批判而形成的文化思潮。而其核心理论之一便是宏大叙事的消亡。宏大叙事被认为是现代性的一种体现，是一种使特定意识形态世界观乃至价值观正当化的趋势，也是一种对事物运行规律和人类历史进程的宏观解释。但后现代主义主张对本质的否定和对非理性的推崇，强调多元性、差异性，反对同一性，并具有不确定性、分裂性、解构性及戏谑性等特点。而解构主义则是由法国哲学家德里达提出的，其原本的主张是对中心主义的一种批判，是破坏有中心的二元对立状态，而在今天，解构一词作为一种行为可以用于任何对象和场合，成为了后现代主义中的一环 。\nJean-Francois Lyotard\nJacques Derrida 为什么说目前网络抽象文化的扩张是中国后现代主义的具体表现呢？ 首先抽象文化本身仅仅只是一个指代，其本身并没有具体的定义，因为其本身就是网络去中心化的产物。这批亚文化的创作者们在17,18年左右伴随着李赣和孙笑川之流的直播间而诞生，但值得注意的是【孙笑川】本身并不是该亚文化的定义，它只是抽象文化的一个符号。而抽象文化的符号是无穷无尽的，因为其本身就是一种会无序扩张的社会模因。我在这里举一些最近抽象文化的新网络符号，比如说“yy丁真”系列；抽象小卖部系列；asoul女团；你说的对，但原神是一款XXX系列；东北往事；柯洁直播间等等，与之类似的还有油管的xqc，Ishowspeed，veibae等主播的直播间。抽象文化的群体很喜欢对一些互联网上的土味视频进行解构和二次创作，比如上述的抽象小卖部和东北往事等视频，其原本只是类似于通过卖丑，猎奇等行为换取播放量的视频，但在抽象文化的二次塑形中爆发了前所未有的生命力，原本的猎奇被视为荒诞的艺术，各类卖丑整活的行为也被视为一种对社会规训下的现实的反抗。\n安迪沃霍尔曾说，在未来，每个人都能出名15分钟。 因此，每个人也都可能被吸纳进抽象文化的符号中，因为当你进入了这个飞速迭代的网络世界后，发掘和被消费是在一个很短的周期内被完成的，一个梗，一个meme的生命力是很短暂的，尽管如此它们却可以在如此短的时间中扩张至互联网的每一个角落。但人们迫切地需要新的故事和梗来娱乐和消费，因此这种符号的产生就和人工智能训练集和产出内容一样，无时不刻不在自我迭代和吸收新的叙事。\n后现代主义在这其中中起到了推波助澜的作用。 一方面，后现代主义中的解构化、碎片化和去中心化等因素助推了抽象文化滋生。同时因为当今包含宏大叙事的现代主义在中国仍占据主导地位，对事物的评判仍保有一个中心化的标准，这正是抽象文化理所应当存在的土壤。\n与早年qq空间的杀马特之流不同，抽象文化本身的特点就是对传统鄙视链的彻底摧毁。\n曾经的杀马特文化有各式各类的家族，不同家族之间等级严明，在家族中的地位甚至与你加入的时间有关。**其本质与其说是对资本社会的反抗不如说是通过建立另一个帝国以此逃避原有的社会秩序。**但抽象文化圈却是其完全相反的另一面，比如在百度贴吧中一个人资历可以通过铭牌显示出来，如果入吧的时间早，发言帖子多会增加自己的经验从而提高在本吧的等级。而孙笑川吧中总会出现“黄牌赶紧给绿牌磕一个”这样的话术（黄牌等级高，绿牌等级低）。通过这样完全颠倒的权力体系说明了抽象文化的对鄙视链结构的否定。与之类似的，还有丁真的爆火。在21年初官方为了宣传少数民族和扶贫工作仅仅因为丁真的纯真笑容便将这位学历连小学都没有的文盲聘请至国企工作同时大肆宣传将其捧成了一个网红。大部分网民对此是愤慨且否认的，但批评并不能阻止这样的事发生，因此在后期他们选择将丁真彻底解构，如同第二个孙笑川一样，丁真也成为了抽象文化的符号，网民们肆无忌惮地发表丑化丁真的表情包，将一切他们认为由于官方宣传而火爆但德不配位的人称之为XX丁真，如赛博丁真，滑雪丁真等等。这体现了抽象文化的反权威性和多元性。\n抽象文化也有着和所有后现代主义思潮一样的弊病， 鲍德里亚说过：每个共同体的建构是不同的，但解构都是大同小异的 抽象文化下的网民们乐忠于这样的一种解构，乃至于任何事物在他们面前都只是一堆可供娱乐和消费的符号。一个最直接的例子“地狱笑话”这个词原意是笑了就要下地狱，因为其所取笑的对象本身是由于各种意外而发生悲剧的个体，比如侏儒症患者，残疾人，父母双亡等等。但地狱笑话吧却会以此为依据对其取笑，这样的一种解构是超脱道德的，他们会用草莓来嘲笑因生前想吃草莓，最后却因糖尿病而死的“墨茶”，用直升机戏谑科比等等。为所欲为的解构和消解严肃性成了他们唯一的目的，这本身就是荒诞的，同时也让他们失去了严肃讨论的土壤。\n“抽象文化”在[后现代]皮囊下的[现代性] 在这种后现代思潮的混乱扩张中，许多刚接触抽象文化的新网民会由于其现代化的思想，以遵从威权的思考角度盲目跟风，此时复制相关的话术成为了他们彰显自己抽象属性的方式，用这样一种不加以思考的刻奇的方式加入到后现代思潮的狂欢当中。就像维特根斯坦说的“人不是思考了再说话，而只是说话”。在这时抽象文化的后现代属性消失，它成为了现代化的一种延伸。\n中文互联网经历过各种非主流时期，从早年的杀马特时期，火星文时期，到如今的饭圈文化，抽象文化等。尽管他们迟早会被时间吞噬，但抽象文化作为后现代主义思潮在中国的一种具体体现，其本身将永久地改变整个中文互联网以至于改变新一代网民的思考方式。\n","date":"2024-03-03T13:10:42+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/db7641dcd583699a45fbb9a138a5679.jpg","permalink":"https://Dendrobium123.github.io/p/%E4%B8%AD%E5%9B%BD%E5%90%8E%E7%8E%B0%E4%BB%A3%E4%B8%BB%E4%B9%89%E4%B8%8E%E7%BD%91%E7%BB%9C%E4%BA%9A%E6%96%87%E5%8C%96%E7%9A%84%E5%8F%91%E5%B1%95%E4%BB%A5%E6%8A%BD%E8%B1%A1%E6%96%87%E5%8C%96%E4%B8%BA%E4%BE%8B/","title":"中国后现代主义与网络亚文化的发展以“抽象文化”为例"},{"content":" 模拟电路 模拟电路是指用来对模拟信号进行传输、变换、处理、放大、测量和显示等工作的电路。它主要包括放大电路、信号运算和处理电路、振荡电路、调制和解调电路及电源。 何为模拟？ 在模拟电路中，电压高低（或电流大小）是模拟了所代表的物理量的变化，例如声音信号，声音被话筒转换成电压时，电压的高低直接反映了音量大小，声音的频率（音调）直接就是电信号的频率，此谓模拟的概念。\n半导体基础 1.本征半导体：无杂质的稳定结构，其本身导电能力差（一般为+4价的硅晶体 其中不同原子的电子会形成共价键 载流子浓度低 易受温度影响 2.何为载流子？ 脱离共价键的电子称为自由电子（negative 原先的空缺位称为空穴（positive 方便数学处理，将空穴视为一种粒子，因此可与自由电子称为载流子 3.杂质半导体（在本征半导体中掺杂+5价元素和+3价元素 N型半导体：多数载流子为自由电子（掺杂5价元素磷，施主原子—\u0026gt;PN结构中为➕ P型半导体：多数载流子为空穴（掺杂3价元素硼，受主原子—\u0026gt;PN结构中为➖ 4.PN结（单向导电性的原因 本质原因（2 stages 扩散运动（多子参与）：参杂的交接面的剧烈浓度差导致，作用后，P区与N区交界面饱和，载流子不再扩散，中间部分称为耗尽层。 高掺杂—\u0026gt;耗尽层宽度变窄 低掺杂—\u0026gt;耗尽层交宽 多子意义为大部分载流子 漂移运动（少子参与）：扩散运动后，耗尽层不是一个电中和的状态，因此会有內电场，从而在內电场的作用下，少子受电场力作用产生的漂移运动 P区自由电子飘向N区，N区空穴飘向P区，最终参与扩散运动的多子数等于参与漂移的少子数，达到动态平衡，PN结由此产生。 少子意义为少部分载流子 5.单向导电性（半导体存在意义 P+正电，N为负电，电场被削弱，PN结导通 正向导通时，因接触电场的存在，将会在结上形成一固定降压，硅PN结降压一般为0.6V左右，为了好算直接按0.7V计算（0.7必定够用~） 正向导通时—\u0026gt;耗尽层变窄—\u0026gt;扩散运动加剧，多子运动浓度高—\u0026gt;扩散电流上升，从而导通 N加负，P加正（反向电压—\u0026gt;截止 耗尽层变大—\u0026gt;内建电场增强—\u0026gt;利于漂移运动—\u0026gt;扩散现象减弱—\u0026gt;漂移电流减少，从而截止 6.PN结的反向击穿 高浓度掺杂—\u0026gt;齐纳击穿 耗尽层窄，从而很小的反向电压就能击穿 低浓度掺杂—\u0026gt;雪崩击穿 耗尽层宽，需要较大电流才能击穿（由于载流子是以类似链式反应的速率1撞10，10撞100的效率反向运动，类似雪崩，因此被称为雪崩击穿 两种击穿都会产生大量的热，从而损坏元器件 二极管 硅管，导通电压 0.6～0.8v\n锗管，导通电压 0.1～0.3v\n​\t伏安特性曲线\n整流作用\n整流二极管的原理是利用PN结的非线性特性，在正向偏置下允许电流通过，在反向偏置下阻止电流通过。这使得整流二极管可以用于电路中的整流（将交流信号转换为直流信号）和电流保护等应用\n二极管等效电路 二极管微变等效电路 静态工作点：二极管添加直流偏置，此时在伏安特性曲线上所对应的点为静态工作点，此时可将二极管等效为一个动态电阻 公式 稳压二极管 硅管反向击穿时，在一定反向电流范围内，可表现出稳压特性 要保证在一定功率下工作，超出该功率就会爆炸 ","date":"2024-03-01T21:56:02+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/微信图片_20240303141619.jpg","permalink":"https://Dendrobium123.github.io/p/%E6%A8%A1%E6%8B%9F%E7%94%B5%E5%AD%90%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","title":"模拟电子技术笔记"},{"content":"纪念一下terraria大师模式的通关 ​\t虽然之前就知道terraria有四种职业玩法，但从来没试过只用一种职业搭配通关游戏，因为这个游戏的自由度太高了，在没有了解功略的情况下很难自发有意识地控制自己的套装搭配，我想大部分人玩这个游戏都是所谓的散人套装吧。不得不说，分工明确的开荒确实能提高不少效率，在一昧的战斗之外，我也是第一次发现了建筑的乐趣，来看看我造的小家吧！\n我造的小家 一些内饰 这次玩的时候不管是跟npc买还是从各种地牢和洞穴拆，总之弄了超多的画~~ 一些外景 这次跟视频学了一下造鸟居和小桥，我只能说雀食好看\n最后还有我们四人的大合影！ “我见证了一位星子的诞生，是的，一位星子”\n“星子的诞生使这个世界开始动摇”\n“星子杀掉了那只夜晚中的可怖眼球”\n“星子斩断了腐化之地的那条巨大的虫子”\n“星子绞碎了那猩红之地中大脑的幻想”\n“星子碾碎了那暴君之骨，让地牢重见天日”\n“我指引星子来到地狱，让他献祭出世界之墙”\n“古老的光明与黑暗之魂已被释放”\n“星子敲碎了那危险是祭坛，挖掘那些不曾拥有过的矿物”\n“这将是一个恐怖的夜晚……你感到来自于大地的震动……你感觉周围的空气越来越冷……”\n“地面那巨大的机械蠕虫以一种奇怪的姿势停下，天空中的那对双眼一直注视着什么，寒冷的机械骨骼包围住了一个人”\n“星子来到丛林，来寻找那粉色的花苞”\n“我听到了一声刺耳的尖叫，是那个古老的花朵的声音” “星子向我告别，或许他很难再回来”\n“我听到了石头坠落的声音，神庙中的守护者轰然倒塌”\n“我注视着地牢，从白昼看到黑夜，地牢上方闪耀出奇怪的光芒” “天界生物已入侵”\n“星子斩断着那四根天界之柱，拿到天界碎片，合成出那来自天界一般的武器”\n“我感到视线越来越暗，来自那位不可一世的神明，而那不可一世的神明即将降世，我提醒着星子，星子却说他早有准备”\n“天空化为黑暗，四周变得昏沉，在一个耀眼的光芒中，那位神明，降临于泰拉大陆上”\n“我注视着他们的战斗，很快，我瞪大双眼，看到了我这辈子都不曾想象过的景象”\n“神明歪着头说到：\u0026lsquo;你并不是神，但你有着神一般的光芒，你杀死了我，而你的征途却不止于此。\u0026rsquo;”\n“神明死亡，而星子，化为了真正的星。”\n末尾的文字来源b站up主雁老师\n","date":"2024-02-24T20:46:50+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/cover terraria.jpg","permalink":"https://Dendrobium123.github.io/p/%E6%88%91%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BC%82%E4%B8%96%E7%95%8C%E7%9A%84%E6%A2%A6/","title":"我做了一个异世界的梦"},{"content":"我只想说，做blog好难，尤其是对我这种毫无经验的若至来说 markdown语法现在也没研究明白，不知道要怎么弄\n第一次发博客就随便放两张我们胡桃的照片好了，啊~真可爱啊。\n","date":"2024-02-23T15:46:50+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/cover2.jpg","permalink":"https://Dendrobium123.github.io/p/hello-world/","title":"第一篇博客"},{"content":"PReNet渐近递归网络+模块改进gam PReNet是什么 PReNet是一种渐进递归网络，通过在基于ResNet的网络中引入递归层，以及将阶段性结果和原始多雨图像作为每个ResNet的输入，来提高去雨的性能。该网络能够有效地去除雨滴，对于图像去雨领域的研究具有重要意义，并可作为未来图像去雨研究的合适基础。\nResNet 我准备以PReNet为基底\n21、gam模块 论文《Global Attention Mechanism: Retain Information to Enhance Channel-Spatial Interactions》\n1、作用 这篇论文提出了全局注意力机制（Global Attention Mechanism, GAM），旨在通过保留通道和空间方面的信息来增强跨维度交互，从而提升深度神经网络的性能。GAM通过引入3D排列与多层感知器（MLP）用于通道注意力，并辅以卷积空间注意力子模块，提高了图像分类任务的表现。该方法在CIFAR-100和ImageNet-1K数据集上的图像分类任务中均稳定地超越了几种最新的注意力机制，包括在ResNet和轻量级MobileNet模型上的应用。\n2、机制 1、通道注意力子模块：\n利用3D排列保留跨三个维度的信息，并通过两层MLP放大跨维度的通道-空间依赖性。这个子模块通过编码器-解码器结构，以一个缩减比例r（与BAM相同）来实现。\n2、空间注意力子模块：\n为了聚焦空间信息，使用了两个卷积层进行空间信息的融合。同时，为了进一步保留特征图，移除了池化操作。此外，为了避免参数数量显著增加，当应用于ResNet50时，采用了分组卷积与通道混洗。\n3、独特优势 1、效率与灵活性：\nGAM展示了与现有的高效SR方法相比，如IMDN，其模型大小小了3倍，同时实现了可比的性能，展现了在内存使用上的高效性。\n2、动态空间调制：\n通过利用独立学习的多尺度特征表示并动态地进行空间调制，GAM能够高效地聚合特征，提升重建性能，同时保持低计算和存储成本。\n3、有效整合局部和非局部特征：\nGAM通过其层和CCM的结合，有效地整合了局部和非局部特征信息，实现了更精确的图像超分辨率重建。\n4、代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import torch.nn as nn import torch class GAM_Attention(nn.Module): def __init__(self, in_channels, rate=4): super(GAM_Attention, self).__init__() # 通道注意力子模块 self.channel_attention = nn.Sequential( # 降维，减少参数数量和计算复杂度 nn.Linear(in_channels, int(in_channels / rate)), nn.ReLU(inplace=True), # 非线性激活 # 升维，恢复到原始通道数 nn.Linear(int(in_channels / rate), in_channels) ) # 空间注意力子模块 self.spatial_attention = nn.Sequential( # 使用7x7卷积核进行空间特征的降维处理 nn.Conv2d(in_channels, int(in_channels / rate), kernel_size=7, padding=3), nn.BatchNorm2d(int(in_channels / rate)), # 批归一化，加速收敛，提升稳定性 nn.ReLU(inplace=True), # 非线性激活 # 使用7x7卷积核进行空间特征的升维处理 nn.Conv2d(int(in_channels / rate), in_channels, kernel_size=7, padding=3), nn.BatchNorm2d(in_channels) # 批归一化 ) def forward(self, x): b, c, h, w = x.shape # 输入张量的维度信息 # 调整张量形状以适配通道注意力处理 x_permute = x.permute(0, 2, 3, 1).view(b, -1, c) # 应用通道注意力，并恢复原始张量形状 x_att_permute = self.channel_attention(x_permute).view(b, h, w, c) # 生成通道注意力图 x_channel_att = x_att_permute.permute(0, 3, 1, 2).sigmoid() # 应用通道注意力图进行特征加权 x = x * x_channel_att # 生成空间注意力图并应用进行特征加权 x_spatial_att = self.spatial_attention(x).sigmoid() out = x * x_spatial_att return out # 示例代码：使用GAM_Attention对一个随机初始化的张量进行处理 if __name__ == \u0026#39;__main__\u0026#39;: x = torch.randn(1, 64, 20, 20) # 随机生成输入张量 b, c, h, w = x.shape # 获取输入张量的维度信息 net = GAM_Attention(in_channels=c) # 实例化GAM_Attention模块 y = net(x) # 通过GAM_Attention模块处理输入张量 print(y.shape) # 打印输出张量的维度信息 缝合代码\n测试后\n第二个为输入通道数\n目前prenet的networks.py中\n要点为输入输出通道数一致，搭积木\n在我准备缝合的prenet的前向传播的获取x输入这步拦截获得通道数\n插入\n1 print(x.shape) 运行了一下train.py，发现x的通道数是3\n现在确定x被赋值前的通道数\n发现没有变化\n18为一次\n下面是 PReNet 前向传播的流程图描述：\n输入数据：接收输入数据 input，这是图像或其他类型的特征张量。\n初始化隐藏状态和细胞状态：\n初始化隐藏状态 h 和细胞状态 c 为全零张量，大小与 input 的空间维度相匹配。 检查是否使用GPU：\n如果设置了使用GPU，则将 h 和 c 转移到GPU上。 循环迭代：对于定义的迭代次数 self.iteration：\n特征合并：将 input 与前一步的输出 x 在通道维度上合并。\n第一层卷积处理：通过 self.conv0 处理合并后的特征。\n状态合并：将卷积后的特征 x 与隐藏状态 h 在通道维度上合并。\nLSTM门控制逻辑\n：\n输入门：i = self.conv_i(x) 遗忘门：f = self.conv_f(x) 更新门：g = self.conv_g(x) 输出门：o = self.conv_o(x) 更新细胞状态：c = f * c + i * g 更新隐藏状态：h = o * torch.tanh(c) 残差连接和激活\n：对每一层残差卷积模块进行处理，加上残差连接并通过ReLU激活函数：\nx = F.relu(self.res_conv1(x) + resx) 依此类推，直到 self.res_conv5 输出卷积处理：通过 self.conv(x) 获取最终的输出特征。\n更新输出：将卷积输出与原始输入相加得到最终输出，保存到输出列表 x_list 中。\n返回输出：迭代完成后，返回最终的输出 x 和每次迭代的输出列表 x_list。\n我在第一层卷积层后缝合了gam模块\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class PReNet(nn.Module): def __init__(self, recurrent_iter=6, use_GPU=True): super(PReNet, self).__init__() self.iteration = recurrent_iter self.use_GPU = use_GPU self.conv0 = nn.Sequential( nn.Conv2d(6, 32, 3, 1, 1), nn.ReLU() ) self.res_conv1 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.res_conv2 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.res_conv3 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.res_conv4 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.res_conv5 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.conv_i = nn.Sequential( nn.Conv2d(32 + 32, 32, 3, 1, 1), nn.Sigmoid() ) self.conv_f = nn.Sequential( nn.Conv2d(32 + 32, 32, 3, 1, 1), nn.Sigmoid() ) self.conv_g = nn.Sequential( nn.Conv2d(32 + 32, 32, 3, 1, 1), nn.Tanh() ) self.conv_o = nn.Sequential( nn.Conv2d(32 + 32, 32, 3, 1, 1), nn.Sigmoid() ) self.conv = nn.Sequential( nn.Conv2d(32, 3, 3, 1, 1), ) self.gam=GAM_Attention(in_channels=32) def forward(self, input): batch_size, row, col = input.size(0), input.size(2), input.size(3) x = input h = Variable(torch.zeros(batch_size, 32, row, col)) c = Variable(torch.zeros(batch_size, 32, row, col)) if self.use_GPU: h = h.cuda() c = c.cuda() x_list = [] for i in range(self.iteration): x = torch.cat((input, x), 1) x = self.conv0(x) x = self.gam(x) x = torch.cat((x, h), 1) i = self.conv_i(x) f = self.conv_f(x) g = self.conv_g(x) o = self.conv_o(x) c = f * c + i * g h = o * torch.tanh(c) x = h resx = x x = F.relu(self.res_conv1(x) + resx) resx = x x = F.relu(self.res_conv2(x) + resx) resx = x x = F.relu(self.res_conv3(x) + resx) resx = x x = F.relu(self.res_conv4(x) + resx) resx = x x = F.relu(self.res_conv5(x) + resx) x = self.conv(x) x = x + input x_list.append(x) return x, x_list 训练到61epoch，20070steps时信噪比到了37.81比原先论文中的\n37.48（100epoch）还高一些\n刚刚那个是tensorboard里给出的数据，不是针对输出结果进行的评判\n一番折腾搞定matlab的问题后，重新对Rain100L和Rain1400数据集进行了针对结果的图像评判\n评判标准为psnr和ssim\n对Rain100L，在我改进的PReNet-gam下表现为\n1 psnr=37.5606, ssim=0.9788 原论文为37.48/0.979，目前psnr比原先高一些，ssim没有达到原模型效果，可能在达到100epoch后会有所改善\n对Rain1400，表现为\n1 psnr=32.5176, ssim=0.9450 原论文为32.60/0.946\nrain100l的模型为epoch71，rain400模型为epoch41\n作为比较，原论文中的参数为训练100epoch\n1 Our progressive networks are implemented using Py torch [24], and are trained on a PC equipped with two N VIDIA GTX 1080Ti GPUs. In our experiments, all the progressive networks share the same training setting. The patch size is 100 100, and the batch size is 18. The ADAM [17] algorithm is adopted to train the models with an initial learning rate 1 10 3, and ends after 100 epochs. When reaching 30, 50 and 80 epochs, the learning rate is decayed by multiplying 02 原论文中对Rain1400的结果检测为32.60和0.946，我的结果还差一点，但可能在达到100epoch后能有更好的表现\n之前用test_real想测试模型在真实图片上的效果，但总是报错说参数对不上，现在改一下text_real的代码\n发现test_real和普通的test_PReNet差别在它调用的是Generator_lstm\n1 2 3 4 # test_PReNet Build model print(\u0026#39;Loading model ...\\n\u0026#39;) model = PReNet(opt.recurrent_iter, opt.use_GPU) print_network(model) 1 2 3 4 # test_real Build model print(\u0026#39;Loading model ...\\n\u0026#39;) model = Generator_lstm(opt.recurrent_iter, opt.use_GPU) print_network(model) 于是修改Generator_lstm\n这个模块通过递归迭代处理图像，每一次迭代都会更新隐藏状态和细胞状态，类似于 LSTM 网络中的操作。模型首先使用一系列的卷积和 ReLU 激活层来提取特征，然后使用四个 LSTM 风格的门（input gate, forget gate, output gate 和 update gate）来更新状态。每次迭代后，都会输出一个新的图像结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 定义初始卷积层 self.conv0 = nn.Sequential( nn.Conv2d(6, 32, 3, 1, 1), nn.ReLU() ) # 定义残差卷积层 self.res_conv = [nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) for _ in range(5)] self.res_conv = nn.ModuleList(self.res_conv) # LSTM风格的卷积门 self.conv_i = nn.Sequential(nn.Conv2d(64, 32, 3, 1, 1), nn.Sigmoid()) self.conv_f = nn.Sequential(nn.Conv2d(64, 32, 3, 1, 1), nn.Sigmoid()) self.conv_g = nn.Sequential(nn.Conv2d(64, 32, 3, 1, 1), nn.Tanh()) self.conv_o = nn.Sequential(nn.Conv2d(64, 32, 3, 1, 1), nn.Sigmoid()) # 输出卷积层 self.conv = nn.Sequential(nn.Conv2d(32, 3, 3, 1, 1)) 按照之前缝合network的经验，直接将模块代码作为class插入，然后通过实例化调用，之后在forward中，对齐通道数，直接嵌入即可\n于是：在forward处\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def forward(self, input): if self.use_GPU: input = input.cuda() x = input h = torch.zeros_like(x) c = torch.zeros_like(x) if self.use_GPU: h = h.cuda() c = c.cuda() x_list = [] for i in range(self.iteration): x = torch.cat((input, x), 1) x = self.conv0(x) for res_layer in self.res_conv: resx = x x = F.relu(res_layer(x) + resx) x = torch.cat((x, h), 1) i = self.conv_i(x) f = self.conv_f(x) g = self.conv_g(x) o = self.conv_o(x) c = f * c + i * g h = o * torch.tanh(c) h = self.gam(h) # 注意力机制作用于隐藏状态 x = self.conv(h) x_list.append(x) return x, x_list 但输出图像似乎是本该在原有图像上减去的雨滴雨线图像，似乎是输出层出现了问题,于是我改成了这样，让输入减去输出，但结果还是不大行，跟没用这个模型一样，现在不知道问题在哪，可能我会再缝合一个se注意力模块\n缝合SE_ATTENTION SE 模块的目的是通过显式地重新校准卷积层的特征通道的响应来增强特征表达。\n1、SE Net模块 论文《Squeeze-and-Excitation Networks》\n1、作用 SENet通过引入一个新的结构单元——“Squeeze-and-Excitation”（SE）块——来增强卷积神经网络的代表能力。是提高卷积神经网络（CNN）的表征能力，通过显式地建模卷积特征通道之间的依赖关系，从而在几乎不增加计算成本的情况下显著提升网络性能。SE模块由两个主要操作组成：压缩（Squeeze）和激励（Excitation）\n2、机制 1、压缩操作：\nSE模块首先通过全局平均池化操作对输入特征图的空间维度（高度H和宽度W）进行聚合，为每个通道生成一个通道描述符。这一步有效地将全局空间信息压缩成一个通道向量，捕获了通道特征响应的全局分布。这一全局信息对于接下来的重新校准过程至关重要。\n2、激励操作：\n在压缩步骤之后，应用一个激励机制，该机制本质上是由两个全连接（FC）层和一个非线性激活函数（通常是sigmoid）组成的自门控机制。第一个FC层降低了通道描述符的维度，应用ReLU非线性激活，随后第二个FC层将其投影回原始通道维度。这个过程建模了通道间的非线性交互，并产生了一组通道权重。\n3、特征重新校准：\n激励操作的输出用于重新校准原始输入特征图。输入特征图的每个通道都由激励输出中对应的标量进行缩放。这一步骤有选择地强调信息丰富的特征，同时抑制不太有用的特征，使模型能够专注于任务中最相关的特征。\n3、独特优势 1、通道间依赖的显式建模：\nSE Net的核心贡献是通过SE块显式建模通道间的依赖关系，有效地提升了网络对不同通道特征重要性的适应性和敏感性。这种方法允许网络学会动态地调整各个通道的特征响应，以增强有用的特征并抑制不那么重要的特征。\n2、轻量级且高效：\n尽管SE块为网络引入了额外的计算，但其设计非常高效，额外的参数量和计算量相对较小。这意味着SENet可以在几乎不影响模型大小和推理速度的情况下，显著提升模型性能。\n3、模块化和灵活性：\nSE块可以视为一个模块，轻松插入到现有CNN架构中的任何位置，包括ResNet、Inception和VGG等流行模型。这种模块化设计提供了极大的灵活性，使得SENet可以广泛应用于各种架构和任务中，无需对原始网络架构进行大幅度修改。\n4、跨任务和跨数据集的泛化能力：\nSENet在多个基准数据集上展现出了优异的性能，包括图像分类、目标检测和语义分割等多个视觉任务。这表明SE块不仅能提升特定任务的性能，还具有良好的泛化能力，能够跨任务和跨数据集提升模型的效果。\n5、增强的特征表征能力：\n通过调整通道特征的重要性，SENet能够更有效地利用模型的特征表征能力。这种增强的表征能力使得模型能够在更细粒度上理解图像内容，从而提高决策的准确性和鲁棒性。\n4、代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import numpy as np import torch from torch import nn from torch.nn import init class SEAttention(nn.Module): # 初始化SE模块，channel为通道数，reduction为降维比率 def __init__(self, channel=512, reduction=16): super().__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) # 自适应平均池化层，将特征图的空间维度压缩为1x1 self.fc = nn.Sequential( # 定义两个全连接层作为激励操作，通过降维和升维调整通道重要性 nn.Linear(channel, channel // reduction, bias=False), # 降维，减少参数数量和计算量 nn.ReLU(inplace=True), # ReLU激活函数，引入非线性 nn.Linear(channel // reduction, channel, bias=False), # 升维，恢复到原始通道数 nn.Sigmoid() # Sigmoid激活函数，输出每个通道的重要性系数 ) # 权重初始化方法 def init_weights(self): for m in self.modules(): # 遍历模块中的所有子模块 if isinstance(m, nn.Conv2d): # 对于卷积层 init.kaiming_normal_(m.weight, mode=\u0026#39;fan_out\u0026#39;) # 使用Kaiming初始化方法初始化权重 if m.bias is not None: init.constant_(m.bias, 0) # 如果有偏置项，则初始化为0 elif isinstance(m, nn.BatchNorm2d): # 对于批归一化层 init.constant_(m.weight, 1) # 权重初始化为1 init.constant_(m.bias, 0) # 偏置初始化为0 elif isinstance(m, nn.Linear): # 对于全连接层 init.normal_(m.weight, std=0.001) # 权重使用正态分布初始化 if m.bias is not None: init.constant_(m.bias, 0) # 偏置初始化为0 # 前向传播方法 def forward(self, x): b, c, _, _ = x.size() # 获取输入x的批量大小b和通道数c y = self.avg_pool(x).view(b, c) # 通过自适应平均池化层后，调整形状以匹配全连接层的输入 y = self.fc(y).view(b, c, 1, 1) # 通过全连接层计算通道重要性，调整形状以匹配原始特征图的形状 return x * y.expand_as(x) # 将通道重要性系数应用到原始特征图上，进行特征重新校准 # 示例使用 if __name__ == \u0026#39;__main__\u0026#39;: input = torch.randn(50, 512, 7, 7) # 随机生成一个输入特征图 se = SEAttention(channel=512, reduction=8) # 实例化SE模块，设置降维比率为8 output = se(input) # 将输入特征图通过SE模块进行处理 print(output.shape) # 打印处理后的特征图形状，验证SE模块的作用 目前我想法是在PReNet的每个残差块后添加 SE 模块\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 self.res_conv1 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.se1 = SE_Attention(32) self.res_conv2 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.se2 = SE_Attention(32) self.res_conv3 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.se3 = SE_Attention(32) self.res_conv4 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.se4 = SE_Attention(32) self.res_conv5 = nn.Sequential( nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU(), nn.Conv2d(32, 32, 3, 1, 1), nn.ReLU() ) self.se5 = SE_Attention(32) self.conv_i = nn.Sequential( nn.Conv2d(32 + 32, 32, 3, 1, 1), nn.Sigmoid() ) 前向过程代码修改：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 x_list = [] for i in range(self.iteration): x = torch.cat((input, x), 1) x = self.conv0(x) x = self.gam(x) x = torch.cat((x, h), 1) i = self.conv_i(x) f = self.conv_f(x) g = self.conv_g(x) o = self.conv_o(x) c = f * c + i * g h = o * torch.tanh(c) x = h resx = x x = self.se1(x) x = F.relu(self.res_conv1(x) + resx) resx = x x = self.se2(x) x = F.relu(self.res_conv2(x) + resx) resx = x x = self.se3(x) x = F.relu(self.res_conv3(x) + resx) resx = x x = self.se4(x) x = F.relu(self.res_conv4(x) + resx) resx = x x = self.se5(x) x = F.relu(self.res_conv5(x) + resx) x = self.conv(x) x = x + input x_list.append(x) return x, x_list 对Rain100L训练93epochs后，loss最低，matlab对生成结果判定：\n1 PReNet_GAM_SE: psnr=37.4457, ssim=0.9788 作为对比，原论文为：37.48/0.979\n感觉效果还不如单放一个gam\n试试去掉gam，只剩se的\n于是，我在自己电脑上训练一个只剩se模块在各残差模块内\n在云端autodl上训练一个gam模块在各残差块内的，作对比实验\n对比实验 1.在每个残差块后加入SE_ATTENTION模块，无gam;训练数据集：Rain100L;100epochs\n结果：\nPReNet_SE_behindRES 1 PReNet_SE_behindRES：psnr=37.6894, ssim=0.9793 比论文的 37.48/0.979 更高\n2在每个残差块后加入gam模块，无se，训练数据集：Rain100L；100epochs\nPReNet_GAM_behindRES 1 PReNet_GAM_behindRES：psnr=37.9671, ssim=0.9804 对Rain12 测试，论文为36.62/0.952\n1 psnr=37.0380, ssim=0.9620 3.在第一个卷积层后添加gam模块，无se，训练数据集：Rain100L；100epochs，显卡：A100-PCIE-40GB\nPReNet_GAM_behindConv0 1 PReNet_GAM_behindConv0：psnr=37.4934, ssim=0.9788 4.在第一个卷积层后添加se模块，无gam，训练数据集：Rain100L；100epochs\nPReNet_SE_behindConv0 1 PReNet_SE_behindConv0：psnr=37.4894, ssim=0.9788 5.第一个卷积层后加gam，每个残差块后加se；训练数据集：Rain100L；100epochs\nPReNet_GAM_SE 1 PReNet_GAM_SE：psnr=37.4457, ssim=0.9788 6.第一个卷积层后加SE，每个残差块后加gam；训练数据集：Rain100L；100epochs\nPReNet_SE_GAM 1 PReNet_SE_GAM: 37.9160, ssim=0.9810 A100-PCIE-40GB\n可以看出，在每个残差块后加入GAM模块，以及在第一个卷积层后加入SE模块，残差块后加入GAM模块，性能提升最明显\n而其中，单在RES层后加入GAM方案中PSNR更高，而SE_GAM方案中，ssim更高\n接下来尝试第一层卷积后加GAM，残差块后加入GAM，因为通过数据可知，性能主要提升发生在RES后加入GAM后，但在第一层卷积后加入GAM，性能似乎也有略微提升\nPReNet_GAM_GAM 1 PReNet_GAM_GAM:psnr=37.9084, ssim=0.9802 实验环境\n64位windows11系统，GPU为NVIDIA GeForce RTX 4070(12GB/华硕)，处理器为 intel i5-13490F 十核，\nanaconda版本conda 23.7.4\ncuda版本12.1\npytorch版本2.1.2\nPython 3.9.18\ntensorboard版本2.16.2\n$$ MSE = \\frac{1}{mn} \\sum_{i=1}^m \\sum_{j=1}^n(I(i, j) - K(i, j))^2 $$\n","date":"0001-01-01T00:00:00Z","permalink":"https://Dendrobium123.github.io/p/","title":""},{"content":"火焰战士 不知从何时起我开始觉得天空好美，\n可能是因为那个少女消失在那里的缘故吧，\n我有些羡慕她，\n没有翅膀的我，\n只有将希望和憧憬寄托在她的身上，\n在天空中飞翔的她，能够感受到我的思念吗？\n天空不再是遥不可及，\n她一定把我的梦想带上了天际，\n我坚信。\n不知从何时起，\n我一直在做一个梦，\n一遍又一遍，\n没有开始也没有终结，\n我暗自祈祷这不是梦，\n因为梦总会结束，\n于是我开始等待，\n等待着某人将我唤醒，\n我仿佛坐在空无一人的山顶上，\n听着一个脚步声由远而近，\n也许那之后才是梦的开始，\n仿佛时间已经停止，\n我一直在等待着，\n直到我已经忘记了，\n为何要等待，\n但是正如黑夜之后一定是黎明，\n奇迹一定会到来，\n我坚信\n","date":"0001-01-01T00:00:00Z","image":"http://tuchuang.dendrobiumcgk.chat/pic/image-20240517233703783.png","permalink":"https://Dendrobium123.github.io/p/%E6%97%B6%E9%97%B4%E7%9A%84%E8%BD%AE%E5%BB%93/","title":"时间的轮廓"}]