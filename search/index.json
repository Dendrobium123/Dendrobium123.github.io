[{"content":"PyTorch基础笔记_01 常用库：torch，numpy，pandas，matplotlib\n1 2 3 4 5 import torch import numpy as np import torch.nn as nn from torch import optim as optim#优化器 import torch.nn.functional as F#用到的函数 基础使用 假设一个最简单的神经网络，没有隐藏层，只有输入层和输出层\neg：5个输入神经元，7个输出神经元，全连接 $$ y=w\\cdot x+b $$ x为输入层，形状为（1,5）； $$ \\begin{bmatrix}x_1 \u0026amp; x_2\u0026amp;x_3\u0026amp;x_4\u0026amp;x_5\\end{bmatrix} $$ Y为输出层，形状为（1,7）#同上 $$ \\begin{bmatrix}y_1 \u0026amp; y_2\u0026amp;y_3\u0026amp;y_4\u0026amp;y_5\u0026amp;y_6\u0026amp;y_7\\end{bmatrix} $$ w为权重，形状为（5,7) #全连接， $$ \\begin{bmatrix}w_{11}\u0026amp;w_{12}\u0026amp;w_{13}\u0026amp;w_{14}\u0026amp;w_{15}\u0026amp;w_{16}\u0026amp;w_{17} \\\\w_{21} \u0026amp; w_{22}\u0026amp;w_{23}\u0026amp;w_{24}\u0026amp;w_{25}\u0026amp;w_{26}\u0026amp;w_{27} \\\\w_{31} \u0026amp; w_{32}\u0026amp;w_{33}\u0026amp;w_{34}\u0026amp;w_{35}\u0026amp;w_{36}\u0026amp;w_{37} \\\\w_{41} \u0026amp; w_{42}\u0026amp;w_{43}\u0026amp;w_{44}\u0026amp;w_{45}\u0026amp;w_{46}\u0026amp;w_{47} \\\\w_{51} \u0026amp; w_{52}\u0026amp;w_{53}\u0026amp;w_{54}\u0026amp;w_{55}\u0026amp;w_{56}\u0026amp;w_{57} \\end{bmatrix} $$ b为bias偏置，每个输出神经元输出前都会经过bias调整，所以形状为7\n1 2 3 4 5 6 7 8 9 10 11 12 13 #定义各个参数 w=torch.randn(5,7,requires_grad=True)#生成5*7的随机数矩阵来填充w,同时设定w是需要后续进行梯度下降更新的属性，后半部分缺失会导致程序报错 b=torch.randn(7,requires_grad=True) x=torch.randn(1,5) Y=torch.randn(1,7) lr=0.001#设置学习率 y=F.relu(x @ w + b) #x与w为矩阵，使用矩阵乘法@,同时调用事前导入为F的激活函数，这里选择relu函数（1） #开始求loss，假定本案例为一个分类任务，选择交叉熵损失函数 loss=F.cross_entropy(y,Y) #开始求参数梯度，进行梯度下降算法更新参数，pytorch提供了一个函数方法backward()，可以直接帮助我们找到所有参数梯度，无需自己算 loss.backward()#这步只求了梯度，顺利运行的前提是需要提前设定参数是需要求梯度的属性 w.grad#查看w经过梯度下降所得的参数值，！还没有进行参数的更新！见（2） w=w -lr * w.grad#这步才是更新了参数（3） 两种损失函数 1.分类任务一般使用交叉熵损失函数：\n$$ \\text{Cross-Entropy}= -\\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)] \\\\y_i：\\text{ 类别的真实标签（通常为0或1）} \\\\\\hat{y_i}: \\text{对应类别的预测概率} $$\n2.回归任务一般使用均方误差损失函数\n$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\y_i：\\text{真实值} \\\\\\hat{y_i}:预测值 \\\\n:样本数量 $$ 损失函数使用时需注意该函数的传参\n（1）结果：\n（2）结果：\n（3）结果：\n搭建模型 搭建网络需要定义一个类，eg此处输入图像为rgb彩色图像，像素为48*48\nps: jupyter lab查看函数参数快捷键shift+tab\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class M_CNN(nn.Module):#定义网络，父类为之前导入的nn类 def __init__(self):#初始化 super().__init__()#父类的函数方法 self.conv1 =nn.Conv2d(3,16,kernel_size=3,padding=1)#设计的卷积层，调用nn父类中的Conv2d卷积函数，参数见（4）eg：输入为彩色图像（输入为3），输出16个通道，卷积核大小为3*3；padding=1为填充1像素保证输出的长宽不变 self.conv2 =nn.Conv2d(16,32,kernel_size=3,padding=1) self.bn1=nn.BatchNorm2d(16)#规范化，通道数为16#第一层卷积的输出通道数 self.bn2=nn.BatchNorm2d(32) self.MaxPool=nn.MaxPool2d(kernel_size=2)#池化层 self.fc1=nn.Linear(12*12*32,1000)#网络全连接层，将处理完的输入图像所有像素拼接为一维，加入中间有1000个神经元的隐藏层 self.fc2=nn.Linear(1000,100)#1000个隐藏层1神经元，最送到100个隐藏层2神经元，由于12*12*32的输入太大，故需要不同神经元数目的隐藏层过渡 self.fc3=nn.Linear(100,7)#最后输出为7个神经元 def forward(self,x): x=F.relu(self.bn1(self.conv1(x)))#输入先过卷积，后过规范器，最后通过激活函数relu，更新原有输入 x=self.MaxPool(x)#经过池化后图像宽高对半减 x=F.relu(self.bn2(self.conv2(x)))#经过第二个卷积层 x=self.MaxPool(x) x=x.view(-1,12*12*32)#进入全连接之前，需要讲3维张量撑成1维的向量，-1代表可能用小批量进行训练，不确定数目，代表自适应，后面的数值代表进入的张量 x=F.relu(self.fc1(x))#进入全连接 x=F.relu(self.fc2(x)) x=F.relu(self.fc3(x)) return x 全连接层参数计算： $$ 一个(3,48\\times48)图像通过一个(3,16,kernel=3\\times3,步长为1)的卷积层，\\\\输出为(16,48\\times48)图像，\\\\通过第一个池化层，窗口为2\\times2,步长为2，\\\\输出为(16，24\\times24),\\\\经过第二个卷积层(16，32，kernel=3\\times3,步长为1)\\\\输出为(32,24\\times24)，\\\\通过第二个池化层，变为(32，12\\times12) $$\n（4）nn.Conv2d函数的参数：\n数据处理 读取数据，转化为dataset，再由dataset转化为dataloader\n1 2 3 4 5 6 7 8 9 from torch.utils.data import DataLoader from torchvision import datasets,tranforms from torchvision.datasets import ImageFolder transform=transforms.Compose([ transforms.ToTensor()#读取图像后将数据转换为tensor类型 transforms.Normalize(mean=(0.5,),std=(0.5,))#去均值，除标准差的归一化操作 ]) dataset=ImageFolder(\u0026#39;file_path\u0026#39;,tansform=transform)#通过imagefolder来为读取的数据进行标注，后半部分使用tranform进行格式转化 dataloader=DataLoader(dataset,batch_size=32,shuffle=True)#shuffle意义为每个batch开始时顺序是否要被打乱 跑模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #检测GPU使用情况 device=torch.device(\u0026#39;cuda:0\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) print(\u0026#39;Using device\u0026#39;,device) #创建模型 model=CNN_M().to(device) #定义损失函数 loss_fun=F.cross_entropy #定义优化器 opt=optim.Adam(model.parameters(),lr=0.01)#parameters为模型的所有参数 epochs=8#以batchsize为32的步长，每个批次通过一遍网络，直至所有样本被扫完作为一个epoch for epoch in range(epochs): for i,data in enumerate(dataloader,0):#每次的小batch做的事 inputs,labels=data[0].to(device),data[1].to(device)#input为作为输入的数据，labels为获取标签，to(device)为把数据往显卡里送 y=model(inputs)#进入模型的forward第一层到最后一层，结果给y loss=F.cross_entropy(y,labels)#通过计算得的y与labels标签正确答案进行损失计算 loss.backward()#计算梯度值 opt.step()#优化器进行梯度下降，更新参数 opt.zero_grad()#参数的梯度清零，不然进入下一轮梯度计算时会累积上一次的结果 print(i,loss) ps:最后来一张之前tutu带我看的不认识的Vtuber演唱会 ","date":"2024-04-18T21:02:02+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/pytorch_0.jpg","permalink":"https://Dendrobium123.github.io/p/pytorch%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0_01/","title":"PyTorch基础笔记_01"},{"content":"\r深度学习基础知识 几种“学习”间的关系\n机器学习\u0026mdash;最大的概念 \u0026lt;让机器通过学习的方式得到一个可以解决问题的模型\u0026gt;\n学习方法：KNN（K近邻），k means（k均值解决聚类问题），SVM，深度学习\n机器学习，隐式学习\n神经网络：输入层，隐藏层，输出层\n神经元\n不改变网络层和算法的情况下，影响输出结果的是各神经元连接线路上的数值权重\n除了一系列加减乘除的线性变换外，还引入了激活函数\n激活函数：阶跃函数（不用 希望通过梯度下降的方式求得参数更新的过程，阶跃函数无法正常求导，需要引入δ函数，因此使用别的函数作为激活函数Sigmoid，以此解决阶跃函数不可导的问题\nSigmoid： $$ S(x)=\\dfrac{1}{1+e^{-x}} $$\nsigmoid导数： $$ S\u0026rsquo;(x)=\\dfrac{e^{-x}}{(1+e^{-x})^2}=S(x)(1-S(x)) $$ Sigmoid函数及其导数的图像：\n注：取值范围在0—1间的sigmoid函数叫logistic函数\rtanh： $$ tanh=\\dfrac{e^x-e^{-x}}{e^x+e^{-x}} $$\n范围在（-1，1）间的激活函数\nRelu函数 $$ f(x)=\\begin{cases}x \u0026amp; x\\geq0 \\\\0\u0026amp;x\u0026lt; 0\\end{cases} $$\n每个神经元所做的事： $$ g_{output}=g(w_1\\times a+w_2\\times b+w_3\\times c+w_4\\times d)—\u0026gt;relu $$ 注：a,b,c,d为权重值，神经元输出结果为各参数加权后通过一个relu函数所得的值\n机器学习的目的是在给定前提情况下，寻找能得到最好输出的w参数们\n梯度下降 如何寻找需要的W\n通过当前所计算得出的结果与已知的正确结果做差，考虑到所得结果的正负号问题，采用对式子求平方的方式（平方好求导，绝对值不好求导） $$ L=(f(x)-y)^2 $$\nL越小，模型性能越好，f(x)与参数w有关，因此L也是个关于w的函数。\n可以通过调整w来使L的取值变小\n动态更新W，eg：初始值w0，第一刻w1\u0026hellip;.. $$ W_1=W_0-lr\\cdot \\frac{\\partial L}{\\partial W_0} \\\\W_2=W_1-lr\\cdot \\frac{\\partial L}{\\partial W_1} \\\\\u0026hellip; $$\n局部最优/全局最优 类似高等数学函数章节中的极值和最值问题，局部导数为0的极值点不代表此处是整个函数的极值\n~~乐经良：说明它是一个地头蛇~~\rps:顺带吐槽一句，这个hugo对LateX数学公式的键入好像不是很友好，比如公式间的换行用要用\\\\，但是他识别代码的时候只识别一条杠\\，这就导致像 $$ f(x)=\\begin{cases}x \u0026amp; x\\geq0 \\\\0\u0026amp;x\u0026lt; 0\\end{cases} $$ 这种分段的函数会显示成这样 $$ f(x)=\\begin{cases}x \u0026amp; x\\geq0 \\0\u0026amp;x\u0026lt; 0\\end{cases} $$\nbyd后来我发现你只需要打3个\\就能解决问题了。\n","date":"2024-04-14T12:20:50+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/深度学习00cover.jpg","permalink":"https://Dendrobium123.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_00/","title":"深度学习_00"},{"content":"这篇文章是我之前一门通识课所布置的作业，今天闲的没事就想水一篇文章。\n小石我纵横现代简中互联网多年，对如今的网络文化也有一定的思考，借着通识课作业的契机一并谈谈我对当今网络抽象话的了解。\n中国后现代主义与网络亚文化的发展 以“抽象文化”为例 近年来，随着网络社交平台和短视频行业的发展，我们越来越不难注意到，有这样一类越来越频繁浮现的群体。他们往往说着荒诞又难以理解的话语，洒脱地嘲讽或攻击一些公众形象，同时又无时不刻进行着自我矮化。这类群体在后疫情时期越发壮大逐渐从网络世界的边缘浮现至仅次于主流文化的亚文化地位。在理解了后现代主义和解构主义后，我们可以说这类“抽象文化”的诞生和扩张正是后现代主义思潮在当今中文网络世界的具体表现之一。\n后现代主义一词在法国哲学家利奥塔出版了《后现代状态》一书后正式出道，它是一种基于对西方现代社会进行反思和批判而形成的文化思潮。而其核心理论之一便是宏大叙事的消亡。宏大叙事被认为是现代性的一种体现，是一种使特定意识形态世界观乃至价值观正当化的趋势，也是一种对事物运行规律和人类历史进程的宏观解释。但后现代主义主张对本质的否定和对非理性的推崇，强调多元性、差异性，反对同一性，并具有不确定性、分裂性、解构性及戏谑性等特点。而解构主义则是由法国哲学家德里达提出的，其原本的主张是对中心主义的一种批判，是破坏有中心的二元对立状态，而在今天，解构一词作为一种行为可以用于任何对象和场合，成为了后现代主义中的一环 。\nJean-Francois Lyotard\nJacques Derrida 为什么说目前网络抽象文化的扩张是中国后现代主义的具体表现呢？ 首先抽象文化本身仅仅只是一个指代，其本身并没有具体的定义，因为其本身就是网络去中心化的产物。这批亚文化的创作者们在17,18年左右伴随着李赣和孙笑川之流的直播间而诞生，但值得注意的是【孙笑川】本身并不是该亚文化的定义，它只是抽象文化的一个符号。而抽象文化的符号是无穷无尽的，因为其本身就是一种会无序扩张的社会模因。我在这里举一些最近抽象文化的新网络符号，比如说“yy丁真”系列；抽象小卖部系列；asoul女团；你说的对，但原神是一款XXX系列；东北往事；柯洁直播间等等，与之类似的还有油管的xqc，Ishowspeed，veibae等主播的直播间。抽象文化的群体很喜欢对一些互联网上的土味视频进行解构和二次创作，比如上述的抽象小卖部和东北往事等视频，其原本只是类似于通过卖丑，猎奇等行为换取播放量的视频，但在抽象文化的二次塑形中爆发了前所未有的生命力，原本的猎奇被视为荒诞的艺术，各类卖丑整活的行为也被视为一种对社会规训下的现实的反抗。\n安迪沃霍尔曾说，在未来，每个人都能出名15分钟。 因此，每个人也都可能被吸纳进抽象文化的符号中，因为当你进入了这个飞速迭代的网络世界后，发掘和被消费是在一个很短的周期内被完成的，一个梗，一个meme的生命力是很短暂的，尽管如此它们却可以在如此短的时间中扩张至互联网的每一个角落。但人们迫切地需要新的故事和梗来娱乐和消费，因此这种符号的产生就和人工智能训练集和产出内容一样，无时不刻不在自我迭代和吸收新的叙事。\n后现代主义在这其中中起到了推波助澜的作用。 一方面，后现代主义中的解构化、碎片化和去中心化等因素助推了抽象文化滋生。同时因为当今包含宏大叙事的现代主义在中国仍占据主导地位，对事物的评判仍保有一个中心化的标准，这正是抽象文化理所应当存在的土壤。\n与早年qq空间的杀马特之流不同，抽象文化本身的特点就是对传统鄙视链的彻底摧毁。\n曾经的杀马特文化有各式各类的家族，不同家族之间等级严明，在家族中的地位甚至与你加入的时间有关。**其本质与其说是对资本社会的反抗不如说是通过建立另一个帝国以此逃避原有的社会秩序。**但抽象文化圈却是其完全相反的另一面，比如在百度贴吧中一个人资历可以通过铭牌显示出来，如果入吧的时间早，发言帖子多会增加自己的经验从而提高在本吧的等级。而孙笑川吧中总会出现“黄牌赶紧给绿牌磕一个”这样的话术（黄牌等级高，绿牌等级低）。通过这样完全颠倒的权力体系说明了抽象文化的对鄙视链结构的否定。与之类似的，还有丁真的爆火。在21年初官方为了宣传少数民族和扶贫工作仅仅因为丁真的纯真笑容便将这位学历连小学都没有的文盲聘请至国企工作同时大肆宣传将其捧成了一个网红。大部分网民对此是愤慨且否认的，但批评并不能阻止这样的事发生，因此在后期他们选择将丁真彻底解构，如同第二个孙笑川一样，丁真也成为了抽象文化的符号，网民们肆无忌惮地发表丑化丁真的表情包，将一切他们认为由于官方宣传而火爆但德不配位的人称之为XX丁真，如赛博丁真，滑雪丁真等等。这体现了抽象文化的反权威性和多元性。\n抽象文化也有着和所有后现代主义思潮一样的弊病， 鲍德里亚说过：每个共同体的建构是不同的，但解构都是大同小异的 抽象文化下的网民们乐忠于这样的一种解构，乃至于任何事物在他们面前都只是一堆可供娱乐和消费的符号。一个最直接的例子“地狱笑话”这个词原意是笑了就要下地狱，因为其所取笑的对象本身是由于各种意外而发生悲剧的个体，比如侏儒症患者，残疾人，父母双亡等等。但地狱笑话吧却会以此为依据对其取笑，这样的一种解构是超脱道德的，他们会用草莓来嘲笑因生前想吃草莓，最后却因糖尿病而死的“墨茶”，用直升机戏谑科比等等。为所欲为的解构和消解严肃性成了他们唯一的目的，这本身就是荒诞的，同时也让他们失去了严肃讨论的土壤。\n“抽象文化”在[后现代]皮囊下的[现代性] 在这种后现代思潮的混乱扩张中，许多刚接触抽象文化的新网民会由于其现代化的思想，以遵从威权的思考角度盲目跟风，此时复制相关的话术成为了他们彰显自己抽象属性的方式，用这样一种不加以思考的刻奇的方式加入到后现代思潮的狂欢当中。就像维特根斯坦说的“人不是思考了再说话，而只是说话”。在这时抽象文化的后现代属性消失，它成为了现代化的一种延伸。\n中文互联网经历过各种非主流时期，从早年的杀马特时期，火星文时期，到如今的饭圈文化，抽象文化等。尽管他们迟早会被时间吞噬，但抽象文化作为后现代主义思潮在中国的一种具体体现，其本身将永久地改变整个中文互联网以至于改变新一代网民的思考方式。\n","date":"2024-03-03T13:10:42+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/db7641dcd583699a45fbb9a138a5679.jpg","permalink":"https://Dendrobium123.github.io/p/%E4%B8%AD%E5%9B%BD%E5%90%8E%E7%8E%B0%E4%BB%A3%E4%B8%BB%E4%B9%89%E4%B8%8E%E7%BD%91%E7%BB%9C%E4%BA%9A%E6%96%87%E5%8C%96%E7%9A%84%E5%8F%91%E5%B1%95%E4%BB%A5%E6%8A%BD%E8%B1%A1%E6%96%87%E5%8C%96%E4%B8%BA%E4%BE%8B/","title":"中国后现代主义与网络亚文化的发展以“抽象文化”为例"},{"content":" 模拟电路 模拟电路是指用来对模拟信号进行传输、变换、处理、放大、测量和显示等工作的电路。它主要包括放大电路、信号运算和处理电路、振荡电路、调制和解调电路及电源。 何为模拟？ 在模拟电路中，电压高低（或电流大小）是模拟了所代表的物理量的变化，例如声音信号，声音被话筒转换成电压时，电压的高低直接反映了音量大小，声音的频率（音调）直接就是电信号的频率，此谓模拟的概念。\n半导体基础 1.本征半导体：无杂质的稳定结构，其本身导电能力差（一般为+4价的硅晶体 其中不同原子的电子会形成共价键 载流子浓度低 易受温度影响 2.何为载流子？ 脱离共价键的电子称为自由电子（negative 原先的空缺位称为空穴（positive 方便数学处理，将空穴视为一种粒子，因此可与自由电子称为载流子 3.杂质半导体（在本征半导体中掺杂+5价元素和+3价元素 N型半导体：多数载流子为自由电子（掺杂5价元素磷，施主原子—\u0026gt;PN结构中为➕ P型半导体：多数载流子为空穴（掺杂3价元素硼，受主原子—\u0026gt;PN结构中为➖ 4.PN结（单向导电性的原因 本质原因（2 stages 扩散运动（多子参与）：参杂的交接面的剧烈浓度差导致，作用后，P区与N区交界面饱和，载流子不再扩散，中间部分称为耗尽层。 高掺杂—\u0026gt;耗尽层宽度变窄 低掺杂—\u0026gt;耗尽层交宽 多子意义为大部分载流子 漂移运动（少子参与）：扩散运动后，耗尽层不是一个电中和的状态，因此会有內电场，从而在內电场的作用下，少子受电场力作用产生的漂移运动 P区自由电子飘向N区，N区空穴飘向P区，最终参与扩散运动的多子数等于参与漂移的少子数，达到动态平衡，PN结由此产生。 少子意义为少部分载流子 5.单向导电性（半导体存在意义 P+正电，N为负电，电场被削弱，PN结导通 正向导通时，因接触电场的存在，将会在结上形成一固定降压，硅PN结降压一般为0.6V左右，为了好算直接按0.7V计算（0.7必定够用~） 正向导通时—\u0026gt;耗尽层变窄—\u0026gt;扩散运动加剧，多子运动浓度高—\u0026gt;扩散电流上升，从而导通 N加负，P加正（反向电压—\u0026gt;截止 耗尽层变大—\u0026gt;内建电场增强—\u0026gt;利于漂移运动—\u0026gt;扩散现象减弱—\u0026gt;漂移电流减少，从而截止 6.PN结的反向击穿 高浓度掺杂—\u0026gt;齐纳击穿 耗尽层窄，从而很小的反向电压就能击穿 低浓度掺杂—\u0026gt;雪崩击穿 耗尽层宽，需要较大电流才能击穿（由于载流子是以类似链式反应的速率1撞10，10撞100的效率反向运动，类似雪崩，因此被称为雪崩击穿 两种击穿都会产生大量的热，从而损坏元器件 二极管 硅管，导通电压 0.6～0.8v\n锗管，导通电压 0.1～0.3v\n​\t伏安特性曲线\n整流作用\n整流二极管的原理是利用PN结的非线性特性，在正向偏置下允许电流通过，在反向偏置下阻止电流通过。这使得整流二极管可以用于电路中的整流（将交流信号转换为直流信号）和电流保护等应用\n二极管等效电路 二极管微变等效电路 静态工作点：二极管添加直流偏置，此时在伏安特性曲线上所对应的点为静态工作点，此时可将二极管等效为一个动态电阻 公式 稳压二极管 硅管反向击穿时，在一定反向电流范围内，可表现出稳压特性 要保证在一定功率下工作，超出该功率就会爆炸 ","date":"2024-03-01T21:56:02+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/微信图片_20240303141619.jpg","permalink":"https://Dendrobium123.github.io/p/%E6%A8%A1%E6%8B%9F%E7%94%B5%E5%AD%90%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","title":"模拟电子技术笔记"},{"content":"纪念一下terraria大师模式的通关 ​\t虽然之前就知道terraria有四种职业玩法，但从来没试过只用一种职业搭配通关游戏，因为这个游戏的自由度太高了，在没有了解功略的情况下很难自发有意识地控制自己的套装搭配，我想大部分人玩这个游戏都是所谓的散人套装吧。不得不说，分工明确的开荒确实能提高不少效率，在一昧的战斗之外，我也是第一次发现了建筑的乐趣，来看看我造的小家吧！\n我造的小家 一些内饰 这次玩的时候不管是跟npc买还是从各种地牢和洞穴拆，总之弄了超多的画~~ 一些外景 这次跟视频学了一下造鸟居和小桥，我只能说雀食好看\n最后还有我们四人的大合影！ “我见证了一位星子的诞生，是的，一位星子”\n“星子的诞生使这个世界开始动摇”\n“星子杀掉了那只夜晚中的可怖眼球”\n“星子斩断了腐化之地的那条巨大的虫子”\n“星子绞碎了那猩红之地中大脑的幻想”\n“星子碾碎了那暴君之骨，让地牢重见天日”\n“我指引星子来到地狱，让他献祭出世界之墙”\n“古老的光明与黑暗之魂已被释放”\n“星子敲碎了那危险是祭坛，挖掘那些不曾拥有过的矿物”\n“这将是一个恐怖的夜晚……你感到来自于大地的震动……你感觉周围的空气越来越冷……”\n“地面那巨大的机械蠕虫以一种奇怪的姿势停下，天空中的那对双眼一直注视着什么，寒冷的机械骨骼包围住了一个人”\n“星子来到丛林，来寻找那粉色的花苞”\n“我听到了一声刺耳的尖叫，是那个古老的花朵的声音” “星子向我告别，或许他很难再回来”\n“我听到了石头坠落的声音，神庙中的守护者轰然倒塌”\n“我注视着地牢，从白昼看到黑夜，地牢上方闪耀出奇怪的光芒” “天界生物已入侵”\n“星子斩断着那四根天界之柱，拿到天界碎片，合成出那来自天界一般的武器”\n“我感到视线越来越暗，来自那位不可一世的神明，而那不可一世的神明即将降世，我提醒着星子，星子却说他早有准备”\n“天空化为黑暗，四周变得昏沉，在一个耀眼的光芒中，那位神明，降临于泰拉大陆上”\n“我注视着他们的战斗，很快，我瞪大双眼，看到了我这辈子都不曾想象过的景象”\n“神明歪着头说到：\u0026lsquo;你并不是神，但你有着神一般的光芒，你杀死了我，而你的征途却不止于此。\u0026rsquo;”\n“神明死亡，而星子，化为了真正的星。”\n末尾的文字来源b站up主雁老师\n","date":"2024-02-24T20:46:50+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/cover terraria.jpg","permalink":"https://Dendrobium123.github.io/p/%E6%88%91%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BC%82%E4%B8%96%E7%95%8C%E7%9A%84%E6%A2%A6/","title":"我做了一个异世界的梦"},{"content":"我只想说，做blog好难，尤其是对我这种毫无经验的若至来说 markdown语法现在也没研究明白，不知道要怎么弄\n第一次发博客就随便放两张我们胡桃的照片好了，啊~真可爱啊。\n","date":"2024-02-23T15:46:50+08:00","image":"http://tuchuang.dendrobiumcgk.chat/pic/cover2.jpg","permalink":"https://Dendrobium123.github.io/p/hello-world/","title":"第一篇博客"}]